{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7beb536e-20fd-4d00-b66e-3315a49409de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Boto3"
    }
   },
   "outputs": [],
   "source": [
    "# # Run cell on cluster restart or if receiving error: \n",
    "# # AttributeError: 'EMR' object has no attribute 'create_persistent_app_ui'\n",
    "\n",
    "# %pip install --upgrade boto3 botocore\n",
    "# %restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "705a0759-61ba-44ed-9262-02adec0e86b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## EMR Spark Event Log Analyzer\n",
    " \n",
    "This script analyzes EMR clusters to extract performance metrics from Spark History Servers. It discovers EMR clusters, connects to their persistent Spark History Server UIs, fetches application, job, stage, and SQL query data, and then processes this information into Spark DataFrames for performance analysis and optimization insights.\n",
    "\n",
    "## Required IAM Permissions\n",
    "#### EMR\n",
    "  - elasticmapreduce:ListClusters\n",
    "  - elasticmapreduce:DescribeCluster\n",
    "  - elasticmapreduce:ListSteps\n",
    "  - elasticmapreduce:DescribeStep\n",
    "  - elasticmapreduce:CreatePersistentAppUI\n",
    "  - elasticmapreduce:GetPersistentAppUIPresignedURL\n",
    "  - elasticmapreduce:ListInstanceGroups\n",
    "  - elasticmapreduce:ListInstanceFleets\n",
    "#### S3\n",
    "  - s3:PutObject\n",
    "  - s3:GetObject\n",
    "  - s3:ListBucket\n",
    "#### STS\n",
    "  - sts:GetCallerIdentity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7811f701-661f-4a92-bfbe-fb417ea1d699",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Config"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n  Environment: dev\n  AWS Region: us-east-1\n  EMR Cluster ARN: Auto-discover clusters\n  Timeout: 300 seconds\n  Max Applications per Cluster: 50\n  Cluster States to Analyze: ['TERMINATED']\n  Cluster Name Filter: None\n  Max Clusters to Analyze: 100\n  Created After Date: None\n  Created Before Date: None\n  S3 Output Path: s3://ttd-vertica-backups/env=test/vertica-ext/databricks-stage/sparkmetrics\n  Persistent UI Timeout: 30 seconds\n  Batch Size: 100 clusters\n  Batch Delay: 30 seconds\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Configuration Parameters\n",
    "# ----------------------------------------------------------------------\n",
    "# Parse and validate configuration\n",
    "dbutils.widgets.text(\"aws_region\", \"us-east-1\", \"AWS Region\")\n",
    "dbutils.widgets.text(\"emr_cluster_arn\", \"\", \"EMR Cluster ARN (optional - leave blank to discover clusters)\")\n",
    "dbutils.widgets.text(\"timeout_seconds\", \"300\", \"Request Timeout (seconds)\")\n",
    "dbutils.widgets.text(\"max_applications\", \"50\", \"Max Applications to Analyze per Cluster\")\n",
    "dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"prod\"], \"Environment (dev/prod)\")\n",
    "dbutils.widgets.text(\"s3_output_path\", \"\", \"S3 Output Path (prod only)\")\n",
    "\n",
    "# Updated: cluster_states dropdown with new options\n",
    "dbutils.widgets.dropdown(\"cluster_states\", \"TERMINATED,WAITING\", [\"TERMINATED\", \"WAITING\", \"TERMINATED,WAITING\", \"ALL\"], \"EMR Cluster States to Analyze\")\n",
    "\n",
    "dbutils.widgets.text(\"cluster_name_filter\", \"\", \"Cluster Name Filter (optional - partial name match)\")\n",
    "dbutils.widgets.text(\"max_clusters\", \"5\", \"Max Clusters to Analyze\")\n",
    "\n",
    "# New: Widgets for CreatedAfter and CreatedBefore dates\n",
    "# Format: YYYY-MM-DD. Leave blank for no date filter.\n",
    "dbutils.widgets.text(\"created_after_date\", \"\", \"EMR Clusters Created After (YYYY-MM-DD)\")\n",
    "dbutils.widgets.text(\"created_before_date\", \"\", \"EMR Clusters Created Before (YYYY-MM-DD)\")\n",
    "\n",
    "# New: Parameters for timeout and batch processing\n",
    "dbutils.widgets.text(\"persistent_ui_timeout_seconds\", \"180\", \"Persistent App UI Timeout (seconds)\")\n",
    "dbutils.widgets.text(\"batch_size\", \"50\", \"Batch Size (clusters to process concurrently)\") \n",
    "dbutils.widgets.text(\"batch_delay_seconds\", \"1800\", \"Delay Between Batches (seconds)\")\n",
    "\n",
    "\n",
    "AWS_REGION = dbutils.widgets.get(\"aws_region\").strip() or \"us-east-1\"\n",
    "EMR_CLUSTER_ARN = dbutils.widgets.get(\"emr_cluster_arn\").strip()\n",
    "TIMEOUT_SECONDS = int(dbutils.widgets.get(\"timeout_seconds\") or \"300\")\n",
    "MAX_APPLICATIONS = int(dbutils.widgets.get(\"max_applications\") or \"50\")\n",
    "CLUSTER_STATES = dbutils.widgets.get(\"cluster_states\").strip()\n",
    "CLUSTER_NAME_FILTER = dbutils.widgets.get(\"cluster_name_filter\").strip()\n",
    "MAX_CLUSTERS = int(dbutils.widgets.get(\"max_clusters\") or \"5\")\n",
    "ENVIRONMENT = dbutils.widgets.get(\"environment\").strip()\n",
    "S3_OUTPUT_PATH = dbutils.widgets.get(\"s3_output_path\").strip()\n",
    "PERSISTENT_UI_TIMEOUT_SECONDS = int(dbutils.widgets.get(\"persistent_ui_timeout_seconds\") or \"180\")\n",
    "BATCH_SIZE = int(dbutils.widgets.get(\"batch_size\") or \"3\")\n",
    "BATCH_DELAY_SECONDS = int(dbutils.widgets.get(\"batch_delay_seconds\") or \"60\") \n",
    "\n",
    "\n",
    "# Date parameters\n",
    "if ENVIRONMENT == \"dev\":\n",
    "    CREATED_AFTER_DATE_STR = dbutils.widgets.get(\"created_after_date\").strip()\n",
    "    CREATED_BEFORE_DATE_STR = dbutils.widgets.get(\"created_before_date\").strip()\n",
    "    PARSED_CREATED_AFTER_DATE = None\n",
    "    if CREATED_AFTER_DATE_STR:\n",
    "        try:\n",
    "            PARSED_CREATED_AFTER_DATE = datetime.datetime.strptime(CREATED_AFTER_DATE_STR, \"%Y-%m-%d\")\n",
    "        except ValueError as e:\n",
    "            logger.error(\"❌ Invalid format for created_after_date: %s. Expected YYYY-MM-DD.\", CREATED_AFTER_DATE_STR, exc_info=True)\n",
    "            raise ValueError(f\"Invalid format for created_after_date: {CREATED_AFTER_DATE_STR}. Expected YYYY-MM-DD.\") from e\n",
    "    PARSED_CREATED_BEFORE_DATE = None\n",
    "    if CREATED_BEFORE_DATE_STR:\n",
    "        try:\n",
    "            PARSED_CREATED_BEFORE_DATE = datetime.datetime.strptime(CREATED_BEFORE_DATE_STR, \"%Y-%m-%d\") + timedelta(days=1, seconds=-1)\n",
    "        except ValueError as e:\n",
    "            logger.error(\"❌ Invalid format for created_before_date: %s. Expected YYYY-MM-DD.\", CREATED_BEFORE_DATE_STR, exc_info=True)\n",
    "            raise ValueError(f\"Invalid format for created_before_date: {CREATED_BEFORE_DATE_STR}. Expected YYYY-MM-DD.\") from e\n",
    "    # Validate date range\n",
    "    if PARSED_CREATED_AFTER_DATE and PARSED_CREATED_BEFORE_DATE and PARSED_CREATED_AFTER_DATE >= PARSED_CREATED_BEFORE_DATE:\n",
    "        logger.error(\"❌ created_after_date (%s) cannot be on or after created_before_date (%s).\", CREATED_AFTER_DATE_STR, CREATED_BEFORE_DATE_STR, exc_info=True)\n",
    "        raise ValueError(\"created_after_date cannot be on or after created_before_date.\")\n",
    "else:\n",
    "    # In prod, analyze only the last 24 hours\n",
    "    PARSED_CREATED_BEFORE_DATE = datetime.datetime.now()\n",
    "    PARSED_CREATED_AFTER_DATE = PARSED_CREATED_BEFORE_DATE - timedelta(days=1)\n",
    "    CREATED_AFTER_DATE_STR = PARSED_CREATED_AFTER_DATE.strftime(\"%Y-%m-%d\")\n",
    "    CREATED_BEFORE_DATE_STR = PARSED_CREATED_BEFORE_DATE.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# Updated: Parse cluster states based on the new dropdown options\n",
    "if CLUSTER_STATES == \"ALL\":\n",
    "    # Comprehensive list of all EMR cluster states for 'ALL' option\n",
    "    CLUSTER_STATES_LIST = [\n",
    "        'STARTING', 'BOOTSTRAPPING', 'RUNNING', 'WAITING',\n",
    "        'TERMINATING', 'TERMINATED', 'TERMINATED_WITH_ERRORS'\n",
    "    ]\n",
    "elif CLUSTER_STATES == \"TERMINATED\":\n",
    "    CLUSTER_STATES_LIST = ['TERMINATED']\n",
    "elif CLUSTER_STATES == \"WAITING\":\n",
    "    CLUSTER_STATES_LIST = ['WAITING']\n",
    "elif CLUSTER_STATES == \"TERMINATED,WAITING\":\n",
    "    CLUSTER_STATES_LIST = ['TERMINATED', 'WAITING']\n",
    "else:\n",
    "    # Default to TERMINATED and WAITING for any unexpected value\n",
    "    CLUSTER_STATES_LIST = ['TERMINATED', 'WAITING']\n",
    "    logger.warning(\"Invalid cluster_states value '%s'. Defaulting to ['TERMINATED', 'WAITING'].\", CLUSTER_STATES)\n",
    "\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Environment: {ENVIRONMENT}\")\n",
    "print(f\"  AWS Region: {AWS_REGION}\")\n",
    "print(f\"  EMR Cluster ARN: {EMR_CLUSTER_ARN or 'Auto-discover clusters'}\")\n",
    "print(f\"  Timeout: {TIMEOUT_SECONDS} seconds\")\n",
    "print(f\"  Max Applications per Cluster: {MAX_APPLICATIONS}\")\n",
    "print(f\"  Cluster States to Analyze: {CLUSTER_STATES_LIST}\")\n",
    "print(f\"  Cluster Name Filter: {CLUSTER_NAME_FILTER or 'None'}\")\n",
    "print(f\"  Max Clusters to Analyze: {MAX_CLUSTERS}\")\n",
    "print(f\"  Created After Date: {CREATED_AFTER_DATE_STR or 'None'}\")\n",
    "print(f\"  Created Before Date: {CREATED_BEFORE_DATE_STR or 'None'}\")\n",
    "print(f\"  S3 Output Path: {S3_OUTPUT_PATH or 'None (dev mode or not specified)'}\")\n",
    "print(f\"  Persistent UI Timeout: {PERSISTENT_UI_TIMEOUT_SECONDS} seconds\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE} clusters\")\n",
    "print(f\"  Batch Delay: {BATCH_DELAY_SECONDS} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b40a36f5-72d3-41c5-aeb9-1149bdba19f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### AWS Boto3 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1ba9040d-56ae-4fcf-99ed-640ee71fe760",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Boto3 Helper Functions"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EMR Persistent App UI Client\n",
    "This module provides functionality to create an EMR Persistent App UI, retrieve its details and presigned URL,\n",
    "and establish an HTTP session with proper cookie management for Spark History Server access.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ServerConfig:\n",
    "    \"\"\"Configuration class for EMR Persistent UI client.\"\"\"\n",
    "\n",
    "    def __init__(self, emr_cluster_arn: str, timeout: int = 300):\n",
    "        \"\"\"\n",
    "        Initialize ServerConfig.\n",
    "\n",
    "        :param emr_cluster_arn: The EMR cluster ARN\n",
    "        :param timeout: Request timeout in seconds\n",
    "        :raises ValueError: If emr_cluster_arn is invalid\n",
    "        \"\"\"\n",
    "        if not emr_cluster_arn or not emr_cluster_arn.startswith(\"arn:aws:elasticmapreduce:\"):\n",
    "            raise ValueError(\"Invalid EMR cluster ARN format\")\n",
    "        self.emr_cluster_arn = emr_cluster_arn\n",
    "        self.timeout = timeout\n",
    "\n",
    "\n",
    "class EMRPersistentUIClient:\n",
    "    \"\"\"Client for managing EMR Persistent App UI and HTTP sessions.\"\"\"\n",
    "\n",
    "    def __init__(self, server_config: ServerConfig):\n",
    "        \"\"\"\n",
    "        Initialize the EMR client.\n",
    "\n",
    "        :param server_config: ServerConfig object\n",
    "        \"\"\"\n",
    "        self.emr_cluster_arn = server_config.emr_cluster_arn\n",
    "        self.region = self.emr_cluster_arn.split(\":\")[3] # Extract region from ARN\n",
    "        # Initialize boto3 client with credentials\n",
    "        self.emr_client = boto3.client(\n",
    "            \"emr\",\n",
    "            region_name=self.region\n",
    "        )\n",
    "        self.session = requests.Session()\n",
    "        self.persistent_ui_id: Optional[str] = None\n",
    "        self.presigned_url: Optional[str] = None\n",
    "        self.base_url: Optional[str] = None\n",
    "        self.timeout: int = server_config.timeout\n",
    "        self.presigned_url_ready: bool = False\n",
    "\n",
    "    def create_persistent_app_ui(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Create a persistent app UI for the given cluster.\n",
    "\n",
    "        :returns: Response from create-persistent-app-ui API call\n",
    "        :raises ClientError: If the API call fails\n",
    "        \"\"\"\n",
    "        logger.info(\"Creating persistent app UI for cluster: %s\", self.emr_cluster_arn)\n",
    "        try:\n",
    "            response = self.emr_client.create_persistent_app_ui(\n",
    "                TargetResourceArn=self.emr_cluster_arn\n",
    "            )\n",
    "            self.persistent_ui_id = response.get(\"PersistentAppUIId\")\n",
    "            runtime_role_enabled = response.get(\"RuntimeRoleEnabledCluster\", False)\n",
    "            logger.info(\"✅ Persistent App UI created successfully\")\n",
    "            logger.info(\" Persistent UI ID: %s\", self.persistent_ui_id)\n",
    "            logger.info(\" Runtime Role Enabled: %s\", runtime_role_enabled)\n",
    "            return response\n",
    "        except ClientError as e:\n",
    "            error_code = e.response[\"Error\"][\"Code\"]\n",
    "            error_message = e.response[\"Error\"][\"Message\"]\n",
    "            logger.error(\n",
    "                \"❌ Failed to create persistent app UI: %s - %s\", error_code, error_message, exc_info=True\n",
    "            )\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error creating persistent app UI: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def get_presigned_url(self, ui_type: str = \"SHS\") -> str:\n",
    "        \"\"\"\n",
    "        Get presigned URL for the persistent app UI.\n",
    "\n",
    "        :param ui_type: Type of UI ('SHS' for Spark History Server)\n",
    "        :returns: Presigned URL string\n",
    "        :raises ValueError: If no persistent UI ID is available\n",
    "        :raises ClientError: If the API call fails\n",
    "        \"\"\"\n",
    "        if not self.persistent_ui_id:\n",
    "            raise ValueError(\"No persistent UI ID available. Create one first.\")\n",
    "        logger.info(\n",
    "            \"Getting presigned URL for persistent app UI: %s (type: %s)\", self.persistent_ui_id, ui_type\n",
    "        )\n",
    "        try:\n",
    "            response = self.emr_client.get_persistent_app_ui_presigned_url(\n",
    "                PersistentAppUIId=self.persistent_ui_id,\n",
    "                PersistentAppUIType=ui_type\n",
    "            )\n",
    "            self.presigned_url_ready = response.get(\"PresignedURLReady\")\n",
    "            self.presigned_url = response.get(\"PresignedURL\")\n",
    "            # Extract base URL from presigned URL\n",
    "            parsed_url = urlparse(self.presigned_url)\n",
    "            self.base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/shs\"\n",
    "            logger.info(\"✅ Presigned URL obtained successfully\")\n",
    "            logger.info(\" Base URL: %s\", self.base_url)\n",
    "            return self.presigned_url\n",
    "        except ClientError as e:\n",
    "            error_code = e.response[\"Error\"][\"Code\"]\n",
    "            error_message = e.response[\"Error\"][\"Message\"]\n",
    "            logger.error(\n",
    "                \"❌ Failed to get presigned URL: %s - %s\", error_code, error_message, exc_info=True\n",
    "            )\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error getting presigned URL: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def setup_http_session(self) -> requests.Session:\n",
    "        \"\"\"\n",
    "        Set up HTTP session with proper headers and cookie management.\n",
    "\n",
    "        :returns: Configured requests.Session object\n",
    "        :raises ValueError: If no presigned URL is available\n",
    "        \"\"\"\n",
    "        if not self.presigned_url:\n",
    "            raise ValueError(\"No presigned URL available. Get one first.\")\n",
    "        logger.info(\"Setting up HTTP session with cookie management\")\n",
    "        # Configure session with appropriate headers\n",
    "        self.session.headers.update(\n",
    "            {\n",
    "                \"User-Agent\": \"EMR-Persistent-UI-Client/1.0\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "                \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "                \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Upgrade-Insecure-Requests\": \"1\"\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            # Make initial request to establish session and get cookies\n",
    "            logger.info(\"Making initial request\")\n",
    "            response = self.session.get(\n",
    "                self.presigned_url, timeout=self.timeout, allow_redirects=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            logger.info(\"✅ HTTP session established successfully\")\n",
    "            logger.info(\" Status Code: %s\", response.status_code)\n",
    "            logger.info(\" Cookies: %s cookie(s) stored\", len(self.session.cookies))\n",
    "            # Log cookie details (without sensitive values)\n",
    "            for cookie in self.session.cookies:\n",
    "                logger.debug(\" Cookie: %s (domain: %s)\", cookie.name, cookie.domain)\n",
    "            return self.session\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(\"❌ Failed to establish HTTP session: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error setting up HTTP session: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def initialize(self, max_wait_time: int) -> Tuple[str, requests.Session]:\n",
    "        \"\"\"\n",
    "        Initialize the EMR Persistent UI client by creating a persistent app UI,\n",
    "        polling until it is ready, getting a presigned URL, and setting up an HTTP session.\n",
    "\n",
    "        :param max_wait_time: The maximum time in seconds to wait for the persistent UI to become ready.\n",
    "        :returns: Tuple containing the base URL and configured session\n",
    "        :raises ValueError: If the persistent UI does not become ready within the timeout period\n",
    "        \"\"\"\n",
    "        # Step 1: Create persistent app UI\n",
    "        self.create_persistent_app_ui()\n",
    "\n",
    "        # Step 2: Poll for the presigned URL until it's ready\n",
    "        wait_interval = 10  # Check every 10 seconds\n",
    "        total_waited = 0\n",
    "        url_is_ready = False\n",
    "\n",
    "        logger.info(\"Waiting for Persistent App UI to become ready...\")\n",
    "\n",
    "        while total_waited < max_wait_time:\n",
    "            try:\n",
    "                # Directly call the boto3 client to check the ready status without failing\n",
    "                response = self.emr_client.get_persistent_app_ui_presigned_url(\n",
    "                    PersistentAppUIId=self.persistent_ui_id, PersistentAppUIType=\"SHS\"\n",
    "                )\n",
    "                url_is_ready = response.get(\"PresignedURLReady\", False)\n",
    "            except ClientError as e:\n",
    "                # This can happen if the UI is not yet fully initialized; we just wait and retry.\n",
    "                logger.warning(\n",
    "                    \"Could not check for presigned URL, will retry. Error: %s\", str(e)\n",
    "                )\n",
    "\n",
    "            if url_is_ready:\n",
    "                logger.info(\"✅ Persistent App UI is ready.\")\n",
    "                break\n",
    "\n",
    "            logger.info(\n",
    "                \"Persistent App UI not ready yet. Waiting %s seconds before retrying...\",\n",
    "                wait_interval\n",
    "            )\n",
    "            time.sleep(wait_interval)\n",
    "            total_waited += wait_interval\n",
    "\n",
    "        # After waiting, check if we succeeded\n",
    "        if not url_is_ready:\n",
    "            raise ValueError(\n",
    "                f\"Persistent App UI did not become ready after waiting {total_waited} seconds.\"\n",
    "            )\n",
    "\n",
    "        # Step 3: Get the actual presigned URL and setup its properties (now that we know it's ready)\n",
    "        self.get_presigned_url()\n",
    "\n",
    "        # Step 4: Setup HTTP session\n",
    "        self.setup_http_session()\n",
    "\n",
    "        return self.base_url, self.session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bde7f2f-4a63-4200-a05c-d5fa4ed277eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### EMR Cluster Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bf723b93-c2a3-40f3-bb86-6e33e675264e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EMR Cluster Discovery"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional # Added Optional\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "class EMRClusterDiscovery:\n",
    "    \"\"\"Discovery and management of EMR clusters.\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, region: str):\n",
    "        \"\"\"\n",
    "        Initialize the EMR cluster discovery client.\n",
    "\n",
    "\n",
    "        :param region: AWS region\n",
    "        :raises TypeError: If region is not a non-empty string.\n",
    "        \"\"\"\n",
    "        if not isinstance(region, str) or not region:\n",
    "            raise TypeError(\"AWS region must be a non-empty string.\")\n",
    "        self.region = region\n",
    "        self.emr_client = boto3.client(\"emr\", region_name=region)\n",
    "\n",
    "\n",
    "    def discover_clusters(self,\n",
    "                          states: Optional[List[str]] = None,\n",
    "                          name_filter: Optional[str] = None,\n",
    "                          max_clusters: int = 10,\n",
    "                          created_after: Optional[datetime] = None,   # Modified: New parameter\n",
    "                          created_before: Optional[datetime] = None) -> List[Dict]: # Modified: New parameter\n",
    "        \"\"\"\n",
    "        Discover EMR clusters based on criteria.\n",
    "\n",
    "\n",
    "        :param states: List of cluster states to filter by. Defaults to ['TERMINATED', 'WAITING'].\n",
    "                       Refer to boto3 EMR documentation for valid states.\n",
    "        :param name_filter: Partial name to filter clusters. Optional.\n",
    "        :param max_clusters: Maximum number of clusters to return. Defaults to 10.\n",
    "        :param created_after: datetime object. Only return clusters created after this time. Optional.\n",
    "        :param created_before: datetime object. Only return clusters created before this time. Optional.\n",
    "        :returns: List of cluster summaries, each a dictionary containing cluster metadata.\n",
    "        :raises TypeError: If inputs are not of the correct type.\n",
    "        :raises ValueError: If input values are out of acceptable ranges (e.g., max_clusters < 1).\n",
    "        :raises ClientError: If AWS API call fails.\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        if not isinstance(states, (list, type(None))):\n",
    "            raise TypeError(\":param states: must be a list of strings or None.\")\n",
    "        if states:\n",
    "            for state in states:\n",
    "                if not isinstance(state, str):\n",
    "                    raise TypeError(\"Each state in the :param states: list must be a string.\")\n",
    "        if not isinstance(name_filter, (str, type(None))):\n",
    "            raise TypeError(\":param name_filter: must be a string or None.\")\n",
    "        if not isinstance(max_clusters, int):\n",
    "            raise TypeError(\":param max_clusters: must be an integer.\")\n",
    "        if max_clusters < 1:\n",
    "            raise ValueError(\":param max_clusters: must be at least 1.\")\n",
    "        # Corrected: Input validation for datetime objects\n",
    "        if created_after is not None and not isinstance(created_after, datetime):\n",
    "            raise TypeError(\":param created_after: must be a datetime object or None.\")\n",
    "        if created_before is not None and not isinstance(created_before, datetime):\n",
    "            raise TypeError(\":param created_before: must be a datetime object or None.\")\n",
    "        if created_after and created_before and created_after >= created_before:\n",
    "            raise ValueError(\":param created_after: date cannot be on or after :param created_before: date.\")\n",
    "\n",
    "\n",
    "\n",
    "        logger.info(\"\uD83D\uDD0D Discovering EMR clusters in region: %s\", self.region)\n",
    "        logger.info(\" States: %s\", states)\n",
    "        logger.info(\" Name filter: %s\", name_filter or \"None\")\n",
    "        logger.info(\" Max clusters: %s\", max_clusters)\n",
    "        logger.info(\" Created After: %s\", created_after or \"None\")\n",
    "        logger.info(\" Created Before: %s\", created_before or \"None\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Build parameters for list_clusters\n",
    "            list_clusters_params = {}\n",
    "            if states:\n",
    "                list_clusters_params['ClusterStates'] = states\n",
    "            else:\n",
    "                list_clusters_params['ClusterStates'] = ['TERMINATED', 'WAITING'] # Default as per new requirement\n",
    "            if created_after:\n",
    "                list_clusters_params['CreatedAfter'] = created_after\n",
    "            if created_before:\n",
    "                list_clusters_params['CreatedBefore'] = created_before\n",
    "\n",
    "\n",
    "            # Use paginator to handle large cluster lists\n",
    "            paginator = self.emr_client.get_paginator('list_clusters')\n",
    "            page_iterator = paginator.paginate(**list_clusters_params)\n",
    "\n",
    "\n",
    "\n",
    "            discovered_clusters = []\n",
    "            for page in page_iterator:\n",
    "                for cluster in page.get('Clusters', []):\n",
    "                    # Apply name filter if specified\n",
    "                    if name_filter and name_filter.lower() not in cluster.get('Name', '').lower():\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    cluster_info = {\n",
    "                        'cluster_id': cluster.get('Id'),\n",
    "                        'cluster_name': cluster.get('Name'),\n",
    "                        'cluster_arn': cluster.get('ClusterArn'),\n",
    "                        'status': cluster.get('Status', {}).get('State'),\n",
    "                        'creation_time': cluster.get('Status', {}).get('Timeline', {}).get('CreationDateTime'),\n",
    "                        'normalized_instance_hours': cluster.get('NormalizedInstanceHours', 0)\n",
    "                    }\n",
    "                    discovered_clusters.append(cluster_info)\n",
    "\n",
    "\n",
    "                    # Stop if we've reached max clusters\n",
    "                    if len(discovered_clusters) >= max_clusters:\n",
    "                        break\n",
    "                if len(discovered_clusters) >= max_clusters:\n",
    "                    break\n",
    "\n",
    "\n",
    "            logger.info(\"✅ Discovered %s clusters\", len(discovered_clusters))\n",
    "            # Log cluster details\n",
    "            for i, cluster in enumerate(discovered_clusters, 1):\n",
    "                logger.info(\" %d. %s (%s) - %s\", i, cluster['cluster_name'], cluster['cluster_id'], cluster['status'])\n",
    "            return discovered_clusters\n",
    "\n",
    "\n",
    "        except ClientError as e:\n",
    "            logger.error(\"❌ Failed to discover clusters: %s - %s\", e.response[\"Error\"][\"Code\"], e.response[\"Error\"][\"Message\"], exc_info=True)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error during cluster discovery: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "\n",
    "    def get_cluster_details(self, cluster_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get detailed information about a specific cluster.\n",
    "\n",
    "\n",
    "        :param cluster_id: EMR cluster ID.\n",
    "        :returns: Detailed cluster information dictionary.\n",
    "        :raises TypeError: If cluster_id is not a string.\n",
    "        :raises ClientError: If AWS API call fails.\n",
    "        \"\"\"\n",
    "        if not isinstance(cluster_id, str) or not cluster_id:\n",
    "            raise TypeError(\"Cluster ID must be a non-empty string.\")\n",
    "        try:\n",
    "            response = self.emr_client.describe_cluster(ClusterId=cluster_id)\n",
    "            cluster = response.get('Cluster', {})\n",
    "\n",
    "\n",
    "            cluster_details = {\n",
    "                'cluster_id': cluster.get('Id'),\n",
    "                'cluster_name': cluster.get('Name'),\n",
    "                'cluster_arn': cluster.get('ClusterArn'),\n",
    "                'status': cluster.get('Status', {}).get('State'),\n",
    "                'emr_release_label': cluster.get('ReleaseLabel'),\n",
    "                'applications': [app.get('Name') for app in cluster.get('Applications', [])],\n",
    "                'instance_count': 0,\n",
    "                'master_instance_type': None,\n",
    "                'core_instance_type': None,\n",
    "                'log_uri': cluster.get('LogUri'),\n",
    "                'ec2_attributes': cluster.get('Ec2InstanceAttributes', {}),\n",
    "                'creation_time': cluster.get('Status', {}).get('Timeline', {}).get('CreationDateTime'),\n",
    "                'ready_time': cluster.get('Status', {}).get('Timeline', {}).get('ReadyDateTime'),\n",
    "                'normalized_instance_hours': cluster.get('NormalizedInstanceHours', 0)\n",
    "            }\n",
    "\n",
    "\n",
    "            # Attempt to get instance group information first\n",
    "            try:\n",
    "                instance_groups = self.emr_client.list_instance_groups(ClusterId=cluster_id)\n",
    "                for group in instance_groups.get('InstanceGroups', []):\n",
    "                    instance_type = group.get('InstanceType')\n",
    "                    running_count = group.get('RunningInstanceCount', 0)\n",
    "                    cluster_details['instance_count'] += running_count\n",
    "                    if group.get('InstanceGroupType') == 'MASTER':\n",
    "                        cluster_details['master_instance_type'] = instance_type\n",
    "                    elif group.get('InstanceGroupType') == 'CORE':\n",
    "                        cluster_details['core_instance_type'] = instance_type\n",
    "                logger.info(\"Successfully retrieved instance group info for %s\", cluster_id)\n",
    "            except ClientError as ce:\n",
    "                # Check if the error is specifically due to instance fleets being used\n",
    "                if (ce.response[\"Error\"][\"Code\"] == \"InvalidRequestException\" and\n",
    "                        \"Instance fleets and instance groups are mutually exclusive\" in ce.response[\"Error\"][\"Message\"]):\n",
    "                    logger.info(\"Cluster %s uses instance fleets. Fetching instance fleet details.\", cluster_id)\n",
    "                    # If it uses instance fleets, try to get instance fleet information\n",
    "                    try:\n",
    "                        instance_fleets = self.emr_client.list_instance_fleets(ClusterId=cluster_id)\n",
    "                        for fleet in instance_fleets.get('InstanceFleets', []):\n",
    "                            # Sum up the target capacities for total instances\n",
    "                            target_on_demand = fleet.get('TargetOnDemandCapacity', 0)\n",
    "                            target_spot = fleet.get('TargetSpotCapacity', 0)\n",
    "                            cluster_details['instance_count'] += target_on_demand + target_spot\n",
    "\n",
    "\n",
    "                            # Extract master/core instance types from fleet details\n",
    "                            # InstanceTypeSpecifications is a LIST, not a dict\n",
    "                            instance_type_specs = fleet.get('InstanceTypeSpecifications', [])\n",
    "                            if fleet.get('InstanceFleetType') == 'MASTER' and instance_type_specs:\n",
    "                                cluster_details['master_instance_type'] = instance_type_specs[0].get('InstanceType')\n",
    "                            elif fleet.get('InstanceFleetType') == 'CORE' and instance_type_specs:\n",
    "                                cluster_details['core_instance_type'] = instance_type_specs[0].get('InstanceType')\n",
    "                        logger.info(\"Successfully retrieved instance fleet info for %s\", cluster_id)\n",
    "                    except ClientError as fleet_ce:\n",
    "                        logger.warning(\"Could not get instance fleet details for %s: %s - %s\",\n",
    "                                       cluster_id, fleet_ce.response[\"Error\"][\"Code\"],\n",
    "                                       fleet_ce.response[\"Error\"][\"Message\"], exc_info=True)\n",
    "                    except Exception as fleet_e:\n",
    "                        logger.warning(\"Unexpected error getting instance fleet details for %s: %s\",\n",
    "                                       cluster_id, str(fleet_e), exc_info=True)\n",
    "                else:\n",
    "                    # Re-raise any other ClientErrors that are not the mutual exclusivity error\n",
    "                    logger.warning(\"Could not get instance group details for %s: %s - %s\",\n",
    "                                   cluster_id, ce.response[\"Error\"][\"Code\"],\n",
    "                                   ce.response[\"Error\"][\"Message\"], exc_info=True)\n",
    "                    raise # Re-raise original ClientError\n",
    "            except Exception as e:\n",
    "                # Catch any other general exceptions during initial instance group/fleet fetching\n",
    "                logger.warning(\"Unexpected error during instance details fetching for %s: %s\",\n",
    "                               cluster_id, str(e), exc_info=True)\n",
    "\n",
    "\n",
    "            return cluster_details\n",
    "\n",
    "\n",
    "        except ClientError as e:\n",
    "            logger.error(\"❌ Failed to describe cluster %s: %s - %s\",\n",
    "                         cluster_id, e.response[\"Error\"][\"Code\"],\n",
    "                         e.response[\"Error\"][\"Message\"], exc_info=True)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error getting cluster details for %s: %s\",\n",
    "                         cluster_id, str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "\n",
    "    def validate_cluster_for_analysis(self, cluster_info: Dict) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if a cluster is suitable for Spark History Server analysis.\n",
    "        A cluster is suitable if it has the Spark application installed and is in a state\n",
    "        where history data might be available.\n",
    "\n",
    "\n",
    "        :param cluster_info: Cluster information dictionary.\n",
    "        :returns: True if cluster is suitable for analysis, False otherwise.\n",
    "        :raises TypeError: If cluster_info is not a dictionary or has incorrect types.\n",
    "        \"\"\"\n",
    "        if not isinstance(cluster_info, dict):\n",
    "            raise TypeError(\":param cluster_info: must be a dictionary.\")\n",
    "\n",
    "\n",
    "        cluster_id = cluster_info.get('cluster_id')\n",
    "        status = cluster_info.get('status')\n",
    "        applications = cluster_info.get('applications', [])\n",
    "\n",
    "\n",
    "        if not isinstance(cluster_id, str):\n",
    "            raise TypeError(\"Cluster ID in cluster_info must be a string.\")\n",
    "        if not isinstance(status, str):\n",
    "            raise TypeError(\"Status in cluster_info must be a string.\")\n",
    "        if not isinstance(applications, list):\n",
    "            raise TypeError(\"Applications in cluster_info must be a list.\")\n",
    "        for app in applications:\n",
    "            if not isinstance(app, str):\n",
    "                raise TypeError(\"Each application name in cluster_info must be a string.\")\n",
    "\n",
    "\n",
    "        # Check if cluster has Spark application\n",
    "        has_spark = any('Spark' in app for app in applications)\n",
    "        if not has_spark:\n",
    "            logger.warning(\"⚠️ Cluster %s does not have Spark installed - skipping\", cluster_id)\n",
    "            return False\n",
    "\n",
    "\n",
    "        # Check if cluster is in a valid state for analysis.\n",
    "        # Even 'TERMINATED' clusters can have history data.\n",
    "        valid_states = ['RUNNING', 'WAITING', 'TERMINATED']\n",
    "        if status not in valid_states:\n",
    "            logger.warning(\"⚠️ Cluster %s is in state '%s' - may not have history data or is not a target state for analysis\", cluster_id, status)\n",
    "            return False # Exclude clusters not in RUNNING, WAITING, or TERMINATED\n",
    "\n",
    "\n",
    "        logger.info(\"✅ Cluster %s is valid for analysis (Spark: %s, State: %s)\", cluster_id, has_spark, status)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e8de55-97c8-47b7-aaba-b715e1e3a1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark History Server REST Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c9cf87-29db-4926-8873-0d8f70727cab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SHS REST Interactions"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Any, Dict, Optional\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SparkHistoryServerClient:\n",
    "    \"\"\"Client for interacting with Spark History Server REST API.\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str, session: requests.Session) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Spark History Server client.\n",
    "\n",
    "        :param base_url: Base URL for the Spark History Server\n",
    "        :param session: Configured HTTP session with authentication\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.session = session\n",
    "        self.api_base = f\"{base_url}/api/v1\"\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: Optional[Dict] = None) -> Any:\n",
    "        \"\"\"\n",
    "        Make a REST API request to the Spark History Server.\n",
    "\n",
    "        :param endpoint: API endpoint (relative to /api/v1)\n",
    "        :param params: Query parameters\n",
    "        :returns: JSON response\n",
    "        :raises requests.exceptions.RequestException: If request fails\n",
    "        :raises json.JSONDecodeError: If JSON parsing fails\n",
    "        \"\"\"\n",
    "        url = f\"{self.api_base}/{endpoint}\"\n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(\"❌ Failed to make request to %s: %s\", url, str(e), exc_info=True)\n",
    "            raise\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(\"❌ Failed to parse JSON response from %s: %s\", url, str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def get_applications(self, status: Optional[str] = None, limit: int = 100) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get list of Spark applications.\n",
    "\n",
    "        :param status: Filter by status (running, completed, failed)\n",
    "        :param limit: Maximum number of applications to return\n",
    "        :returns: List of application metadata\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching applications (status: %s, limit: %s)\", status, limit)\n",
    "        params = {}\n",
    "        if status:\n",
    "            params['status'] = status\n",
    "        if limit:\n",
    "            params['limit'] = limit\n",
    "        applications = self._make_request(\"applications\", params)\n",
    "        logger.info(\"✅ Retrieved %s applications\", len(applications))\n",
    "        return applications\n",
    "\n",
    "    def get_application_details(self, app_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get detailed information about a specific application.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :returns: Application details\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching application details for: %s\", app_id)\n",
    "        details = self._make_request(f\"applications/{app_id}\")\n",
    "        logger.info(\"✅ Retrieved details for application: %s\", app_id)\n",
    "        return details\n",
    "\n",
    "    def get_application_jobs(self, app_id: str, attempt_id: Optional[str] = None, status: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get jobs for a specific application with enhanced parameters.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param attempt_id: The application attempt ID. If None, uses the base application endpoint.\n",
    "        :param status: Filter by job status (running, succeeded, failed, unknown)\n",
    "        :returns: List of jobs with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching jobs for application: %s (Attempt ID: %s, Status: %s)\", app_id, attempt_id or \"N/A\", status or \"All\")\n",
    "        endpoint = f\"applications/{app_id}\"\n",
    "        if attempt_id:\n",
    "            endpoint += f\"/{attempt_id}\"\n",
    "        endpoint += \"/jobs\"\n",
    "        \n",
    "        params = {}\n",
    "        if status:\n",
    "            params['status'] = status\n",
    "            \n",
    "        jobs = self._make_request(endpoint, params)\n",
    "        logger.info(\"✅ Retrieved %s jobs for application: %s\", len(jobs), app_id)\n",
    "        return jobs\n",
    "\n",
    "    def get_job_details(self, app_id: str, job_id: int, attempt_id: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Get detailed information about a specific job.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param job_id: Job ID\n",
    "        :param attempt_id: The application attempt ID\n",
    "        :returns: Job details with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching job details for job %s in application: %s\", job_id, app_id)\n",
    "        endpoint = f\"applications/{app_id}\"\n",
    "        if attempt_id:\n",
    "            endpoint += f\"/{attempt_id}\"\n",
    "        endpoint += f\"/jobs/{job_id}\"\n",
    "        \n",
    "        job_details = self._make_request(endpoint)\n",
    "        logger.info(\"✅ Retrieved job details for job %s\", job_id)\n",
    "        return job_details\n",
    "\n",
    "    def get_application_stages(self, app_id: str, attempt_id: Optional[str] = None, \n",
    "                             status: Optional[str] = None, details: bool = True, \n",
    "                             with_summaries: bool = True, task_status: Optional[List[str]] = None,\n",
    "                             quantiles: str = \"0.0,0.25,0.5,0.75,1.0\") -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get stages for a specific application with ALL available parameters and data.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param attempt_id: The application attempt ID\n",
    "        :param status: Filter by stage status (active, complete, pending, failed)\n",
    "        :param details: Include task data\n",
    "        :param with_summaries: Include task metrics distribution and executor metrics distribution\n",
    "        :param task_status: List of task statuses to filter (RUNNING, SUCCESS, FAILED, KILLED, PENDING)\n",
    "        :param quantiles: Quantiles for metric summaries\n",
    "        :returns: List of stages with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching stages for application: %s (Attempt ID: %s)\", app_id, attempt_id or \"N/A\")\n",
    "        endpoint = f\"applications/{app_id}\"\n",
    "        if attempt_id:\n",
    "            endpoint += f\"/{attempt_id}\"\n",
    "        endpoint += \"/stages\"\n",
    "        \n",
    "        params = {}\n",
    "        if status:\n",
    "            params['status'] = status\n",
    "        if details:\n",
    "            params['details'] = 'true'\n",
    "        if with_summaries:\n",
    "            params['withSummaries'] = 'true'\n",
    "            params['quantiles'] = quantiles\n",
    "        if task_status:\n",
    "            for ts in task_status:\n",
    "                params[f'taskStatus'] = ts  # This will create multiple taskStatus params\n",
    "                \n",
    "        stages = self._make_request(endpoint, params)\n",
    "        logger.info(\"✅ Retrieved %s stages for application: %s\", len(stages), app_id)\n",
    "        return stages\n",
    "\n",
    "    def get_stage_details(self, app_id: str, stage_id: int, stage_attempt_id: int = 0,\n",
    "                         details: bool = True, with_summaries: bool = True,\n",
    "                         task_status: Optional[List[str]] = None,\n",
    "                         quantiles: str = \"0.0,0.25,0.5,0.75,1.0\") -> Dict:\n",
    "        \"\"\"\n",
    "        Get detailed information about a specific stage attempt.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param stage_id: Stage ID\n",
    "        :param stage_attempt_id: Stage attempt ID\n",
    "        :param details: Include task data\n",
    "        :param with_summaries: Include metrics distribution\n",
    "        :param task_status: List of task statuses to filter\n",
    "        :param quantiles: Quantiles for metric summaries\n",
    "        :returns: Stage details with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching stage details for stage %s/%s in application: %s\", stage_id, stage_attempt_id, app_id)\n",
    "        endpoint = f\"applications/{app_id}/stages/{stage_id}/{stage_attempt_id}\"\n",
    "        \n",
    "        params = {}\n",
    "        if details:\n",
    "            params['details'] = 'true'\n",
    "        if with_summaries:\n",
    "            params['withSummaries'] = 'true'\n",
    "            params['quantiles'] = quantiles\n",
    "        if task_status:\n",
    "            for ts in task_status:\n",
    "                params[f'taskStatus'] = ts\n",
    "                \n",
    "        stage_details = self._make_request(endpoint, params)\n",
    "        logger.info(\"✅ Retrieved stage details for stage %s/%s\", stage_id, stage_attempt_id)\n",
    "        return stage_details\n",
    "\n",
    "    def get_stage_tasks(self, app_id: str, stage_id: int, stage_attempt: int = 0,\n",
    "                       offset: Optional[int] = None, length: Optional[int] = None,\n",
    "                       sort_by: Optional[str] = None, status: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get tasks for a specific stage with ALL available parameters and fields.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param stage_id: Stage ID\n",
    "        :param stage_attempt: Stage attempt number\n",
    "        :param offset: Offset for pagination\n",
    "        :param length: Number of tasks to return\n",
    "        :param sort_by: Sort criteria (runtime, -runtime, etc.)\n",
    "        :param status: Filter by task status (running, success, killed, failed, unknown)\n",
    "        :returns: List of tasks with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching tasks for stage %s (attempt %s) in application: %s\", stage_id, stage_attempt, app_id)\n",
    "        endpoint = f\"applications/{app_id}/stages/{stage_id}/{stage_attempt}/taskList\"\n",
    "        \n",
    "        params = {}\n",
    "        if offset is not None:\n",
    "            params['offset'] = offset\n",
    "        if length is not None:\n",
    "            params['length'] = length\n",
    "        if sort_by:\n",
    "            params['sortBy'] = sort_by\n",
    "        if status:\n",
    "            params['status'] = status\n",
    "            \n",
    "        tasks = self._make_request(endpoint, params)\n",
    "        logger.info(\"✅ Retrieved %s tasks for stage %s\", len(tasks), stage_id)\n",
    "        return tasks\n",
    "\n",
    "    def get_stage_task_summary(self, app_id: str, stage_id: int, stage_attempt: int = 0,\n",
    "                              quantiles: str = \"0.0,0.25,0.5,0.75,1.0\") -> Dict:\n",
    "        \"\"\"\n",
    "        Get task summary metrics for a specific stage.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param stage_id: Stage ID\n",
    "        :param stage_attempt: Stage attempt number\n",
    "        :param quantiles: Quantiles for metric summaries\n",
    "        :returns: Task summary with ALL available metrics\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching task summary for stage %s (attempt %s) in application: %s\", stage_id, stage_attempt, app_id)\n",
    "        endpoint = f\"applications/{app_id}/stages/{stage_id}/{stage_attempt}/taskSummary\"\n",
    "        \n",
    "        params = {'quantiles': quantiles}\n",
    "        task_summary = self._make_request(endpoint, params)\n",
    "        logger.info(\"✅ Retrieved task summary for stage %s\", stage_id)\n",
    "        return task_summary\n",
    "\n",
    "    def get_application_executors(self, app_id: str, all_executors: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get executors for a specific application with ALL available fields.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param all_executors: If True, get all executors (active and dead), otherwise only active\n",
    "        :returns: List of executors with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching executors for application: %s (all: %s)\", app_id, all_executors)\n",
    "        endpoint = f\"applications/{app_id}/allexecutors\" if all_executors else f\"applications/{app_id}/executors\"\n",
    "        executors = self._make_request(endpoint)\n",
    "        logger.info(\"✅ Retrieved %s executors for application: %s\", len(executors), app_id)\n",
    "        return executors\n",
    "\n",
    "    def get_application_sql_queries(self, app_id: str, min_duration_minutes: int = 0, \n",
    "                                  attempt_id: Optional[str] = None, details: bool = True,\n",
    "                                  plan_description: bool = True, offset: Optional[int] = None,\n",
    "                                  length: Optional[int] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get SQL queries for a specific application with ALL available parameters and fields.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param min_duration_minutes: Minimum duration in minutes for SQL queries to include\n",
    "        :param attempt_id: The application attempt ID\n",
    "        :param details: Include details of Spark plan nodes\n",
    "        :param plan_description: Enable physical plan description\n",
    "        :param offset: Offset for pagination\n",
    "        :param length: Number of queries to return\n",
    "        :returns: List of SQL query data with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching SQL queries for application: %s (min_duration_minutes: %s, Attempt ID: %s)\", \n",
    "                   app_id, min_duration_minutes, attempt_id or \"N/A\")\n",
    "        endpoint = f\"applications/{app_id}\"\n",
    "        if attempt_id:\n",
    "            endpoint += f\"/{attempt_id}\"\n",
    "        endpoint += \"/sql\"\n",
    "        \n",
    "        params = {}\n",
    "        if not details:\n",
    "            params['details'] = 'false'\n",
    "        if not plan_description:\n",
    "            params['planDescription'] = 'false'\n",
    "        if offset is not None:\n",
    "            params['offset'] = offset\n",
    "        if length is not None:\n",
    "            params['length'] = length\n",
    "            \n",
    "        sql_queries = self._make_request(endpoint, params)\n",
    "        logger.info(\"✅ Retrieved %s raw SQL queries for application: %s\", len(sql_queries), app_id)\n",
    "\n",
    "        # Filter by duration and include the full JSON\n",
    "        filtered_queries = []\n",
    "        for query in sql_queries:\n",
    "            duration_ms = query.get(\"duration\", 0)\n",
    "            if duration_ms >= (min_duration_minutes * 60 * 1000):\n",
    "                filtered_queries.append(query)\n",
    "        \n",
    "        logger.info(\"✅ Filtered to %s SQL queries with duration >= %s minutes for application: %s\", \n",
    "                   len(filtered_queries), min_duration_minutes, app_id)\n",
    "        return filtered_queries\n",
    "\n",
    "    def get_sql_query_details(self, app_id: str, execution_id: int, details: bool = True,\n",
    "                             plan_description: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Get detailed information about a specific SQL query.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param execution_id: SQL execution ID\n",
    "        :param details: Include metric details\n",
    "        :param plan_description: Enable physical plan description\n",
    "        :returns: SQL query details with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching SQL query details for execution %s in application: %s\", execution_id, app_id)\n",
    "        endpoint = f\"applications/{app_id}/sql/{execution_id}\"\n",
    "        \n",
    "        params = {}\n",
    "        if not details:\n",
    "            params['details'] = 'false'\n",
    "        if not plan_description:\n",
    "            params['planDescription'] = 'false'\n",
    "            \n",
    "        query_details = self._make_request(endpoint, params)\n",
    "        logger.info(\"✅ Retrieved SQL query details for execution %s\", execution_id)\n",
    "        return query_details\n",
    "\n",
    "    def get_application_environment(self, app_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get environment details for a specific application.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :returns: Environment details with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching environment details for application: %s\", app_id)\n",
    "        environment = self._make_request(f\"applications/{app_id}/environment\")\n",
    "        logger.info(\"✅ Retrieved environment details for application: %s\", app_id)\n",
    "        return environment\n",
    "\n",
    "    def get_application_storage_rdd(self, app_id: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get RDD storage information for a specific application.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :returns: List of stored RDDs with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching RDD storage info for application: %s\", app_id)\n",
    "        rdd_storage = self._make_request(f\"applications/{app_id}/storage/rdd\")\n",
    "        logger.info(\"✅ Retrieved RDD storage info for application: %s\", app_id)\n",
    "        return rdd_storage\n",
    "\n",
    "    def get_rdd_storage_details(self, app_id: str, rdd_id: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Get detailed storage information for a specific RDD.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param rdd_id: RDD ID\n",
    "        :returns: RDD storage details with ALL available fields\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching RDD storage details for RDD %s in application: %s\", rdd_id, app_id)\n",
    "        rdd_details = self._make_request(f\"applications/{app_id}/storage/rdd/{rdd_id}\")\n",
    "        logger.info(\"✅ Retrieved RDD storage details for RDD %s\", rdd_id)\n",
    "        return rdd_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f69e3b9-a1fb-4d4f-89b6-a6e7f2838359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Metrics Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7d5750e9-6716-4a47-89a8-d4cc356a9d6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SHS Metric Analysis"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, ArrayType, MapType, IntegerType, BooleanType\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SparkMetricsAnalyzer:\n",
    "    \"\"\"Analyzer for Spark application metrics and performance data.\"\"\"\n",
    "\n",
    "    def __init__(self, spark: SparkSession) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the metrics analyzer.\n",
    "        :param spark: Spark session\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "\n",
    "    def _flatten_dict(self, d: Dict, parent_key: str = '', sep: str = '_') -> Dict:\n",
    "        \"\"\"\n",
    "        Flatten a nested dictionary.\n",
    "        \n",
    "        :param d: Dictionary to flatten\n",
    "        :param parent_key: Parent key prefix\n",
    "        :param sep: Separator for nested keys\n",
    "        :returns: Flattened dictionary\n",
    "        \"\"\"\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(self._flatten_dict(v, new_key, sep=sep).items())\n",
    "            elif isinstance(v, list):\n",
    "                # Convert lists to JSON strings for storage\n",
    "                items.append((new_key, json.dumps(v) if v else None))\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "    def analyze_application_performance(self, app_data: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for a single application, extracting ALL available fields.\n",
    "        \n",
    "        :param app_data: Application data from Spark History Server\n",
    "        :returns: List of comprehensive performance analysis results\n",
    "        \"\"\"\n",
    "        app_id = app_data.get('id')\n",
    "        app_name = app_data.get('name') or 'Unknown'\n",
    "        attempts = app_data.get('attempts', [])\n",
    "        \n",
    "        if not attempts:\n",
    "            return []\n",
    "\n",
    "        all_attempts_analysis = []\n",
    "        for attempt in attempts:\n",
    "            # Flatten the attempt data to capture ALL fields\n",
    "            flattened_attempt = self._flatten_dict(attempt)\n",
    "            \n",
    "            # Create comprehensive analysis with all available fields\n",
    "            analysis = {\n",
    "                'application_id': app_id,\n",
    "                'application_name': app_name,\n",
    "                'cluster_id': '',\n",
    "                'cluster_name': '',\n",
    "                'raw_json': json.dumps(app_data)  # Store complete JSON for reference\n",
    "            }\n",
    "            \n",
    "            # Add all flattened fields\n",
    "            analysis.update(flattened_attempt)\n",
    "            \n",
    "            all_attempts_analysis.append(analysis)\n",
    "        \n",
    "        return all_attempts_analysis\n",
    "\n",
    "    def analyze_job_performance(self, jobs: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for jobs, extracting ALL available fields.\n",
    "        \n",
    "        :param jobs: List of job data\n",
    "        :returns: Comprehensive job performance analysis\n",
    "        \"\"\"\n",
    "        job_analysis = []\n",
    "        for job in jobs:\n",
    "            # Flatten the job data to capture ALL fields\n",
    "            flattened_job = self._flatten_dict(job)\n",
    "            \n",
    "            analysis = {\n",
    "                'cluster_id': '',\n",
    "                'cluster_name': '',\n",
    "                'application_id': '',\n",
    "                'raw_json': json.dumps(job)  # Store complete JSON for reference\n",
    "            }\n",
    "            \n",
    "            # Add all flattened fields\n",
    "            analysis.update(flattened_job)\n",
    "            \n",
    "            job_analysis.append(analysis)\n",
    "        return job_analysis\n",
    "\n",
    "    def analyze_stage_performance(self, stages: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for stages, extracting ALL available fields.\n",
    "        \n",
    "        :param stages: List of stage data\n",
    "        :returns: Comprehensive stage performance analysis\n",
    "        \"\"\"\n",
    "        stage_analysis = []\n",
    "        for stage in stages:\n",
    "            # Flatten the stage data to capture ALL fields\n",
    "            flattened_stage = self._flatten_dict(stage)\n",
    "            \n",
    "            analysis = {\n",
    "                'cluster_id': '',\n",
    "                'cluster_name': '',\n",
    "                'application_id': '',\n",
    "                'raw_json': json.dumps(stage)  # Store complete JSON for reference\n",
    "            }\n",
    "            \n",
    "            # Add all flattened fields\n",
    "            analysis.update(flattened_stage)\n",
    "            \n",
    "            stage_analysis.append(analysis)\n",
    "        return stage_analysis\n",
    "\n",
    "    def analyze_task_performance(self, tasks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for tasks, extracting ALL available fields.\n",
    "        \n",
    "        :param tasks: List of task data\n",
    "        :returns: Comprehensive task performance analysis\n",
    "        \"\"\"\n",
    "        task_analysis = []\n",
    "        for task in tasks:\n",
    "            # Flatten the task data to capture ALL fields\n",
    "            flattened_task = self._flatten_dict(task)\n",
    "            \n",
    "            analysis = {\n",
    "                'cluster_id': '',\n",
    "                'cluster_name': '',\n",
    "                'application_id': '',\n",
    "                'stage_id': '',\n",
    "                'stage_attempt_id': '',\n",
    "                'raw_json': json.dumps(task)  # Store complete JSON for reference\n",
    "            }\n",
    "            \n",
    "            # Add all flattened fields\n",
    "            analysis.update(flattened_task)\n",
    "            \n",
    "            task_analysis.append(analysis)\n",
    "        return task_analysis\n",
    "\n",
    "    def analyze_executor_performance(self, executors: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for executors, extracting ALL available fields.\n",
    "        \n",
    "        :param executors: List of executor data\n",
    "        :returns: Comprehensive executor performance analysis\n",
    "        \"\"\"\n",
    "        executor_analysis = []\n",
    "        for executor in executors:\n",
    "            # Flatten the executor data to capture ALL fields\n",
    "            flattened_executor = self._flatten_dict(executor)\n",
    "            \n",
    "            analysis = {\n",
    "                'cluster_id': '',\n",
    "                'cluster_name': '',\n",
    "                'application_id': '',\n",
    "                'raw_json': json.dumps(executor)  # Store complete JSON for reference\n",
    "            }\n",
    "            \n",
    "            # Add all flattened fields\n",
    "            analysis.update(flattened_executor)\n",
    "            \n",
    "            executor_analysis.append(analysis)\n",
    "        return executor_analysis\n",
    "\n",
    "    def analyze_sql_queries(self, sql_queries: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze SQL query metrics, extracting ALL available fields.\n",
    "        \n",
    "        :param sql_queries: List of SQL query data\n",
    "        :returns: Comprehensive SQL query analysis\n",
    "        \"\"\"\n",
    "        sql_analysis = []\n",
    "        for query in sql_queries:\n",
    "            # Flatten the query data to capture ALL fields\n",
    "            flattened_query = self._flatten_dict(query)\n",
    "            \n",
    "            analysis = {\n",
    "                'cluster_id': '',\n",
    "                'cluster_name': '',\n",
    "                'application_id': '',\n",
    "                'raw_json': json.dumps(query)  # Store complete JSON for reference\n",
    "            }\n",
    "            \n",
    "            # Add all flattened fields\n",
    "            analysis.update(flattened_query)\n",
    "            \n",
    "            sql_analysis.append(analysis)\n",
    "        return sql_analysis\n",
    "\n",
    "    def create_dynamic_dataframes(\n",
    "        self,\n",
    "        applications_analysis: List[Dict],\n",
    "        jobs_analysis: List[Dict],\n",
    "        stages_analysis: List[Dict],\n",
    "        tasks_analysis: List[Dict],\n",
    "        sql_analysis: List[Dict],\n",
    "        executors_analysis: List[Dict]\n",
    "        ) -> Tuple[Optional[Any], Optional[Any], Optional[Any], Optional[Any], Optional[Any], Optional[Any]]:\n",
    "        \"\"\"\n",
    "        Create Spark DataFrames from analysis results with dynamic schemas based on actual data.\n",
    "        This approach captures ALL fields present in the JSON responses.\n",
    "        \n",
    "        :param applications_analysis: Application analysis results\n",
    "        :param jobs_analysis: Job analysis results\n",
    "        :param stages_analysis: Stage analysis results\n",
    "        :param tasks_analysis: Task analysis results\n",
    "        :param sql_analysis: SQL analysis results\n",
    "        :param executors_analysis: Executor analysis results\n",
    "        :returns: Tuple of DataFrames with comprehensive schemas\n",
    "        \"\"\"\n",
    "        def create_df_from_data(data: List[Dict], df_name: str):\n",
    "            if not data:\n",
    "                return None\n",
    "            try:\n",
    "                # Let Spark infer the schema from the data\n",
    "                df = self.spark.createDataFrame(data)\n",
    "                logger.info(\"✅ Created %s DataFrame with %s rows and %s columns\", \n",
    "                           df_name, df.count(), len(df.columns))\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                logger.error(\"❌ Failed to create %s DataFrame: %s\", df_name, str(e), exc_info=True)\n",
    "                return None\n",
    "\n",
    "        # Create DataFrames with dynamic schemas\n",
    "        apps_df = create_df_from_data(applications_analysis, \"applications\")\n",
    "        jobs_df = create_df_from_data(jobs_analysis, \"jobs\") \n",
    "        stages_df = create_df_from_data(stages_analysis, \"stages\")\n",
    "        tasks_df = create_df_from_data(tasks_analysis, \"tasks\")\n",
    "        sql_df = create_df_from_data(sql_analysis, \"sql\")\n",
    "        executors_df = create_df_from_data(executors_analysis, \"executors\")\n",
    "\n",
    "        return apps_df, jobs_df, stages_df, tasks_df, sql_df, executors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c220e39c-99f5-4106-92ce-853067e94ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cluster Analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "362467e8-99d1-4d8c-86a5-fabec728768f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EMR Cluster Analysis"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def analyze_single_cluster(\n",
    "    cluster_info: Dict,\n",
    "    shs_client_class: Any,\n",
    "    analyzer_class: Any,\n",
    "    timeout_seconds: int,\n",
    "    max_applications: int,\n",
    "    spark_session: 'SparkSession',\n",
    "    persistent_ui_timeout: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyzes a single EMR cluster, fetching application, job, stage, task, executor, and SQL query data.\n",
    "    Updated to process applications sequentially to avoid resource limits.\n",
    "    \"\"\"\n",
    "    cluster_id = cluster_info['cluster_id']\n",
    "    cluster_name = cluster_info['cluster_name']\n",
    "    cluster_arn = cluster_info['cluster_arn']\n",
    "    logger.info(\"\uD83D\uDD75️ Starting analysis for cluster: %s (%s)\", cluster_name, cluster_id)\n",
    "\n",
    "    cluster_analysis_results = {\n",
    "        'cluster_id': cluster_id, 'cluster_name': cluster_name,\n",
    "        'status': cluster_info.get('status', 'UNKNOWN'),\n",
    "        'total_applications': 0, 'total_jobs': 0, 'total_stages': 0, 'total_tasks': 0, 'total_sql_queries': 0, 'total_executors': 0,\n",
    "        'analysis_status': 'FAILED', 'applications': [], 'jobs': [], 'stages': [], 'tasks': [],\n",
    "        'sql_queries': [], 'executors': [], 'applications_endpoint_success': None,\n",
    "        'jobs_endpoint_success': None, 'sql_endpoint_success': None, 'executors_endpoint_success': None, 'error_message': \"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        server_config = ServerConfig(emr_cluster_arn=cluster_arn, timeout=timeout_seconds)\n",
    "        emr_client = EMRPersistentUIClient(server_config)\n",
    "        logger.info(\"\uD83D\uDD11 Initializing EMR Persistent UI connection for %s...\", cluster_name)\n",
    "        base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n",
    "        logger.info(\"\uD83D\uDCCA Setting up Spark History Server client for %s...\", cluster_name)\n",
    "        shs_client = shs_client_class(base_url, session)\n",
    "        analyzer = analyzer_class(spark_session)\n",
    "\n",
    "        logger.info(\"\uD83D\uDCC4 Fetching Spark applications for %s...\", cluster_name)\n",
    "        try:\n",
    "            applications = shs_client.get_applications(limit=max_applications)\n",
    "            cluster_analysis_results['applications_endpoint_success'] = True\n",
    "        except Exception as app_ex:\n",
    "            cluster_analysis_results['applications_endpoint_success'] = False\n",
    "            cluster_analysis_results['error_message'] += f\"Applications endpoint error: {str(app_ex)}; \"\n",
    "            logger.error(\"❌ Failed to fetch applications for %s: %s\", cluster_name, str(app_ex), exc_info=True)\n",
    "            return cluster_analysis_results\n",
    "\n",
    "        if not applications:\n",
    "            logger.warning(\"⚠️ No applications found in Spark History Server for %s\", cluster_name)\n",
    "            cluster_analysis_results['analysis_status'] = 'NO_APPLICATIONS'\n",
    "            return cluster_analysis_results\n",
    "\n",
    "        logger.info(\"✅ Found %s applications to analyze in %s\", len(applications), cluster_name)\n",
    "\n",
    "        # Process applications sequentially instead of concurrently\n",
    "        for app in applications:\n",
    "            app_id = app.get('id')\n",
    "            try:\n",
    "                app_data = process_single_application(app_id, shs_client, analyzer, cluster_analysis_results)\n",
    "                if app_data:\n",
    "                    cluster_analysis_results['applications'].extend(app_data['applications'])\n",
    "                    cluster_analysis_results['jobs'].extend(app_data['jobs'])\n",
    "                    cluster_analysis_results['stages'].extend(app_data['stages'])\n",
    "                    cluster_analysis_results['tasks'].extend(app_data['tasks'])  # ADD TASKS\n",
    "                    cluster_analysis_results['sql_queries'].extend(app_data['sql_queries'])\n",
    "                    cluster_analysis_results['executors'].extend(app_data['executors'])\n",
    "                    cluster_analysis_results['total_applications'] += 1\n",
    "                    cluster_analysis_results['total_jobs'] += len(app_data['jobs'])\n",
    "                    cluster_analysis_results['total_stages'] += len(app_data['stages'])\n",
    "                    cluster_analysis_results['total_tasks'] += len(app_data['tasks'])  # ADD TASKS COUNT\n",
    "                    cluster_analysis_results['total_sql_queries'] += len(app_data['sql_queries'])\n",
    "                    cluster_analysis_results['total_executors'] += len(app_data['executors'])\n",
    "            except Exception as app_e:\n",
    "                logger.error(\" ❌ Error analyzing application %s in cluster %s: %s\", app_id, cluster_name, str(app_e), exc_info=True)\n",
    "                continue # Continue with next application\n",
    "\n",
    "        cluster_analysis_results['analysis_status'] = 'COMPLETED'\n",
    "        logger.info(\"✅ Completed analysis for cluster: %s\", cluster_name)\n",
    "        return cluster_analysis_results\n",
    "    except Exception as e:\n",
    "        logger.error(\"❌ Failed to analyze cluster %s: %s\", cluster_name, str(e), exc_info=True)\n",
    "        cluster_analysis_results['analysis_status'] = 'FAILED'\n",
    "        cluster_analysis_results['error_message'] += f\"Cluster analysis error: {str(e)}\"\n",
    "        return cluster_analysis_results\n",
    "\n",
    "def process_single_application(app_id: str, shs_client: Any, analyzer: Any, cluster_results: Dict) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes a single Spark application's data with comprehensive data extraction.\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDD0D Analyzing application: %s\", app_id)\n",
    "    app_results = {'applications': [], 'jobs': [], 'stages': [], 'tasks': [], 'sql_queries': [], 'executors': []}\n",
    "\n",
    "    try:\n",
    "        app_details = shs_client.get_application_details(app_id)\n",
    "        latest_attempt_id_to_use = None\n",
    "        attempts = app_details.get('attempts')\n",
    "\n",
    "        if attempts and isinstance(attempts, list) and len(attempts) > 0:\n",
    "            app_results['applications'] = analyzer.analyze_application_performance(app_details)\n",
    "            attempt_ids = [int(a['attemptId']) for a in attempts if a.get('attemptId')]\n",
    "            if attempt_ids:\n",
    "                latest_attempt_id = max(attempt_ids)\n",
    "                if latest_attempt_id > 1:\n",
    "                    latest_attempt_id_to_use = str(latest_attempt_id)\n",
    "        else:\n",
    "            logger.warning(\"⚠️ Application %s has no attempts data. Skipping further analysis.\", app_id)\n",
    "            return None\n",
    "\n",
    "        # Enhanced Jobs Analysis\n",
    "        try:\n",
    "            jobs = shs_client.get_application_jobs(app_id, attempt_id=latest_attempt_id_to_use)\n",
    "            if jobs:\n",
    "                app_results['jobs'] = analyzer.analyze_job_performance(jobs)\n",
    "                # Get detailed job information for each job\n",
    "                for job in jobs:\n",
    "                    job_id = job.get('jobId')\n",
    "                    if job_id is not None:\n",
    "                        try:\n",
    "                            job_details = shs_client.get_job_details(app_id, job_id, latest_attempt_id_to_use)\n",
    "                            # Could extend job analysis with detailed data here\n",
    "                        except Exception as job_detail_ex:\n",
    "                            logger.warning(\"⚠️ Failed to fetch job details for job %s: %s\", job_id, str(job_detail_ex))\n",
    "\n",
    "            if cluster_results['jobs_endpoint_success'] is None:\n",
    "                cluster_results['jobs_endpoint_success'] = True\n",
    "        except Exception as job_ex:\n",
    "            if cluster_results['jobs_endpoint_success'] is None:\n",
    "                cluster_results['jobs_endpoint_success'] = False\n",
    "            cluster_results['error_message'] += f\"Jobs endpoint error: {str(job_ex)}; \"\n",
    "            logger.error(\" ❌ Failed to fetch jobs for application %s: %s\", app_id, str(job_ex), exc_info=True)\n",
    "\n",
    "        # Enhanced Stages Analysis with comprehensive data\n",
    "        try:\n",
    "            stages = shs_client.get_application_stages(\n",
    "                app_id,\n",
    "                attempt_id=latest_attempt_id_to_use,\n",
    "                details=True,\n",
    "                with_summaries=True,\n",
    "                quantiles=\"0.0,0.25,0.5,0.75,1.0\"\n",
    "            )\n",
    "\n",
    "            if stages:\n",
    "                app_results['stages'] = analyzer.analyze_stage_performance(stages)\n",
    "\n",
    "                # Get tasks for each stage\n",
    "                for stage in stages:\n",
    "                    stage_id = stage.get('stageId')\n",
    "                    stage_attempt_id = stage.get('attemptId', 0)\n",
    "                    if stage_id is not None:\n",
    "                        try:\n",
    "                            # Get all tasks for this stage\n",
    "                            tasks = shs_client.get_stage_tasks(app_id, stage_id, stage_attempt_id)\n",
    "                            if tasks:\n",
    "                                # Add stage context to tasks\n",
    "                                for task in tasks:\n",
    "                                    task['stage_id'] = stage_id\n",
    "                                    task['stage_attempt_id'] = stage_attempt_id\n",
    "                                task_analysis = analyzer.analyze_task_performance(tasks)\n",
    "                                app_results['tasks'].extend(task_analysis)\n",
    "\n",
    "                            # Get task summary\n",
    "                            task_summary = shs_client.get_stage_task_summary(app_id, stage_id, stage_attempt_id)\n",
    "                            # Could store task summary data separately if needed\n",
    "                        except Exception as task_ex:\n",
    "                            logger.warning(\"⚠️ Failed to fetch tasks for stage %s: %s\", stage_id, str(task_ex))\n",
    "\n",
    "        except Exception as stage_ex:\n",
    "            logger.error(\" ❌ Failed to fetch stages for application %s: %s\", app_id, str(stage_ex), exc_info=True)\n",
    "\n",
    "        # Enhanced SQL Analysis\n",
    "        try:\n",
    "            sql_queries = shs_client.get_application_sql_queries(\n",
    "                app_id,\n",
    "                attempt_id=latest_attempt_id_to_use,\n",
    "                details=True,\n",
    "                plan_description=True\n",
    "            )\n",
    "\n",
    "            if sql_queries:\n",
    "                app_results['sql_queries'] = analyzer.analyze_sql_queries(sql_queries)\n",
    "\n",
    "                # Get detailed information for each SQL query\n",
    "                for query in sql_queries:\n",
    "                    execution_id = query.get('id')\n",
    "                    if execution_id is not None:\n",
    "                        try:\n",
    "                            query_details = shs_client.get_sql_query_details(app_id, execution_id)\n",
    "                            # Could extend SQL analysis with detailed data here\n",
    "                        except Exception as sql_detail_ex:\n",
    "                            logger.warning(\"⚠️ Failed to fetch SQL query details for execution %s: %s\", execution_id, str(sql_detail_ex))\n",
    "\n",
    "            if cluster_results['sql_endpoint_success'] is None:\n",
    "                cluster_results['sql_endpoint_success'] = True\n",
    "        except Exception as sql_ex:\n",
    "            if cluster_results['sql_endpoint_success'] is None:\n",
    "                cluster_results['sql_endpoint_success'] = False\n",
    "            cluster_results['error_message'] += f\"SQL endpoint error: {str(sql_ex)}; \"\n",
    "            logger.error(\" ❌ Failed to fetch SQL queries for application %s: %s\", app_id, str(sql_ex), exc_info=True)\n",
    "\n",
    "        # Enhanced Executor Analysis\n",
    "        try:\n",
    "            executors = shs_client.get_application_executors(app_id, all_executors=True)\n",
    "            if executors:\n",
    "                app_results['executors'] = analyzer.analyze_executor_performance(executors)\n",
    "            if cluster_results['executors_endpoint_success'] is None:\n",
    "                cluster_results['executors_endpoint_success'] = True\n",
    "        except Exception as executor_ex:\n",
    "            if cluster_results['executors_endpoint_success'] is None:\n",
    "                cluster_results['executors_endpoint_success'] = False\n",
    "            cluster_results['error_message'] += f\"Executors endpoint error: {str(executor_ex)}; \"\n",
    "            logger.error(\" ❌ Failed to fetch executors for application %s: %s\", app_id, str(executor_ex), exc_info=True)\n",
    "\n",
    "        return app_results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\" ❌ Failed to analyze application %s: %s\", app_id, str(e), exc_info=True)\n",
    "        return None\n",
    "\n",
    "def process_clusters_in_batches(\n",
    "    clusters_to_analyze: List[Dict],\n",
    "    batch_size: int,\n",
    "    batch_delay_seconds: int,\n",
    "    analyzer_instance: Any,\n",
    "    spark_session: Any\n",
    ") -> Tuple[List[Dict], List[Dict], List[Dict], List[Dict], List[Dict], List[Dict], List[Dict]]:  # UPDATED RETURN TYPE\n",
    "    \"\"\"\n",
    "    Process clusters in sequential batches to avoid hitting the 200 Persistent UI limit.\n",
    "    \"\"\"\n",
    "    all_applications_analysis = []\n",
    "    all_jobs_analysis = []\n",
    "    all_stages_analysis = []\n",
    "    all_tasks_analysis = []  # ADDED TASKS\n",
    "    all_sql_analysis = []\n",
    "    all_executors_analysis = []\n",
    "    cluster_summaries = []\n",
    "\n",
    "    total_batches = (len(clusters_to_analyze) + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_num in range(0, len(clusters_to_analyze), batch_size):\n",
    "        batch_clusters = clusters_to_analyze[batch_num:batch_num + batch_size]\n",
    "        current_batch = (batch_num // batch_size) + 1\n",
    "\n",
    "        logger.info(\"\uD83D\uDD04 Processing batch %d/%d: clusters %d-%d\",\n",
    "                    current_batch, total_batches,\n",
    "                    batch_num + 1, min(batch_num + batch_size, len(clusters_to_analyze)))\n",
    "\n",
    "        # Process this batch with limited concurrency\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=len(batch_clusters)) as executor:\n",
    "            batch_futures = {\n",
    "                executor.submit(\n",
    "                    analyze_single_cluster,\n",
    "                    c_info,\n",
    "                    SparkHistoryServerClient,\n",
    "                    SparkMetricsAnalyzer,\n",
    "                    TIMEOUT_SECONDS,\n",
    "                    MAX_APPLICATIONS,\n",
    "                    spark_session,\n",
    "                    PERSISTENT_UI_TIMEOUT_SECONDS\n",
    "                ): c_info['cluster_id']\n",
    "                for c_info in batch_clusters\n",
    "            }\n",
    "\n",
    "            for future in concurrent.futures.as_completed(batch_futures):\n",
    "                cluster_id = batch_futures[future]\n",
    "                try:\n",
    "                    cluster_results = future.result()\n",
    "                    if cluster_results:\n",
    "                        cluster_summaries.append({\n",
    "                            'cluster_id': cluster_results['cluster_id'],\n",
    "                            'cluster_name': cluster_results['cluster_name'],\n",
    "                            'status': cluster_results['status'],\n",
    "                            'analysis_status': cluster_results['analysis_status'],\n",
    "                            'status_details': summarize_cluster_status(cluster_results),\n",
    "                            'total_applications': cluster_results['total_applications'],\n",
    "                            'total_jobs': cluster_results['total_jobs'],\n",
    "                            'total_stages': cluster_results['total_stages'],\n",
    "                            'total_tasks': cluster_results['total_tasks'],  # ADDED TASKS\n",
    "                            'total_sql_queries': cluster_results['total_sql_queries'],\n",
    "                            'total_executors': cluster_results['total_executors']\n",
    "                        })\n",
    "\n",
    "                        # Add cluster info to all results - UPDATED TO INCLUDE TASKS\n",
    "                        for item_list, key in [(all_applications_analysis, 'applications'),\n",
    "                                              (all_jobs_analysis, 'jobs'),\n",
    "                                              (all_stages_analysis, 'stages'),\n",
    "                                              (all_tasks_analysis, 'tasks'),  # ADDED TASKS\n",
    "                                              (all_sql_analysis, 'sql_queries'),\n",
    "                                              (all_executors_analysis, 'executors')]:\n",
    "                            for data in cluster_results[key]:\n",
    "                                data['cluster_id'] = cluster_results['cluster_id']\n",
    "                                data['cluster_name'] = cluster_results['cluster_name']\n",
    "                                item_list.append(data)\n",
    "                except Exception as e:\n",
    "                    logger.error(\"❌ Error processing results for cluster %s: %s\", cluster_id, str(e), exc_info=True)\n",
    "                    # Add failed cluster to summaries\n",
    "                    if not any(s['cluster_id'] == cluster_id for s in cluster_summaries):\n",
    "                        cluster_name = next((c['cluster_name'] for c in batch_clusters if c['cluster_id'] == cluster_id), 'Unknown')\n",
    "                        cluster_summaries.append({\n",
    "                            'cluster_id': cluster_id,\n",
    "                            'cluster_name': cluster_name,\n",
    "                            'status': 'FAILED',\n",
    "                            'total_applications': 0,\n",
    "                            'total_jobs': 0,\n",
    "                            'total_stages': 0,\n",
    "                            'total_tasks': 0,  # ADDED TASKS\n",
    "                            'total_sql_queries': 0,\n",
    "                            'total_executors': 0,\n",
    "                            'analysis_status': 'FAILED',\n",
    "                            'status_details': f\"Error processing cluster results: {str(e)}\"\n",
    "                        })\n",
    "\n",
    "        # Wait between batches to allow AWS cleanup (except for the last batch)\n",
    "        if current_batch < total_batches:\n",
    "            logger.info(\"⏳ Waiting %d seconds between batches to allow AWS Persistent UI cleanup...\", batch_delay_seconds)\n",
    "            time.sleep(batch_delay_seconds)\n",
    "\n",
    "    # UPDATED RETURN STATEMENT TO INCLUDE TASKS\n",
    "    return all_applications_analysis, all_jobs_analysis, all_stages_analysis, all_tasks_analysis, all_sql_analysis, all_executors_analysis, cluster_summaries\n",
    "\n",
    "def summarize_cluster_status(cluster_results: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate status details based on cluster analysis results.\n",
    "    \"\"\"\n",
    "    status_details = \"\"\n",
    "    if cluster_results['analysis_status'] == 'FAILED':\n",
    "        status_details = cluster_results.get('error_message', 'Unknown error - see logs')\n",
    "    elif cluster_results['analysis_status'] == 'COMPLETED':\n",
    "        endpoint_statuses = []\n",
    "        if cluster_results.get('applications_endpoint_success') is not None:\n",
    "            endpoint_statuses.append(f\"applications: {'OK' if cluster_results['applications_endpoint_success'] else 'FAILED'}\")\n",
    "        if cluster_results.get('jobs_endpoint_success') is not None:\n",
    "            endpoint_statuses.append(f\"jobs: {'OK' if cluster_results['jobs_endpoint_success'] else 'FAILED'}\")\n",
    "        if cluster_results.get('sql_endpoint_success') is not None:\n",
    "            endpoint_statuses.append(f\"sql: {'OK' if cluster_results['sql_endpoint_success'] else 'FAILED'}\")\n",
    "        if cluster_results.get('executors_endpoint_success') is not None:\n",
    "            endpoint_statuses.append(f\"executors: {'OK' if cluster_results['executors_endpoint_success'] else 'FAILED'}\")\n",
    "        status_details = \"; \".join(endpoint_statuses) if endpoint_statuses else \"Endpoints not checked\"\n",
    "    elif cluster_results['analysis_status'] == 'NO_APPLICATIONS':\n",
    "        status_details = \"No applications found in Spark History Server\"\n",
    "    return status_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4609b16-b029-4e8c-a02b-838292503a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c190ed-bd9a-46b1-9abe-77fab6102293",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Execution"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 19:15:04,436 - INFO - \uD83D\uDE80 Starting EMR Spark History Server Analysis\n2025-08-29 19:15:04,437 - INFO - \uD83C\uDF10 Multi-cluster discovery mode - searching for EMR clusters\n2025-08-29 19:15:04,439 - INFO - \uD83D\uDD0D Discovering EMR clusters in region: us-east-1\n2025-08-29 19:15:04,440 - INFO -  States: ['TERMINATED']\n2025-08-29 19:15:04,440 - INFO -  Name filter: None\n2025-08-29 19:15:04,440 - INFO -  Max clusters: 100\n2025-08-29 19:15:04,440 - INFO -  Created After: None\n2025-08-29 19:15:04,440 - INFO -  Created Before: None\n2025-08-29 19:15:04,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:04,681 - INFO - ✅ Discovered 100 clusters\n2025-08-29 19:15:04,681 - INFO -  1. spark-canary-job-aws @ 2025-08-29T17:13:00+00:00 :: Airflow :: Test (j-2LDTC0KC4RWZR) - TERMINATED\n2025-08-29 19:15:04,681 - INFO -  2. KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test (j-1HX6NYOHT3PI3) - TERMINATED\n2025-08-29 19:15:04,681 - INFO -  3. KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test (j-1S25M8TOS4B9V) - TERMINATED\n2025-08-29 19:15:04,682 - INFO -  4. KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test (j-2LQQYXCT2O98L) - TERMINATED\n2025-08-29 19:15:04,682 - INFO -  5. spark-canary-job-aws @ 2025-08-28T16:24:51.236322+00:00 :: Airflow :: Test (j-1SRRJD4VIGE8I) - TERMINATED\n2025-08-29 19:15:04,682 - INFO -  6. spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow :: Test (j-CK04NK3092PH) - TERMINATED\n2025-08-29 19:15:04,682 - INFO -  7. spark-canary-job-aws @ 2025-08-26T19:23:33.087323+00:00 :: Airflow :: Test (j-ZBIDJZSYAUC5) - TERMINATED\n2025-08-29 19:15:04,682 - INFO -  8. KongmingETLClusterCalibration @ 2025-08-25T10:00:00+00:00 :: Airflow :: Test (j-XWR1ATD9D25P) - TERMINATED\n2025-08-29 19:15:04,682 - INFO -  9. spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow :: Test (j-13HHLAF8A203N) - TERMINATED\n2025-08-29 19:15:04,683 - INFO -  10. spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow (j-N11M3Y9IQDEM) - TERMINATED\n2025-08-29 19:15:04,683 - INFO -  11. spark-canary-job-aws @ 2025-08-25T17:03:33.192584+00:00 :: Airflow (j-2XGZVZTB95DO0) - TERMINATED\n2025-08-29 19:15:04,683 - INFO -  12. ContainmentRecordsPreBid @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test (j-2BH4MH6ZYF1NN) - TERMINATED\n2025-08-29 19:15:04,683 - INFO -  13. ContainmentRecordsPreBid @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test (j-2HLY3YSX5FF6C) - TERMINATED\n2025-08-29 19:15:04,683 - INFO -  14. PreprocessPreBidCluster @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test (j-CJVZOKNNY5Y3) - TERMINATED\n2025-08-29 19:15:04,683 - INFO -  15. PreprocessPreBidCluster @ 2025-08-18T08:00:00+00:00 :: Airflow :: Test (j-1JGD3V2O0786C) - TERMINATED\n2025-08-29 19:15:04,684 - INFO -  16. spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow (j-2C9L9Y9WYQ6H8) - TERMINATED\n2025-08-29 19:15:04,684 - INFO -  17. spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow (j-2F9HRWX3WB2KP) - TERMINATED\n2025-08-29 19:15:04,684 - INFO -  18. spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow (j-137KTJ1T8J5TS) - TERMINATED\n2025-08-29 19:15:04,684 - INFO -  19. spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow (j-160Y1JZF990JG) - TERMINATED\n2025-08-29 19:15:04,684 - INFO -  20. Gautam: CleanRoomUserIdSellerAgg (j-1ILKHEQ510RAI) - TERMINATED\n2025-08-29 19:15:04,684 - INFO -  21. Gautam: CleanRoomUserIdSellerAgg (j-1KPXX0HR3U0BN) - TERMINATED\n2025-08-29 19:15:04,685 - INFO -  22. Gautam: CleanRoomUserIdSellerAgg (j-2O70N006Y1DEP) - TERMINATED\n2025-08-29 19:15:04,685 - INFO -  23. Gautam: UserId Seller Agg (j-310AFAY6L57Q) - TERMINATED\n2025-08-29 19:15:04,685 - INFO -  24. Gautam: UserId Seller Agg (j-38XT10OQKQR6U) - TERMINATED\n2025-08-29 19:15:04,685 - INFO -  25. Gautam: UserId Seller Agg (j-1VJLYTUUO02M0) - TERMINATED\n2025-08-29 19:15:04,685 - INFO -  26. Gautam: UserId Seller Agg (j-3UI2PXNPHBW5I) - TERMINATED\n2025-08-29 19:15:04,685 - INFO -  27. Gautam:  (j-89OZ40IG6D6F) - TERMINATED\n2025-08-29 19:15:04,686 - INFO -  28. Gautam: CleanroomCompactedAvails Test (j-PQW0MEADIC19) - TERMINATED\n2025-08-29 19:15:04,686 - INFO -  29. DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test (j-2LX7JEDEA2NXY) - TERMINATED\n2025-08-29 19:15:04,686 - INFO -  30. DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test (j-2FM0B0MTGHLAR) - TERMINATED\n2025-08-29 19:15:04,686 - INFO -  31. DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test (j-1BNIJAGUY895W) - TERMINATED\n2025-08-29 19:15:04,686 - INFO -  32. DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test (j-7HUGX0HL91O2) - TERMINATED\n2025-08-29 19:15:04,686 - INFO -  33. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-3NETQ5HD28KNN) - TERMINATED\n2025-08-29 19:15:04,687 - INFO -  34. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-FPBBMU8488M3) - TERMINATED\n2025-08-29 19:15:04,687 - INFO -  35. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-2A3AEORSGBMXV) - TERMINATED\n2025-08-29 19:15:04,687 - INFO -  36. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-YY2WFXRN6QC8) - TERMINATED\n2025-08-29 19:15:04,687 - INFO -  37. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-8J1WEFG7Z21J) - TERMINATED\n2025-08-29 19:15:04,687 - INFO -  38. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-UHXFK8IWP75A) - TERMINATED\n2025-08-29 19:15:04,687 - INFO -  39. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-1Z61L282MMAP9) - TERMINATED\n2025-08-29 19:15:04,687 - INFO -  40. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-3DCM9RMK4Q8GJ) - TERMINATED\n2025-08-29 19:15:04,688 - INFO -  41. spark-canary-job-aws @ 2025-07-25T16:25:43.068986+00:00 :: Airflow (j-VE8UZD5NPJ7B) - TERMINATED\n2025-08-29 19:15:04,688 - INFO -  42. UTS generation - LiveIntent 7/2/25  (j-1RPZF333MAXMX) - TERMINATED\n2025-08-29 19:15:04,688 - INFO -  43. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-1DVUR32ZX57G8) - TERMINATED\n2025-08-29 19:15:04,688 - INFO -  44. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-1FWDQ2DBFG222) - TERMINATED\n2025-08-29 19:15:04,688 - INFO -  45. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-17SP6VUGUCVA4) - TERMINATED\n2025-08-29 19:15:04,688 - INFO -  46. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-2MZTITEYFO818) - TERMINATED\n2025-08-29 19:15:04,689 - INFO -  47. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-3TLPAYHCIN61J) - TERMINATED\n2025-08-29 19:15:04,689 - INFO -  48. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-3JK2ZVMJD698W) - TERMINATED\n2025-08-29 19:15:04,689 - INFO -  49. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-2NGXIOFWCFTE5) - TERMINATED\n2025-08-29 19:15:04,689 - INFO -  50. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-2L64PD24BVEUB) - TERMINATED\n2025-08-29 19:15:04,689 - INFO -  51. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-25DYH14KZZDDX) - TERMINATED\n2025-08-29 19:15:04,689 - INFO -  52. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-2PS13W9CCXK6G) - TERMINATED\n2025-08-29 19:15:04,689 - INFO -  53. spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test (j-1WIAROADJFG40) - TERMINATED\n2025-08-29 19:15:04,690 - INFO -  54. datalake3-bidrequest-aws @ 2025-07-15T15:00:00+00:00 :: Airflow :: Test (j-3EUQOZY9A601C) - TERMINATED\n2025-08-29 19:15:04,690 - INFO -  55. DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-17T00:00:00+00:00 :: Airflow :: Test (j-IFPFURHW6UQE) - TERMINATED\n2025-08-29 19:15:04,690 - INFO -  56. UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 - Filter by Participant ID (j-2BE432OSC64KA) - TERMINATED\n2025-08-29 19:15:04,690 - INFO -  57. UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 - Filter by Participant ID (j-2XV5ZOFEU2KJZ) - TERMINATED\n2025-08-29 19:15:04,690 - INFO -  58. UTS generation - LiveIntent 1 day 7/14/25  (j-30719V08ACJVN) - TERMINATED\n2025-08-29 19:15:04,690 - INFO -  59. UTS generation - LiveIntent 1 day 7/14/25  (j-2CBXLKXOFU3KD) - TERMINATED\n2025-08-29 19:15:04,691 - INFO -  60. UTS generation - LiveIntent 1 day 7/14/25  (j-JAODY99JNLN7) - TERMINATED\n2025-08-29 19:15:04,691 - INFO -  61. UTS generation - LiveIntent 1 day 7/14/25  (j-3R9PRPTDBLCS0) - TERMINATED\n2025-08-29 19:15:04,691 - INFO -  62. UTS generation - LiveIntent 1 day 7/14/25  (j-17JTKGO5QY7EM) - TERMINATED\n2025-08-29 19:15:04,691 - INFO -  63. UTS generation - LiveIntent 1 day 7/14/25  (j-3QEKGBE785PV3) - TERMINATED\n2025-08-29 19:15:04,691 - INFO -  64. UTS generation - LiveIntent 1 day 7/14/25  (j-1WRXNLW9CGLNT) - TERMINATED\n2025-08-29 19:15:04,691 - INFO -  65. UTS generation - LiveIntent 1 day 7/14/25  (j-3DHLYXMVV4XKI) - TERMINATED\n2025-08-29 19:15:04,692 - INFO -  66. UTS generation - LiveIntent 1 day 7/14/25  (j-3UN62VYOM92I6) - TERMINATED\n2025-08-29 19:15:04,692 - INFO -  67. UTS generation - TextNow, DirecTV, LiveIntent 7/14/25  (j-3F246IATX40Z) - TERMINATED\n2025-08-29 19:15:04,692 - INFO -  68. UTS generation - TextNow, DirecTV, LiveIntent 7/14/25  (j-1N3NSNTANTUDT) - TERMINATED\n2025-08-29 19:15:04,692 - INFO -  69. spark-canary-job-aws @ 2025-07-14T10:13:00+00:00 :: Airflow :: Test (j-105SOVTHBNXL4) - TERMINATED\n2025-08-29 19:15:04,692 - INFO -  70. UTS generation - TextNow Full Data 7/7/25  (j-YZ5WJ2Y7WW4H) - TERMINATED\n2025-08-29 19:15:04,692 - INFO -  71. jada - Unity Iceberg Test (j-D0W606FYMZDR) - TERMINATED\n2025-08-29 19:15:04,692 - INFO -  72. jada - Unity Iceberg Test (j-7NJ8DENAWJQQ) - TERMINATED\n2025-08-29 19:15:04,693 - INFO -  73. jada - Unity Iceberg Test (j-1DBW31F0FS3S9) - TERMINATED\n2025-08-29 19:15:04,693 - INFO -  74. jada - Unity Iceberg Test (j-2NVYDVV2LI5TP) - TERMINATED\n2025-08-29 19:15:04,693 - INFO -  75. jada - Unity Iceberg Test (j-14BMRHQXFT9YD) - TERMINATED\n2025-08-29 19:15:04,693 - INFO -  76. UTS generation - LiveIntent, DirecTV,  Plex, Ibotta, TextNow 6/23/25 - Try 2 (j-1GP4NGMN98OJP) - TERMINATED\n2025-08-29 19:15:04,693 - INFO -  77. UTS generation - TextNow 6/23/25 (j-3AJM6NQQA9Q0N) - TERMINATED\n2025-08-29 19:15:04,693 - INFO -  78. UTS generation - TextNow 6/23/25 (j-2PRR8Q2VF0DPI) - TERMINATED\n2025-08-29 19:15:04,694 - INFO -  79. sgk cluster 2025-06-19 (j-370LX2NQQIX66) - TERMINATED\n2025-08-29 19:15:04,694 - INFO -  80. UTS generation - TextNow 6/23/25 (j-ADQBIJVXZCXT) - TERMINATED\n2025-08-29 19:15:04,694 - INFO -  81. UTS generation - TextNow 6/23/25 (j-29FBX9V6XEM1L) - TERMINATED\n2025-08-29 19:15:04,694 - INFO -  82. UTS generation - TextNow 6/23/25 (j-U0M0Q7GY5T57) - TERMINATED\n2025-08-29 19:15:04,694 - INFO -  83. UTS generation - TextNow 6/23/25 (j-3LH544P1CI67) - TERMINATED\n2025-08-29 19:15:04,694 - INFO -  84. UTS generation - LiveIntent, DirecTV,  Plex, Ibotta, TextNow 6/23/25 (j-31URMH9THDVMV) - TERMINATED\n2025-08-29 19:15:04,695 - INFO -  85. spark-canary-job-aws @ 2025-06-19T17:05:12.714264+00:00 :: Airflow (j-9FI52YABYLF3) - TERMINATED\n2025-08-29 19:15:04,695 - INFO -  86. spark-canary-job-aws @ 2025-06-19T16:18:19.037593+00:00 :: Airflow (j-16RJMT0M52OLX) - TERMINATED\n2025-08-29 19:15:04,695 - INFO -  87. spark-canary-job-aws @ 2025-06-19T14:11:42.088135+00:00 :: Airflow (j-3F9HP7BISWS6F) - TERMINATED\n2025-08-29 19:15:04,695 - INFO -  88. spark-canary-job-aws @ 2025-06-18T19:48:57.105802+00:00 :: Airflow (j-1SFUFZGK0HWF7) - TERMINATED\n2025-08-29 19:15:04,695 - INFO -  89. Gautam: CleanroomCompactedAvails Test (j-UBW66RQAYLKY) - TERMINATED\n2025-08-29 19:15:04,695 - INFO -  90. Gautam: CleanroomCompactedAvails Test (j-335XER9MDXKZ8) - TERMINATED\n2025-08-29 19:15:04,696 - INFO -  91. Gautam: CleanroomCompactedAvails Test (j-1KRXP00BY3HII) - TERMINATED\n2025-08-29 19:15:04,696 - INFO -  92. Gautam: CleanroomCompactedAvails Test (j-1OLQ7ZOBKKFIN) - TERMINATED\n2025-08-29 19:15:04,696 - INFO -  93. Gautam: CleanroomCompactedAvails Test (j-34S41A7LRQKR0) - TERMINATED\n2025-08-29 19:15:04,696 - INFO -  94. Test (j-4OX783O3ON2P) - TERMINATED\n2025-08-29 19:15:04,696 - INFO -  95. gautam.pulla - Unity external-table Test (j-3FPZE8W1C68Q1) - TERMINATED\n2025-08-29 19:15:04,696 - INFO -  96. gautam.pulla - Unity partitioned external-table Test (j-1W9CI5E0Y8Z3W) - TERMINATED\n2025-08-29 19:15:04,696 - INFO -  97. gautam.pulla - Unity partitioned external-table Test (j-3960TCK1GPKPH) - TERMINATED\n2025-08-29 19:15:04,697 - INFO -  98. gautam.pulla - Unity partitioned external-table Test (j-1980W1IPAZJAL) - TERMINATED\n2025-08-29 19:15:04,698 - INFO -  99. gautam.pulla - Unity partitioned external-table Test (j-3AU5YZH2R4A1S) - TERMINATED\n2025-08-29 19:15:04,698 - INFO -  100. gautam.pulla - Unity partitioned external-table Test (j-1YDSO7T9DRMB0) - TERMINATED\n2025-08-29 19:15:04,800 - INFO - Cluster j-2LDTC0KC4RWZR uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:04,910 - INFO - Successfully retrieved instance fleet info for j-2LDTC0KC4RWZR\n2025-08-29 19:15:04,911 - INFO - ✅ Cluster j-2LDTC0KC4RWZR is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:05,008 - INFO - Cluster j-1HX6NYOHT3PI3 uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:05,134 - INFO - Successfully retrieved instance fleet info for j-1HX6NYOHT3PI3\n2025-08-29 19:15:05,134 - INFO - ✅ Cluster j-1HX6NYOHT3PI3 is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:05,234 - INFO - Cluster j-1S25M8TOS4B9V uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:05,401 - INFO - Successfully retrieved instance fleet info for j-1S25M8TOS4B9V\n2025-08-29 19:15:05,401 - INFO - ✅ Cluster j-1S25M8TOS4B9V is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:05,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:05,550 - INFO - Cluster j-2LQQYXCT2O98L uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:05,672 - INFO - Successfully retrieved instance fleet info for j-2LQQYXCT2O98L\n2025-08-29 19:15:05,672 - INFO - ✅ Cluster j-2LQQYXCT2O98L is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:05,764 - INFO - Cluster j-1SRRJD4VIGE8I uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:05,884 - INFO - Successfully retrieved instance fleet info for j-1SRRJD4VIGE8I\n2025-08-29 19:15:05,884 - INFO - ✅ Cluster j-1SRRJD4VIGE8I is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:06,018 - INFO - Cluster j-CK04NK3092PH uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:06,132 - INFO - Successfully retrieved instance fleet info for j-CK04NK3092PH\n2025-08-29 19:15:06,132 - INFO - ✅ Cluster j-CK04NK3092PH is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:06,222 - INFO - Cluster j-ZBIDJZSYAUC5 uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:06,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:07,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:08,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:09,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:10,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:11,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:11,582 - INFO - Successfully retrieved instance fleet info for j-ZBIDJZSYAUC5\n2025-08-29 19:15:11,582 - INFO - ✅ Cluster j-ZBIDJZSYAUC5 is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:11,697 - INFO - Cluster j-XWR1ATD9D25P uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:11,828 - INFO - Successfully retrieved instance fleet info for j-XWR1ATD9D25P\n2025-08-29 19:15:11,828 - INFO - ✅ Cluster j-XWR1ATD9D25P is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:11,925 - INFO - Cluster j-13HHLAF8A203N uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:12,042 - INFO - Successfully retrieved instance fleet info for j-13HHLAF8A203N\n2025-08-29 19:15:12,042 - INFO - ✅ Cluster j-13HHLAF8A203N is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:12,135 - INFO - Cluster j-N11M3Y9IQDEM uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:12,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:13,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:14,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:15,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:16,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:17,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:18,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:19,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:20,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:21,128 - INFO - Successfully retrieved instance fleet info for j-N11M3Y9IQDEM\n2025-08-29 19:15:21,129 - INFO - ✅ Cluster j-N11M3Y9IQDEM is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:21,229 - INFO - Cluster j-2XGZVZTB95DO0 uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:21,334 - INFO - Successfully retrieved instance fleet info for j-2XGZVZTB95DO0\n2025-08-29 19:15:21,334 - INFO - ✅ Cluster j-2XGZVZTB95DO0 is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:21,429 - INFO - Cluster j-2BH4MH6ZYF1NN uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:21,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:21,553 - INFO - Successfully retrieved instance fleet info for j-2BH4MH6ZYF1NN\n2025-08-29 19:15:21,553 - INFO - ✅ Cluster j-2BH4MH6ZYF1NN is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:21,661 - INFO - Cluster j-2HLY3YSX5FF6C uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:21,795 - INFO - Successfully retrieved instance fleet info for j-2HLY3YSX5FF6C\n2025-08-29 19:15:21,795 - INFO - ✅ Cluster j-2HLY3YSX5FF6C is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:21,921 - INFO - Cluster j-CJVZOKNNY5Y3 uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:22,018 - INFO - Successfully retrieved instance fleet info for j-CJVZOKNNY5Y3\n2025-08-29 19:15:22,019 - INFO - ✅ Cluster j-CJVZOKNNY5Y3 is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:22,097 - INFO - Cluster j-1JGD3V2O0786C uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:22,212 - INFO - Successfully retrieved instance fleet info for j-1JGD3V2O0786C\n2025-08-29 19:15:22,212 - INFO - ✅ Cluster j-1JGD3V2O0786C is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:22,300 - INFO - Cluster j-2C9L9Y9WYQ6H8 uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:22,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:23,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:24,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:25,523 - WARNING - Could not get instance fleet details for j-2C9L9Y9WYQ6H8: ThrottlingException - Rate exceeded\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350322-1860489246\", line 173, in get_cluster_details\n    instance_groups = self.emr_client.list_instance_groups(ClusterId=cluster_id)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-6b9df388-c160-4c3c-801a-a5908d70345e/lib/python3.12/site-packages/botocore/client.py\", line 602, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-6b9df388-c160-4c3c-801a-a5908d70345e/lib/python3.12/site-packages/botocore/context.py\", line 123, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-6b9df388-c160-4c3c-801a-a5908d70345e/lib/python3.12/site-packages/botocore/client.py\", line 1078, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the ListInstanceGroups operation: Instance fleets and instance groups are mutually exclusive. The EMR cluster specified in the request uses instance fleets. The ListInstanceGroups operation does not support clusters that use instance fleets. Use the ListInstanceFleets operation instead.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350322-1860489246\", line 190, in get_cluster_details\n    instance_fleets = self.emr_client.list_instance_fleets(ClusterId=cluster_id)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-6b9df388-c160-4c3c-801a-a5908d70345e/lib/python3.12/site-packages/botocore/client.py\", line 602, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-6b9df388-c160-4c3c-801a-a5908d70345e/lib/python3.12/site-packages/botocore/context.py\", line 123, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-6b9df388-c160-4c3c-801a-a5908d70345e/lib/python3.12/site-packages/botocore/client.py\", line 1078, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (ThrottlingException) when calling the ListInstanceFleets operation (reached max retries: 4): Rate exceeded\n2025-08-29 19:15:25,524 - INFO - ✅ Cluster j-2C9L9Y9WYQ6H8 is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:25,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:25,653 - INFO - Cluster j-2F9HRWX3WB2KP uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:26,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:27,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:28,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:29,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:30,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:31,101 - INFO - Successfully retrieved instance fleet info for j-2F9HRWX3WB2KP\n2025-08-29 19:15:31,101 - INFO - ✅ Cluster j-2F9HRWX3WB2KP is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:31,202 - INFO - Cluster j-137KTJ1T8J5TS uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:31,299 - INFO - Successfully retrieved instance fleet info for j-137KTJ1T8J5TS\n2025-08-29 19:15:31,299 - INFO - ✅ Cluster j-137KTJ1T8J5TS is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:31,382 - INFO - Cluster j-160Y1JZF990JG uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:31,475 - INFO - Successfully retrieved instance fleet info for j-160Y1JZF990JG\n2025-08-29 19:15:31,476 - INFO - ✅ Cluster j-160Y1JZF990JG is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:31,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:31,665 - INFO - Cluster j-1ILKHEQ510RAI uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:31,845 - INFO - Successfully retrieved instance fleet info for j-1ILKHEQ510RAI\n2025-08-29 19:15:31,846 - INFO - ✅ Cluster j-1ILKHEQ510RAI is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:31,962 - INFO - Cluster j-1KPXX0HR3U0BN uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:32,087 - INFO - Successfully retrieved instance fleet info for j-1KPXX0HR3U0BN\n2025-08-29 19:15:32,088 - INFO - ✅ Cluster j-1KPXX0HR3U0BN is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-29 19:15:32,190 - INFO - Cluster j-2O70N006Y1DEP uses instance fleets. Fetching instance fleet details.\n2025-08-29 19:15:32,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:33,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:34,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:35,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:36,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:37,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:38,537 - INFO - Received command c on object id p0\n2025-08-29 19:15:39,537 - INFO - Received command c on object id p0\n2025-08-29 19:1\n\n*** WARNING: max output size exceeded, skipping output. ***\n\ne 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:44,386 - ERROR - ❌ Failed to analyze cluster UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 - Filter by Participant ID: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:44,406 - ERROR - ❌ Failed to analyze cluster sgk cluster 2025-06-19: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:44,537 - INFO - Received command c on object id p0\n2025-08-29 19:18:44,572 - ERROR - ❌ Failed to analyze cluster DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-17T00:00:00+00:00 :: Airflow :: Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:44,589 - ERROR - ❌ Failed to analyze cluster spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:44,615 - ERROR - ❌ Failed to analyze cluster spark-canary-job-aws @ 2025-06-19T14:11:42.088135+00:00 :: Airflow: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:44,913 - ERROR - ❌ Failed to analyze cluster gautam.pulla - Unity partitioned external-table Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:44,918 - ERROR - ❌ Failed to analyze cluster spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:44,964 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent, DirecTV,  Plex, Ibotta, TextNow 6/23/25: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,019 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent 1 day 7/14/25 : Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,033 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent 1 day 7/14/25 : Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,114 - ERROR - ❌ Failed to analyze cluster Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,158 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent 1 day 7/14/25 : Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,178 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent 1 day 7/14/25 : Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,226 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent 1 day 7/14/25 : Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,259 - ERROR - ❌ Failed to analyze cluster spark-canary-job-aws @ 2025-06-18T19:48:57.105802+00:00 :: Airflow: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,281 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent 1 day 7/14/25 : Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,294 - ERROR - ❌ Failed to analyze cluster spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,294 - ERROR - ❌ Failed to analyze cluster jada - Unity Iceberg Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,308 - ERROR - ❌ Failed to analyze cluster UTS generation - TextNow 6/23/25: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,312 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent 1 day 7/14/25 : Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,319 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent, DirecTV,  Plex, Ibotta, TextNow 6/23/25 - Try 2: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,328 - ERROR - ❌ Failed to analyze cluster gautam.pulla - Unity partitioned external-table Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,356 - ERROR - ❌ Failed to analyze cluster gautam.pulla - Unity external-table Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,359 - ERROR - ❌ Failed to analyze cluster gautam.pulla - Unity partitioned external-table Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,361 - ERROR - ❌ Failed to analyze cluster Gautam: CleanroomCompactedAvails Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,366 - ERROR - ❌ Failed to analyze cluster jada - Unity Iceberg Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,369 - ERROR - ❌ Failed to analyze cluster UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 : Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,371 - ERROR - ❌ Failed to analyze cluster Gautam: CleanroomCompactedAvails Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,376 - ERROR - ❌ Failed to analyze cluster UTS generation - LiveIntent 7/2/25 : Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,385 - ERROR - ❌ Failed to analyze cluster UTS generation - TextNow 6/23/25: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,388 - ERROR - ❌ Failed to analyze cluster spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,390 - ERROR - ❌ Failed to analyze cluster spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,395 - ERROR - ❌ Failed to analyze cluster UTS generation - TextNow 6/23/25: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,400 - ERROR - ❌ Failed to analyze cluster jada - Unity Iceberg Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,403 - ERROR - ❌ Failed to analyze cluster spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,404 - ERROR - ❌ Failed to analyze cluster UTS generation - TextNow 6/23/25: Persistent App UI did not become ready after waiting 30 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1943/command-1814999120350328-717076234\", line 39, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1943/command-1814999120350320-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 30 seconds.\n2025-08-29 19:18:45,409 - INFO - \uD83D\uDCCA Creating analysis DataFrames...\n2025-08-29 19:18:45,537 - INFO - Received command c on object id p0\n2025-08-29 19:18:46,537 - INFO - Received command c on object id p0\n2025-08-29 19:18:47,041 - INFO - ✅ Created applications DataFrame with 22 rows and 16 columns\n2025-08-29 19:18:47,085 - INFO - \uD83D\uDCDD Multi-Cluster Analysis Complete - Summary Results:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n====================================================================================================\n\uD83D\uDCCA CLUSTER ANALYSIS SUMMARY\n====================================================================================================\nCluster Summary (100 clusters):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 19:18:47,537 - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------------------------------------------------------------------------------+----------+---------------+---------------------------------------------------------------------------------------------------------------------------------+------------------+----------+------------+-----------+-----------------+---------------+\n|cluster_id     |cluster_name                                                                              |status    |analysis_status|status_details                                                                                                                   |total_applications|total_jobs|total_stages|total_tasks|total_sql_queries|total_executors|\n+---------------+------------------------------------------------------------------------------------------+----------+---------------+---------------------------------------------------------------------------------------------------------------------------------+------------------+----------+------------+-----------+-----------------+---------------+\n|j-ZBIDJZSYAUC5 |spark-canary-job-aws @ 2025-08-26T19:23:33.087323+00:00 :: Airflow :: Test                |TERMINATED|FAILED         |Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-zbidjzsyauc5.emrappui-prod.us-east-1.amazonaws.com/shs/ |0                 |0         |0           |0          |0                |0              |\n|j-XWR1ATD9D25P |KongmingETLClusterCalibration @ 2025-08-25T10:00:00+00:00 :: Airflow :: Test              |TERMINATED|NO_APPLICATIONS|No applications found in Spark History Server                                                                                    |0                 |0         |0           |0          |0                |0              |\n|j-2LDTC0KC4RWZR|spark-canary-job-aws @ 2025-08-29T17:13:00+00:00 :: Airflow :: Test                       |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |1                 |0         |0           |0          |0                |0              |\n|j-CK04NK3092PH |spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow :: Test                |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |1                 |0         |0           |0          |0                |0              |\n|j-1SRRJD4VIGE8I|spark-canary-job-aws @ 2025-08-28T16:24:51.236322+00:00 :: Airflow :: Test                |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |1                 |0         |0           |0          |0                |0              |\n|j-1JGD3V2O0786C|PreprocessPreBidCluster @ 2025-08-18T08:00:00+00:00 :: Airflow :: Test                    |TERMINATED|FAILED         |Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-1jgd3v2o0786c.emrappui-prod.us-east-1.amazonaws.com/shs/|0                 |0         |0           |0          |0                |0              |\n|j-1S25M8TOS4B9V|KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test                         |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |2                 |0         |0           |0          |0                |0              |\n|j-CJVZOKNNY5Y3 |PreprocessPreBidCluster @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test                    |TERMINATED|FAILED         |Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-cjvzoknny5y3.emrappui-prod.us-east-1.amazonaws.com/shs/ |0                 |0         |0           |0          |0                |0              |\n|j-2LQQYXCT2O98L|KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test                         |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |2                 |0         |0           |0          |0                |0              |\n|j-2BH4MH6ZYF1NN|ContainmentRecordsPreBid @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test                   |TERMINATED|FAILED         |Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2bh4mh6zyf1nn.emrappui-prod.us-east-1.amazonaws.com/shs/|0                 |0         |0           |0          |0                |0              |\n|j-FPBBMU8488M3 |spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test                       |TERMINATED|FAILED         |Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-fpbbmu8488m3.emrappui-prod.us-east-1.amazonaws.com/shs/ |0                 |0         |0           |0          |0                |0              |\n|j-2FM0B0MTGHLAR|DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test|TERMINATED|FAILED         |Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2fm0b0mtghlar.emrappui-prod.us-east-1.amazonaws.com/shs/|0                 |0         |0           |0          |0                |0              |\n|j-2A3AEORSGBMXV|spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test                       |TERMINATED|FAILED         |Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2a3aeorsgbmxv.emrappui-prod.us-east-1.amazonaws.com/shs/|0                 |0         |0           |0          |0                |0              |\n|j-310AFAY6L57Q |Gautam: UserId Seller Agg                                                                 |TERMINATED|NO_APPLICATIONS|No applications found in Spark History Server                                                                                    |0                 |0         |0           |0          |0                |0              |\n|j-N11M3Y9IQDEM |spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow                        |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |1                 |0         |0           |0          |0                |0              |\n|j-2C9L9Y9WYQ6H8|spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow                        |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |1                 |0         |0           |0          |0                |0              |\n|j-2HLY3YSX5FF6C|ContainmentRecordsPreBid @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test                   |TERMINATED|FAILED         |Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2hly3ysx5ff6c.emrappui-prod.us-east-1.amazonaws.com/shs/|0                 |0         |0           |0          |0                |0              |\n|j-137KTJ1T8J5TS|spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow                        |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |1                 |0         |0           |0          |0                |0              |\n|j-2F9HRWX3WB2KP|spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow                        |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |1                 |0         |0           |0          |0                |0              |\n|j-1KPXX0HR3U0BN|Gautam: CleanRoomUserIdSellerAgg                                                          |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED                                                                   |1                 |0         |0           |0          |0                |0              |\n+---------------+------------------------------------------------------------------------------------------+----------+---------------+---------------------------------------------------------------------------------------------------------------------------------+------------------+----------+------------+-----------+-----------------+---------------+\nonly showing top 20 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 19:18:48,035 - INFO -  • Total Applications analyzed: 22\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== TOP 10 APPLICATIONS BY DURATION (ACROSS ALL CLUSTERS) ===\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 19:18:48,341 - INFO - \uD83C\uDF89 Multi-cluster analysis complete! All data returned as DataFrames.\n2025-08-29 19:18:48,344 - INFO - \uD83E\uDDEA Running in DEV mode - S3 output skipped\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+------------------------------------------------------+---------+--------+\n|cluster_name                                                       |application_name                                      |attemptId|duration|\n+-------------------------------------------------------------------+------------------------------------------------------+---------+--------+\n|Gautam: CleanRoomUserIdSellerAgg                                   |CleanRoomUserIdSellerAgg                              |1        |2067182 |\n|KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test  |DailyScoreSet-class-job.DailyOfflineScoringSet        |1        |328898  |\n|KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test  |DailyScoreSet-class-job.DailyOfflineScoringSet        |1        |319869  |\n|KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test  |DailyScoreSet-class-job.DailyOfflineScoringSet        |1        |310995  |\n|KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test  |DailyConversion-class-job.ConversionDataDailyProcessor|1        |192007  |\n|KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test  |DailyFeedbackSignals-class-job.DailyFeedbackSignals   |1        |142049  |\n|KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test  |DailyAttributedEvents-class-job.DailyAttributedEvents |1        |65920   |\n|KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test  |DailyAttributedEvents-class-job.DailyAttributedEvents |1        |57899   |\n|spark-canary-job-aws @ 2025-08-29T17:13:00+00:00 :: Airflow :: Test|CanaryPipeline-class-jobs.dataproc.canary.CanaryJob   |1        |22520   |\n|spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow |CanaryPipeline-class-jobs.dataproc.canary.CanaryJob   |1        |22014   |\n+-------------------------------------------------------------------+------------------------------------------------------+---------+--------+\nonly showing top 10 rows\n\n====================================================================================================\n✨ EMR MULTI-CLUSTER SPARK HISTORY SERVER ANALYSIS COMPLETED! ✨\n====================================================================================================\n✅ Analyzed 16 clusters\n✅ Analyzed 22 applications\n✅ Analyzed 0 jobs\n✅ Analyzed 0 stages\n✅ Analyzed 0 tasks\n✅ Analyzed 0 SQL queries\n✅ Analyzed 0 executors\n\n\uD83D\uDCCA DataFrames available for analysis:\n • cluster_summaries_df\n • applications_df\n • jobs_df\n • stages_df\n • tasks_df\n • sql_df\n • executors_df\n • analysis_summary\n\n\uD83D\uDCA1 Example usage:\n applications_df.filter(col('duration') > 10000).show()\n stages_df.groupBy('cluster_name').agg(sum('inputBytes')).show()\n sql_df.select('description', 'raw_json').show(truncate=False)\n executors_df.select('cluster_name', 'id', 'memoryUsed', 'totalCores').show()\n tasks_df.select('cluster_name', 'taskId', 'duration', 'status').show()\n====================================================================================================\n\uD83E\uDDEA DEV Mode: Analysis results available in DataFrames only (no S3 output)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main_analysis() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to execute the complete Spark History Server analysis.\n",
    "    Updated to use sequential batch processing to avoid 200 Persistent UI limit.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"\uD83D\uDE80 Starting EMR Spark History Server Analysis\")\n",
    "\n",
    "        clusters_to_analyze = []\n",
    "        if EMR_CLUSTER_ARN:\n",
    "            logger.info(\"\uD83C\uDFAF Single cluster mode - using provided EMR Cluster ARN\")\n",
    "            cluster_id = EMR_CLUSTER_ARN.split('/')[-1]\n",
    "            clusters_to_analyze.append({'cluster_id': cluster_id, 'cluster_name': 'User-Specified', 'cluster_arn': EMR_CLUSTER_ARN, 'status': 'UNKNOWN'})\n",
    "        else:\n",
    "            logger.info(\"\uD83C\uDF10 Multi-cluster discovery mode - searching for EMR clusters\")\n",
    "            discovery = EMRClusterDiscovery(AWS_REGION)\n",
    "            discovered_clusters = discovery.discover_clusters(\n",
    "                states=CLUSTER_STATES_LIST,\n",
    "                name_filter=CLUSTER_NAME_FILTER,\n",
    "                max_clusters=MAX_CLUSTERS,\n",
    "                created_after=PARSED_CREATED_AFTER_DATE,\n",
    "                created_before=PARSED_CREATED_BEFORE_DATE\n",
    "            )\n",
    "\n",
    "            if not discovered_clusters:\n",
    "                logger.warning(\"⚠️ No clusters found matching criteria\")\n",
    "                return {'cluster_summaries_df': None, 'applications_df': None, 'jobs_df': None, 'stages_df': None, 'tasks_df': None, 'sql_df': None, 'executors_df': None, 'summary': {'total_clusters_analyzed': 0, 'total_applications': 0, 'total_jobs': 0, 'total_stages': 0, 'total_tasks': 0, 'total_sql_queries': 0, 'total_executors': 0}}\n",
    "\n",
    "            for cluster_summary in discovered_clusters:\n",
    "                try:\n",
    "                    cluster_details = discovery.get_cluster_details(cluster_summary['cluster_id'])\n",
    "                    if discovery.validate_cluster_for_analysis(cluster_details):\n",
    "                        clusters_to_analyze.append({'cluster_id': cluster_details['cluster_id'], 'cluster_name': cluster_details['cluster_name'], 'cluster_arn': cluster_details['cluster_arn'], 'status': cluster_details['status'], 'details': cluster_details})\n",
    "                except Exception as e:\n",
    "                    logger.error(\"❌ Failed to validate cluster %s: %s\", cluster_summary['cluster_id'], str(e), exc_info=True)\n",
    "                    continue\n",
    "\n",
    "        if not clusters_to_analyze:\n",
    "            logger.error(\"❌ No valid clusters found for analysis\")\n",
    "            return {'cluster_summaries_df': None, 'applications_df': None, 'jobs_df': None, 'stages_df': None, 'tasks_df': None, 'sql_df': None, 'executors_df': None, 'summary': {'total_clusters_analyzed': 0, 'total_applications': 0, 'total_jobs': 0, 'total_stages': 0, 'total_tasks': 0, 'total_sql_queries': 0, 'total_executors': 0}}\n",
    "\n",
    "        logger.info(\"\uD83D\uDCCA Will analyze %s cluster(s) in batches of %s\", len(clusters_to_analyze), BATCH_SIZE)\n",
    "\n",
    "        spark = SparkSession.builder.appName(\"EMR-Multi-Cluster-Spark-History-Analysis\").getOrCreate()\n",
    "        analyzer_instance = SparkMetricsAnalyzer(spark)\n",
    "\n",
    "        # Process clusters in batches - UPDATED TO INCLUDE TASKS\n",
    "        all_applications_analysis, all_jobs_analysis, all_stages_analysis, all_tasks_analysis, all_sql_analysis, all_executors_analysis, cluster_summaries = process_clusters_in_batches(\n",
    "            clusters_to_analyze, BATCH_SIZE, BATCH_DELAY_SECONDS, analyzer_instance, spark\n",
    "        )\n",
    "\n",
    "        logger.info(\"\uD83D\uDCCA Creating analysis DataFrames...\")\n",
    "        # UPDATED TO INCLUDE TASKS\n",
    "        apps_df, jobs_df, stages_df, tasks_df, sql_df, executors_df = analyzer_instance.create_dynamic_dataframes(\n",
    "            all_applications_analysis, all_jobs_analysis, all_stages_analysis, all_tasks_analysis, all_sql_analysis, all_executors_analysis\n",
    "        )\n",
    "\n",
    "        cluster_summary_df = None\n",
    "        if cluster_summaries:\n",
    "            cluster_summary_schema = StructType([\n",
    "                StructField('cluster_id', StringType(), True),\n",
    "                StructField('cluster_name', StringType(), True),\n",
    "                StructField('status', StringType(), True),\n",
    "                StructField('analysis_status', StringType(), True),\n",
    "                StructField('status_details', StringType(), True),\n",
    "                StructField('total_applications', IntegerType(), True),\n",
    "                StructField('total_jobs', IntegerType(), True),\n",
    "                StructField('total_stages', IntegerType(), True),\n",
    "                StructField('total_tasks', IntegerType(), True),  # ADDED TASKS\n",
    "                StructField('total_sql_queries', IntegerType(), True),\n",
    "                StructField('total_executors', IntegerType(), True)\n",
    "            ])\n",
    "            cluster_summary_df = spark.createDataFrame(cluster_summaries, schema=cluster_summary_schema)\n",
    "\n",
    "        logger.info(\"\uD83D\uDCDD Multi-Cluster Analysis Complete - Summary Results:\")\n",
    "        print(\"\\n\" + \"=\"*100 + \"\\n\uD83D\uDCCA CLUSTER ANALYSIS SUMMARY\\n\" + \"=\"*100)\n",
    "        if cluster_summary_df:\n",
    "            print(f\"Cluster Summary ({cluster_summary_df.count()} clusters):\")\n",
    "            cluster_summary_df.show(truncate=False)\n",
    "\n",
    "        if apps_df and apps_df.count() > 0:\n",
    "            logger.info(\" • Total Applications analyzed: %s\", apps_df.count())\n",
    "            print(\"\\n=== TOP 10 APPLICATIONS BY DURATION (ACROSS ALL CLUSTERS) ===\")\n",
    "            apps_df.select(\"cluster_name\", \"application_name\", \"attemptId\", \"duration\").orderBy(\"duration\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        if jobs_df and jobs_df.count() > 0:\n",
    "            logger.info(\" • Total Jobs analyzed: %s\", jobs_df.count())\n",
    "            print(\"\\n=== JOB SUCCESS RATES BY CLUSTER ===\")\n",
    "            jobs_df.select(\"cluster_name\", \"application_id\", \"name\", \"status\", \"numTasks\", \"numCompletedTasks\").orderBy(\"numTasks\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        if stages_df and stages_df.count() > 0:\n",
    "            logger.info(\" • Total Stages analyzed: %s\", stages_df.count())\n",
    "            print(\"\\n=== TOP 10 STAGES BY DATA PROCESSED (ACROSS ALL CLUSTERS) ===\")\n",
    "            stages_df.select(\"cluster_name\", \"application_id\", \"name\", \"inputBytes\", \"outputBytes\").orderBy(\"inputBytes\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        # ADDED TASKS ANALYSIS\n",
    "        if tasks_df and tasks_df.count() > 0:\n",
    "            logger.info(\" • Total Tasks analyzed: %s\", tasks_df.count())\n",
    "            print(\"\\n=== TOP 10 TASKS BY RUNTIME (ACROSS ALL CLUSTERS) ===\")\n",
    "            tasks_df.select(\"cluster_name\", \"application_id\", \"stage_id\", \"taskId\", \"duration\").orderBy(\"duration\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        if sql_df and sql_df.count() > 0:\n",
    "            logger.info(\" • Total SQL Queries analyzed: %s\", sql_df.count())\n",
    "            print(\"\\n=== TOP 10 SQL QUERIES BY DURATION (ACROSS ALL CLUSTERS) ===\")\n",
    "            sql_df.select(\"cluster_name\", \"application_id\", \"id\", \"description\", \"duration\").orderBy(\"duration\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        if executors_df and executors_df.count() > 0:\n",
    "            logger.info(\" • Total Executors analyzed: %s\", executors_df.count())\n",
    "            print(\"\\n=== TOP 10 EXECUTORS BY MEMORY USAGE (ACROSS ALL CLUSTERS) ===\")\n",
    "            executors_df.select(\"cluster_name\", \"application_id\", \"id\", \"hostPort\", \"memoryUsed\", \"totalCores\").orderBy(\"memoryUsed\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        logger.info(\"\uD83C\uDF89 Multi-cluster analysis complete! All data returned as DataFrames.\")\n",
    "        \n",
    "        # UPDATED SUMMARY TO INCLUDE TASKS\n",
    "        summary = {\n",
    "            'total_clusters_analyzed': len([c for c in cluster_summaries if c['analysis_status'] == 'COMPLETED']),\n",
    "            'total_applications': len(all_applications_analysis),\n",
    "            'total_jobs': len(all_jobs_analysis),\n",
    "            'total_stages': len(all_stages_analysis),\n",
    "            'total_tasks': len(all_tasks_analysis),  # ADDED TASKS\n",
    "            'total_sql_queries': len(all_sql_analysis),\n",
    "            'total_executors': len(all_executors_analysis)\n",
    "        }\n",
    "\n",
    "        # UPDATED RETURN TO INCLUDE TASKS\n",
    "        return {'cluster_summaries_df': cluster_summary_df, 'applications_df': apps_df, 'jobs_df': jobs_df, 'stages_df': stages_df, 'tasks_df': tasks_df, 'sql_df': sql_df, 'executors_df': executors_df, 'summary': summary}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"❌ Multi-cluster analysis failed: %s\", str(e), exc_info=True)\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    results = main_analysis()\n",
    "    cluster_summaries_df = results['cluster_summaries_df']\n",
    "    applications_df = results['applications_df']\n",
    "    jobs_df = results['jobs_df']\n",
    "    stages_df = results['stages_df']\n",
    "    tasks_df = results['tasks_df']  # ADDED TASKS\n",
    "    sql_df = results['sql_df']\n",
    "    executors_df = results['executors_df']\n",
    "    analysis_summary = results['summary']\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n✨ EMR MULTI-CLUSTER SPARK HISTORY SERVER ANALYSIS COMPLETED! ✨\\n\" + \"=\"*100)\n",
    "    # UPDATED PRINT STATEMENT TO INCLUDE TASKS\n",
    "    print(f\"✅ Analyzed {analysis_summary['total_clusters_analyzed']} clusters\\n✅ Analyzed {analysis_summary['total_applications']} applications\\n✅ Analyzed {analysis_summary['total_jobs']} jobs\\n✅ Analyzed {analysis_summary['total_stages']} stages\\n✅ Analyzed {analysis_summary['total_tasks']} tasks\\n✅ Analyzed {analysis_summary['total_sql_queries']} SQL queries\\n✅ Analyzed {analysis_summary['total_executors']} executors\")\n",
    "\n",
    "    # UPDATED TO INCLUDE TASKS\n",
    "    print(\"\\n\uD83D\uDCCA DataFrames available for analysis:\\n • cluster_summaries_df\\n • applications_df\\n • jobs_df\\n • stages_df\\n • tasks_df\\n • sql_df\\n • executors_df\\n • analysis_summary\")\n",
    "\n",
    "    print(\"\\n\uD83D\uDCA1 Example usage:\\n applications_df.filter(col('duration') > 10000).show()\\n stages_df.groupBy('cluster_name').agg(sum('inputBytes')).show()\\n sql_df.select('description', 'raw_json').show(truncate=False)\\n executors_df.select('cluster_name', 'id', 'memoryUsed', 'totalCores').show()\\n tasks_df.select('cluster_name', 'taskId', 'duration', 'status').show()\\n\" + \"=\"*100)\n",
    "\n",
    "    if ENVIRONMENT == \"prod\" and S3_OUTPUT_PATH:\n",
    "        logger.info(\"\uD83D\uDCE4 Writing analysis results to S3: %s\", S3_OUTPUT_PATH)\n",
    "        try:\n",
    "            if cluster_summaries_df and cluster_summaries_df.count() > 0:\n",
    "                cluster_summaries_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/cluster_summaries/\")\n",
    "            if applications_df and applications_df.count() > 0:\n",
    "                applications_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/applications/\")\n",
    "            if jobs_df and jobs_df.count() > 0:\n",
    "                jobs_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/jobs/\")\n",
    "            if stages_df and stages_df.count() > 0:\n",
    "                stages_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/stages/\")\n",
    "            if tasks_df and tasks_df.count() > 0:  # ADDED TASKS S3 WRITE\n",
    "                tasks_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/tasks/\")\n",
    "            if sql_df and sql_df.count() > 0:\n",
    "                sql_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/sql/\")\n",
    "            if executors_df and executors_df.count() > 0:\n",
    "                executors_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/executors/\")\n",
    "            logger.info(\"✅ All analysis results successfully written to S3\")\n",
    "        except Exception as s3_error:\n",
    "            logger.error(\"❌ Failed to write results to S3: %s\", str(s3_error), exc_info=True)\n",
    "            print(f\"⚠️ Warning: S3 write failed, but analysis completed successfully. Error: {str(s3_error)}\")\n",
    "    elif ENVIRONMENT == \"dev\":\n",
    "        logger.info(\"\uD83E\uDDEA Running in DEV mode - S3 output skipped\")\n",
    "        print(\"\uD83E\uDDEA DEV Mode: Analysis results available in DataFrames only (no S3 output)\")\n",
    "    else:\n",
    "        logger.info(\"\uD83D\uDEAB PROD mode but no S3 output path specified - S3 output skipped\")\n",
    "        print(\"⚠️ PROD mode detected but no S3 output path provided - results available in DataFrames only\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ANALYSIS FAILED: {str(e)}\\nPlease check the logs above for detailed error information.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ce8970-1746-462e-8b8e-66cab16b3077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Table Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c68ba71-f276-4118-8ba8-213ed1037509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'total_clusters_analyzed': 16,\n",
       " 'total_applications': 22,\n",
       " 'total_jobs': 0,\n",
       " 'total_stages': 0,\n",
       " 'total_tasks': 0,\n",
       " 'total_sql_queries': 0,\n",
       " 'total_executors': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(analysis_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb33c573-1959-4e44-99fd-2a1355377fbd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756495145942}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cluster_id</th><th>cluster_name</th><th>status</th><th>analysis_status</th><th>status_details</th><th>total_applications</th><th>total_jobs</th><th>total_stages</th><th>total_tasks</th><th>total_sql_queries</th><th>total_executors</th></tr></thead><tbody><tr><td>j-ZBIDJZSYAUC5</td><td>spark-canary-job-aws @ 2025-08-26T19:23:33.087323+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-zbidjzsyauc5.emrappui-prod.us-east-1.amazonaws.com/shs/</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-XWR1ATD9D25P</td><td>KongmingETLClusterCalibration @ 2025-08-25T10:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>NO_APPLICATIONS</td><td>No applications found in Spark History Server</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2LDTC0KC4RWZR</td><td>spark-canary-job-aws @ 2025-08-29T17:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-CK04NK3092PH</td><td>spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1SRRJD4VIGE8I</td><td>spark-canary-job-aws @ 2025-08-28T16:24:51.236322+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1JGD3V2O0786C</td><td>PreprocessPreBidCluster @ 2025-08-18T08:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-1jgd3v2o0786c.emrappui-prod.us-east-1.amazonaws.com/shs/</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1S25M8TOS4B9V</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-CJVZOKNNY5Y3</td><td>PreprocessPreBidCluster @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-cjvzoknny5y3.emrappui-prod.us-east-1.amazonaws.com/shs/</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2LQQYXCT2O98L</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2BH4MH6ZYF1NN</td><td>ContainmentRecordsPreBid @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2bh4mh6zyf1nn.emrappui-prod.us-east-1.amazonaws.com/shs/</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-FPBBMU8488M3</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-fpbbmu8488m3.emrappui-prod.us-east-1.amazonaws.com/shs/</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2FM0B0MTGHLAR</td><td>DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2fm0b0mtghlar.emrappui-prod.us-east-1.amazonaws.com/shs/</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2A3AEORSGBMXV</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2a3aeorsgbmxv.emrappui-prod.us-east-1.amazonaws.com/shs/</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-310AFAY6L57Q</td><td>Gautam: UserId Seller Agg</td><td>TERMINATED</td><td>NO_APPLICATIONS</td><td>No applications found in Spark History Server</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-N11M3Y9IQDEM</td><td>spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2C9L9Y9WYQ6H8</td><td>spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2HLY3YSX5FF6C</td><td>ContainmentRecordsPreBid @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2hly3ysx5ff6c.emrappui-prod.us-east-1.amazonaws.com/shs/</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-137KTJ1T8J5TS</td><td>spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2F9HRWX3WB2KP</td><td>spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1KPXX0HR3U0BN</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1VJLYTUUO02M0</td><td>Gautam: UserId Seller Agg</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1ILKHEQ510RAI</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3UI2PXNPHBW5I</td><td>Gautam: UserId Seller Agg</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: OK; sql: OK; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2XGZVZTB95DO0</td><td>spark-canary-job-aws @ 2025-08-25T17:03:33.192584+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1HX6NYOHT3PI3</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2LX7JEDEA2NXY</td><td>DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>NO_APPLICATIONS</td><td>No applications found in Spark History Server</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-160Y1JZF990JG</td><td>spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3NETQ5HD28KNN</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-3netq5hd28knn.emrappui-prod.us-east-1.amazonaws.com/shs/</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-13HHLAF8A203N</td><td>spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-25DYH14KZZDDX</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-YY2WFXRN6QC8</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1BNIJAGUY895W</td><td>DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-UHXFK8IWP75A</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-38XT10OQKQR6U</td><td>Gautam: UserId Seller Agg</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3EUQOZY9A601C</td><td>datalake3-bidrequest-aws @ 2025-07-15T15:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-7HUGX0HL91O2</td><td>DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-16RJMT0M52OLX</td><td>spark-canary-job-aws @ 2025-06-19T16:18:19.037593+00:00 :: Airflow</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-335XER9MDXKZ8</td><td>Gautam: CleanroomCompactedAvails Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2MZTITEYFO818</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-34S41A7LRQKR0</td><td>Gautam: CleanroomCompactedAvails Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-8J1WEFG7Z21J</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-7NJ8DENAWJQQ</td><td>jada - Unity Iceberg Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3LH544P1CI67</td><td>UTS generation - TextNow 6/23/25</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3DCM9RMK4Q8GJ</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3TLPAYHCIN61J</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-29FBX9V6XEM1L</td><td>UTS generation - TextNow 6/23/25</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1FWDQ2DBFG222</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1980W1IPAZJAL</td><td>gautam.pulla - Unity partitioned external-table Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2XV5ZOFEU2KJZ</td><td>UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 - Filter by Participant ID</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-9FI52YABYLF3</td><td>spark-canary-job-aws @ 2025-06-19T17:05:12.714264+00:00 :: Airflow</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1Z61L282MMAP9</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3AU5YZH2R4A1S</td><td>gautam.pulla - Unity partitioned external-table Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-89OZ40IG6D6F</td><td>Gautam: </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2CBXLKXOFU3KD</td><td>UTS generation - LiveIntent 1 day 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-VE8UZD5NPJ7B</td><td>spark-canary-job-aws @ 2025-07-25T16:25:43.068986+00:00 :: Airflow</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-105SOVTHBNXL4</td><td>spark-canary-job-aws @ 2025-07-14T10:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-UBW66RQAYLKY</td><td>Gautam: CleanroomCompactedAvails Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2L64PD24BVEUB</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-PQW0MEADIC19</td><td>Gautam: CleanroomCompactedAvails Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3QEKGBE785PV3</td><td>UTS generation - LiveIntent 1 day 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-YZ5WJ2Y7WW4H</td><td>UTS generation - TextNow Full Data 7/7/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3F246IATX40Z</td><td>UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-D0W606FYMZDR</td><td>jada - Unity Iceberg Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2O70N006Y1DEP</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2BE432OSC64KA</td><td>UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 - Filter by Participant ID</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-370LX2NQQIX66</td><td>sgk cluster 2025-06-19</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-IFPFURHW6UQE</td><td>DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-17T00:00:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-17SP6VUGUCVA4</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3F9HP7BISWS6F</td><td>spark-canary-job-aws @ 2025-06-19T14:11:42.088135+00:00 :: Airflow</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1W9CI5E0Y8Z3W</td><td>gautam.pulla - Unity partitioned external-table Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1WIAROADJFG40</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-31URMH9THDVMV</td><td>UTS generation - LiveIntent, DirecTV,  Plex, Ibotta, TextNow 6/23/25</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3DHLYXMVV4XKI</td><td>UTS generation - LiveIntent 1 day 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-17JTKGO5QY7EM</td><td>UTS generation - LiveIntent 1 day 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-4OX783O3ON2P</td><td>Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3R9PRPTDBLCS0</td><td>UTS generation - LiveIntent 1 day 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-30719V08ACJVN</td><td>UTS generation - LiveIntent 1 day 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3UN62VYOM92I6</td><td>UTS generation - LiveIntent 1 day 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1SFUFZGK0HWF7</td><td>spark-canary-job-aws @ 2025-06-18T19:48:57.105802+00:00 :: Airflow</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-JAODY99JNLN7</td><td>UTS generation - LiveIntent 1 day 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3JK2ZVMJD698W</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2NVYDVV2LI5TP</td><td>jada - Unity Iceberg Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3AJM6NQQA9Q0N</td><td>UTS generation - TextNow 6/23/25</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1WRXNLW9CGLNT</td><td>UTS generation - LiveIntent 1 day 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1GP4NGMN98OJP</td><td>UTS generation - LiveIntent, DirecTV,  Plex, Ibotta, TextNow 6/23/25 - Try 2</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3960TCK1GPKPH</td><td>gautam.pulla - Unity partitioned external-table Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-3FPZE8W1C68Q1</td><td>gautam.pulla - Unity external-table Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1YDSO7T9DRMB0</td><td>gautam.pulla - Unity partitioned external-table Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1OLQ7ZOBKKFIN</td><td>Gautam: CleanroomCompactedAvails Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1DBW31F0FS3S9</td><td>jada - Unity Iceberg Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1N3NSNTANTUDT</td><td>UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1KRXP00BY3HII</td><td>Gautam: CleanroomCompactedAvails Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1RPZF333MAXMX</td><td>UTS generation - LiveIntent 7/2/25 </td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2PRR8Q2VF0DPI</td><td>UTS generation - TextNow 6/23/25</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2NGXIOFWCFTE5</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2PS13W9CCXK6G</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-U0M0Q7GY5T57</td><td>UTS generation - TextNow 6/23/25</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-14BMRHQXFT9YD</td><td>jada - Unity Iceberg Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1DVUR32ZX57G8</td><td>spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-ADQBIJVXZCXT</td><td>UTS generation - TextNow 6/23/25</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "j-ZBIDJZSYAUC5",
         "spark-canary-job-aws @ 2025-08-26T19:23:33.087323+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-zbidjzsyauc5.emrappui-prod.us-east-1.amazonaws.com/shs/",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-XWR1ATD9D25P",
         "KongmingETLClusterCalibration @ 2025-08-25T10:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "NO_APPLICATIONS",
         "No applications found in Spark History Server",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2LDTC0KC4RWZR",
         "spark-canary-job-aws @ 2025-08-29T17:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-CK04NK3092PH",
         "spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow :: Test",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1SRRJD4VIGE8I",
         "spark-canary-job-aws @ 2025-08-28T16:24:51.236322+00:00 :: Airflow :: Test",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1JGD3V2O0786C",
         "PreprocessPreBidCluster @ 2025-08-18T08:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-1jgd3v2o0786c.emrappui-prod.us-east-1.amazonaws.com/shs/",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1S25M8TOS4B9V",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         2,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-CJVZOKNNY5Y3",
         "PreprocessPreBidCluster @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-cjvzoknny5y3.emrappui-prod.us-east-1.amazonaws.com/shs/",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2LQQYXCT2O98L",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         2,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2BH4MH6ZYF1NN",
         "ContainmentRecordsPreBid @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2bh4mh6zyf1nn.emrappui-prod.us-east-1.amazonaws.com/shs/",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-FPBBMU8488M3",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-fpbbmu8488m3.emrappui-prod.us-east-1.amazonaws.com/shs/",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2FM0B0MTGHLAR",
         "DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2fm0b0mtghlar.emrappui-prod.us-east-1.amazonaws.com/shs/",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2A3AEORSGBMXV",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2a3aeorsgbmxv.emrappui-prod.us-east-1.amazonaws.com/shs/",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-310AFAY6L57Q",
         "Gautam: UserId Seller Agg",
         "TERMINATED",
         "NO_APPLICATIONS",
         "No applications found in Spark History Server",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-N11M3Y9IQDEM",
         "spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2C9L9Y9WYQ6H8",
         "spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2HLY3YSX5FF6C",
         "ContainmentRecordsPreBid @ 2025-08-19T08:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-2hly3ysx5ff6c.emrappui-prod.us-east-1.amazonaws.com/shs/",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-137KTJ1T8J5TS",
         "spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2F9HRWX3WB2KP",
         "spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1KPXX0HR3U0BN",
         "Gautam: CleanRoomUserIdSellerAgg",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1VJLYTUUO02M0",
         "Gautam: UserId Seller Agg",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1ILKHEQ510RAI",
         "Gautam: CleanRoomUserIdSellerAgg",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3UI2PXNPHBW5I",
         "Gautam: UserId Seller Agg",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: OK; sql: OK; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2XGZVZTB95DO0",
         "spark-canary-job-aws @ 2025-08-25T17:03:33.192584+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1HX6NYOHT3PI3",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         4,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2LX7JEDEA2NXY",
         "DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "NO_APPLICATIONS",
         "No applications found in Spark History Server",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-160Y1JZF990JG",
         "spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED; executors: FAILED",
         1,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3NETQ5HD28KNN",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: 502 Server Error: Bad Gateway for url: https://p-3netq5hd28knn.emrappui-prod.us-east-1.amazonaws.com/shs/",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-13HHLAF8A203N",
         "spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-25DYH14KZZDDX",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-YY2WFXRN6QC8",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1BNIJAGUY895W",
         "DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-UHXFK8IWP75A",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-38XT10OQKQR6U",
         "Gautam: UserId Seller Agg",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3EUQOZY9A601C",
         "datalake3-bidrequest-aws @ 2025-07-15T15:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-7HUGX0HL91O2",
         "DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-31T00:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-16RJMT0M52OLX",
         "spark-canary-job-aws @ 2025-06-19T16:18:19.037593+00:00 :: Airflow",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-335XER9MDXKZ8",
         "Gautam: CleanroomCompactedAvails Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2MZTITEYFO818",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-34S41A7LRQKR0",
         "Gautam: CleanroomCompactedAvails Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-8J1WEFG7Z21J",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-7NJ8DENAWJQQ",
         "jada - Unity Iceberg Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3LH544P1CI67",
         "UTS generation - TextNow 6/23/25",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3DCM9RMK4Q8GJ",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3TLPAYHCIN61J",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-29FBX9V6XEM1L",
         "UTS generation - TextNow 6/23/25",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1FWDQ2DBFG222",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1980W1IPAZJAL",
         "gautam.pulla - Unity partitioned external-table Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2XV5ZOFEU2KJZ",
         "UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 - Filter by Participant ID",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-9FI52YABYLF3",
         "spark-canary-job-aws @ 2025-06-19T17:05:12.714264+00:00 :: Airflow",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1Z61L282MMAP9",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3AU5YZH2R4A1S",
         "gautam.pulla - Unity partitioned external-table Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-89OZ40IG6D6F",
         "Gautam: ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2CBXLKXOFU3KD",
         "UTS generation - LiveIntent 1 day 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-VE8UZD5NPJ7B",
         "spark-canary-job-aws @ 2025-07-25T16:25:43.068986+00:00 :: Airflow",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-105SOVTHBNXL4",
         "spark-canary-job-aws @ 2025-07-14T10:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-UBW66RQAYLKY",
         "Gautam: CleanroomCompactedAvails Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2L64PD24BVEUB",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-PQW0MEADIC19",
         "Gautam: CleanroomCompactedAvails Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3QEKGBE785PV3",
         "UTS generation - LiveIntent 1 day 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-YZ5WJ2Y7WW4H",
         "UTS generation - TextNow Full Data 7/7/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3F246IATX40Z",
         "UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-D0W606FYMZDR",
         "jada - Unity Iceberg Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2O70N006Y1DEP",
         "Gautam: CleanRoomUserIdSellerAgg",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2BE432OSC64KA",
         "UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 - Filter by Participant ID",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-370LX2NQQIX66",
         "sgk cluster 2025-06-19",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-IFPFURHW6UQE",
         "DATPRD-SegGen-BrandSentiment-DailyIngestion @ 2025-07-17T00:00:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-17SP6VUGUCVA4",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3F9HP7BISWS6F",
         "spark-canary-job-aws @ 2025-06-19T14:11:42.088135+00:00 :: Airflow",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1W9CI5E0Y8Z3W",
         "gautam.pulla - Unity partitioned external-table Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1WIAROADJFG40",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-31URMH9THDVMV",
         "UTS generation - LiveIntent, DirecTV,  Plex, Ibotta, TextNow 6/23/25",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3DHLYXMVV4XKI",
         "UTS generation - LiveIntent 1 day 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-17JTKGO5QY7EM",
         "UTS generation - LiveIntent 1 day 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-4OX783O3ON2P",
         "Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3R9PRPTDBLCS0",
         "UTS generation - LiveIntent 1 day 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-30719V08ACJVN",
         "UTS generation - LiveIntent 1 day 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3UN62VYOM92I6",
         "UTS generation - LiveIntent 1 day 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1SFUFZGK0HWF7",
         "spark-canary-job-aws @ 2025-06-18T19:48:57.105802+00:00 :: Airflow",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-JAODY99JNLN7",
         "UTS generation - LiveIntent 1 day 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3JK2ZVMJD698W",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2NVYDVV2LI5TP",
         "jada - Unity Iceberg Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3AJM6NQQA9Q0N",
         "UTS generation - TextNow 6/23/25",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1WRXNLW9CGLNT",
         "UTS generation - LiveIntent 1 day 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1GP4NGMN98OJP",
         "UTS generation - LiveIntent, DirecTV,  Plex, Ibotta, TextNow 6/23/25 - Try 2",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3960TCK1GPKPH",
         "gautam.pulla - Unity partitioned external-table Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-3FPZE8W1C68Q1",
         "gautam.pulla - Unity external-table Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1YDSO7T9DRMB0",
         "gautam.pulla - Unity partitioned external-table Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1OLQ7ZOBKKFIN",
         "Gautam: CleanroomCompactedAvails Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1DBW31F0FS3S9",
         "jada - Unity Iceberg Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1N3NSNTANTUDT",
         "UTS generation - TextNow, DirecTV, LiveIntent 7/14/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1KRXP00BY3HII",
         "Gautam: CleanroomCompactedAvails Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1RPZF333MAXMX",
         "UTS generation - LiveIntent 7/2/25 ",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2PRR8Q2VF0DPI",
         "UTS generation - TextNow 6/23/25",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2NGXIOFWCFTE5",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-2PS13W9CCXK6G",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-U0M0Q7GY5T57",
         "UTS generation - TextNow 6/23/25",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-14BMRHQXFT9YD",
         "jada - Unity Iceberg Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-1DVUR32ZX57G8",
         "spark-canary-job-aws @ 2025-07-23T21:13:00+00:00 :: Airflow :: Test",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "j-ADQBIJVXZCXT",
         "UTS generation - TextNow 6/23/25",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 30 seconds.",
         0,
         0,
         0,
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cluster_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cluster_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "analysis_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status_details",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_applications",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_jobs",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_stages",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_tasks",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_sql_queries",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_executors",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(cluster_summaries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c596d8c-7ad3-40ed-a0e6-5c155e2856bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>appSparkVersion</th><th>application_id</th><th>application_name</th><th>attemptId</th><th>cluster_id</th><th>cluster_name</th><th>completed</th><th>duration</th><th>endTime</th><th>endTimeEpoch</th><th>lastUpdated</th><th>lastUpdatedEpoch</th><th>raw_json</th><th>sparkUser</th><th>startTime</th><th>startTimeEpoch</th></tr></thead><tbody><tr><td>3.3.2-amzn-0.1</td><td>application_1756490232506_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>1</td><td>j-2LDTC0KC4RWZR</td><td>spark-canary-job-aws @ 2025-08-29T17:13:00+00:00 :: Airflow :: Test</td><td>true</td><td>22520</td><td>2025-08-29T17:58:50.155GMT</td><td>1756490330155</td><td>2025-08-29T17:59:52.000GMT</td><td>1756490392000</td><td>{\"id\": \"application_1756490232506_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T17:58:27.635GMT\", \"endTime\": \"2025-08-29T17:58:50.155GMT\", \"lastUpdated\": \"2025-08-29T17:59:52.000GMT\", \"duration\": 22520, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756490307635, \"lastUpdatedEpoch\": 1756490392000, \"endTimeEpoch\": 1756490330155}]}</td><td>hadoop</td><td>2025-08-29T17:58:27.635GMT</td><td>1756490307635</td></tr><tr><td>3.3.2-amzn-0.1</td><td>application_1756238221578_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>1</td><td>j-CK04NK3092PH</td><td>spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow :: Test</td><td>true</td><td>21512</td><td>2025-08-26T19:58:44.682GMT</td><td>1756238324682</td><td>2025-08-26T19:59:39.000GMT</td><td>1756238379000</td><td>{\"id\": \"application_1756238221578_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-26T19:58:23.170GMT\", \"endTime\": \"2025-08-26T19:58:44.682GMT\", \"lastUpdated\": \"2025-08-26T19:59:39.000GMT\", \"duration\": 21512, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756238303170, \"lastUpdatedEpoch\": 1756238379000, \"endTimeEpoch\": 1756238324682}]}</td><td>hadoop</td><td>2025-08-26T19:58:23.170GMT</td><td>1756238303170</td></tr><tr><td>3.3.2-amzn-0.1</td><td>application_1756398594692_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>1</td><td>j-1SRRJD4VIGE8I</td><td>spark-canary-job-aws @ 2025-08-28T16:24:51.236322+00:00 :: Airflow :: Test</td><td>true</td><td>21735</td><td>2025-08-28T16:31:39.411GMT</td><td>1756398699411</td><td>2025-08-28T16:32:24.000GMT</td><td>1756398744000</td><td>{\"id\": \"application_1756398594692_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-28T16:31:17.676GMT\", \"endTime\": \"2025-08-28T16:31:39.411GMT\", \"lastUpdated\": \"2025-08-28T16:32:24.000GMT\", \"duration\": 21735, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756398677676, \"lastUpdatedEpoch\": 1756398744000, \"endTimeEpoch\": 1756398699411}]}</td><td>hadoop</td><td>2025-08-28T16:31:17.676GMT</td><td>1756398677676</td></tr><tr><td>3.5.5-amzn-0</td><td>application_1756435989506_0001</td><td>DailyScoreSet-class-job.DailyOfflineScoringSet</td><td>1</td><td>j-1S25M8TOS4B9V</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>true</td><td>319869</td><td>2025-08-29T03:00:16.549GMT</td><td>1756436416549</td><td>2025-08-29T03:01:30.000GMT</td><td>1756436490000</td><td>{\"id\": \"application_1756435989506_0001\", \"name\": \"DailyScoreSet-class-job.DailyOfflineScoringSet\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T02:54:56.680GMT\", \"endTime\": \"2025-08-29T03:00:16.549GMT\", \"lastUpdated\": \"2025-08-29T03:01:30.000GMT\", \"duration\": 319869, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756436096680, \"lastUpdatedEpoch\": 1756436490000, \"endTimeEpoch\": 1756436416549}]}</td><td>hadoop</td><td>2025-08-29T02:54:56.680GMT</td><td>1756436096680</td></tr><tr><td>3.5.5-amzn-0</td><td>application_1756435989506_0002</td><td>DailyAttributedEvents-class-job.DailyAttributedEvents</td><td>1</td><td>j-1S25M8TOS4B9V</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>false</td><td>0</td><td>1969-12-31T23:59:59.999GMT</td><td>-1</td><td>2025-08-29T18:27:27.454GMT</td><td>1756492047454</td><td>{\"id\": \"application_1756435989506_0002\", \"name\": \"DailyAttributedEvents-class-job.DailyAttributedEvents\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:00:39.179GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:27.454GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756436439179, \"lastUpdatedEpoch\": 1756492047454, \"endTimeEpoch\": -1}]}</td><td>hadoop</td><td>2025-08-29T03:00:39.179GMT</td><td>1756436439179</td></tr><tr><td>3.5.5-amzn-0</td><td>application_1756435822132_0002</td><td>DailyAttributedEvents-class-job.DailyAttributedEvents</td><td>1</td><td>j-2LQQYXCT2O98L</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>true</td><td>65920</td><td>2025-08-29T02:59:10.706GMT</td><td>1756436350706</td><td>2025-08-29T03:01:48.000GMT</td><td>1756436508000</td><td>{\"id\": \"application_1756435822132_0002\", \"name\": \"DailyAttributedEvents-class-job.DailyAttributedEvents\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T02:58:04.786GMT\", \"endTime\": \"2025-08-29T02:59:10.706GMT\", \"lastUpdated\": \"2025-08-29T03:01:48.000GMT\", \"duration\": 65920, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756436284786, \"lastUpdatedEpoch\": 1756436508000, \"endTimeEpoch\": 1756436350706}]}</td><td>hadoop</td><td>2025-08-29T02:58:04.786GMT</td><td>1756436284786</td></tr><tr><td>3.5.5-amzn-0</td><td>application_1756435822132_0001</td><td>DailyScoreSet-class-job.DailyOfflineScoringSet</td><td>1</td><td>j-2LQQYXCT2O98L</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>true</td><td>310995</td><td>2025-08-29T02:57:36.755GMT</td><td>1756436256755</td><td>2025-08-29T03:01:43.000GMT</td><td>1756436503000</td><td>{\"id\": \"application_1756435822132_0001\", \"name\": \"DailyScoreSet-class-job.DailyOfflineScoringSet\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T02:52:25.760GMT\", \"endTime\": \"2025-08-29T02:57:36.755GMT\", \"lastUpdated\": \"2025-08-29T03:01:43.000GMT\", \"duration\": 310995, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756435945760, \"lastUpdatedEpoch\": 1756436503000, \"endTimeEpoch\": 1756436256755}]}</td><td>hadoop</td><td>2025-08-29T02:52:25.760GMT</td><td>1756435945760</td></tr><tr><td>3.3.2-amzn-0.1</td><td>application_1756142359759_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>1</td><td>j-N11M3Y9IQDEM</td><td>spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow</td><td>true</td><td>20911</td><td>2025-08-25T17:20:54.060GMT</td><td>1756142454060</td><td>2025-08-25T17:22:01.000GMT</td><td>1756142521000</td><td>{\"id\": \"application_1756142359759_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-25T17:20:33.149GMT\", \"endTime\": \"2025-08-25T17:20:54.060GMT\", \"lastUpdated\": \"2025-08-25T17:22:01.000GMT\", \"duration\": 20911, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756142433149, \"lastUpdatedEpoch\": 1756142521000, \"endTimeEpoch\": 1756142454060}]}</td><td>hadoop</td><td>2025-08-25T17:20:33.149GMT</td><td>1756142433149</td></tr><tr><td>3.3.2-amzn-0.1</td><td>application_1755540700327_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>1</td><td>j-2C9L9Y9WYQ6H8</td><td>spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow</td><td>true</td><td>22014</td><td>2025-08-18T18:13:51.696GMT</td><td>1755540831696</td><td>2025-08-18T18:15:12.000GMT</td><td>1755540912000</td><td>{\"id\": \"application_1755540700327_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-18T18:13:29.682GMT\", \"endTime\": \"2025-08-18T18:13:51.696GMT\", \"lastUpdated\": \"2025-08-18T18:15:12.000GMT\", \"duration\": 22014, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1755540809682, \"lastUpdatedEpoch\": 1755540912000, \"endTimeEpoch\": 1755540831696}]}</td><td>hadoop</td><td>2025-08-18T18:13:29.682GMT</td><td>1755540809682</td></tr><tr><td>3.3.2-amzn-0.1</td><td>application_1755296864657_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>1</td><td>j-137KTJ1T8J5TS</td><td>spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow</td><td>true</td><td>20543</td><td>2025-08-15T22:29:13.894GMT</td><td>1755296953894</td><td>2025-08-15T22:30:30.000GMT</td><td>1755297030000</td><td>{\"id\": \"application_1755296864657_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-15T22:28:53.351GMT\", \"endTime\": \"2025-08-15T22:29:13.894GMT\", \"lastUpdated\": \"2025-08-15T22:30:30.000GMT\", \"duration\": 20543, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1755296933351, \"lastUpdatedEpoch\": 1755297030000, \"endTimeEpoch\": 1755296953894}]}</td><td>hadoop</td><td>2025-08-15T22:28:53.351GMT</td><td>1755296933351</td></tr><tr><td>3.3.2-amzn-0.1</td><td>application_1755529524344_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>1</td><td>j-2F9HRWX3WB2KP</td><td>spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow</td><td>true</td><td>21326</td><td>2025-08-18T15:07:01.907GMT</td><td>1755529621907</td><td>2025-08-18T15:07:56.000GMT</td><td>1755529676000</td><td>{\"id\": \"application_1755529524344_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-18T15:06:40.581GMT\", \"endTime\": \"2025-08-18T15:07:01.907GMT\", \"lastUpdated\": \"2025-08-18T15:07:56.000GMT\", \"duration\": 21326, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1755529600581, \"lastUpdatedEpoch\": 1755529676000, \"endTimeEpoch\": 1755529621907}]}</td><td>hadoop</td><td>2025-08-18T15:06:40.581GMT</td><td>1755529600581</td></tr><tr><td>3.5.3-amzn-1</td><td>application_1754583978065_0001</td><td>CleanRoomUserIdSellerAgg</td><td>1</td><td>j-1KPXX0HR3U0BN</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>false</td><td>0</td><td>1969-12-31T23:59:59.999GMT</td><td>-1</td><td>2025-08-29T18:27:33.140GMT</td><td>1756492053140</td><td>{\"id\": \"application_1754583978065_0001\", \"name\": \"CleanRoomUserIdSellerAgg\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-07T16:28:10.196GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:33.140GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754584090196, \"lastUpdatedEpoch\": 1756492053140, \"endTimeEpoch\": -1}]}</td><td>hadoop</td><td>2025-08-07T16:28:10.196GMT</td><td>1754584090196</td></tr><tr><td>3.5.3-amzn-1</td><td>application_1754533647407_0001</td><td>CleanRoomExplodedDealAvailsAggTransformer</td><td>1</td><td>j-1VJLYTUUO02M0</td><td>Gautam: UserId Seller Agg</td><td>false</td><td>0</td><td>1969-12-31T23:59:59.999GMT</td><td>-1</td><td>2025-08-29T18:27:32.649GMT</td><td>1756492052649</td><td>{\"id\": \"application_1754533647407_0001\", \"name\": \"CleanRoomExplodedDealAvailsAggTransformer\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-07T02:29:10.021GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:32.649GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754533750021, \"lastUpdatedEpoch\": 1756492052649, \"endTimeEpoch\": -1}]}</td><td>hadoop</td><td>2025-08-07T02:29:10.021GMT</td><td>1754533750021</td></tr><tr><td>3.5.3-amzn-1</td><td>application_1754586783613_0001</td><td>CleanRoomUserIdSellerAgg</td><td>1</td><td>j-1ILKHEQ510RAI</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>true</td><td>2067182</td><td>2025-08-07T17:49:23.215GMT</td><td>1754588963215</td><td>2025-08-07T17:50:42.000GMT</td><td>1754589042000</td><td>{\"id\": \"application_1754586783613_0001\", \"name\": \"CleanRoomUserIdSellerAgg\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-07T17:14:56.033GMT\", \"endTime\": \"2025-08-07T17:49:23.215GMT\", \"lastUpdated\": \"2025-08-07T17:50:42.000GMT\", \"duration\": 2067182, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754586896033, \"lastUpdatedEpoch\": 1754589042000, \"endTimeEpoch\": 1754588963215}]}</td><td>hadoop</td><td>2025-08-07T17:14:56.033GMT</td><td>1754586896033</td></tr><tr><td>3.5.3-amzn-1</td><td>application_1754529907328_0001</td><td>CleanRoomExplodedDealAvailsAggTransformer</td><td>2</td><td>j-3UI2PXNPHBW5I</td><td>Gautam: UserId Seller Agg</td><td>false</td><td>0</td><td>1969-12-31T23:59:59.999GMT</td><td>-1</td><td>2025-08-29T18:27:41.211GMT</td><td>1756492061211</td><td>{\"id\": \"application_1754529907328_0001\", \"name\": \"CleanRoomExplodedDealAvailsAggTransformer\", \"attempts\": [{\"attemptId\": \"2\", \"startTime\": \"2025-08-07T01:29:43.109GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:41.211GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754530183109, \"lastUpdatedEpoch\": 1756492061211, \"endTimeEpoch\": -1}, {\"attemptId\": \"1\", \"startTime\": \"2025-08-07T01:26:39.098GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:40.738GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754529999098, \"lastUpdatedEpoch\": 1756492060738, \"endTimeEpoch\": -1}]}</td><td>hadoop</td><td>2025-08-07T01:29:43.109GMT</td><td>1754530183109</td></tr><tr><td>3.5.3-amzn-1</td><td>application_1754529907328_0001</td><td>CleanRoomExplodedDealAvailsAggTransformer</td><td>1</td><td>j-3UI2PXNPHBW5I</td><td>Gautam: UserId Seller Agg</td><td>false</td><td>0</td><td>1969-12-31T23:59:59.999GMT</td><td>-1</td><td>2025-08-29T18:27:40.738GMT</td><td>1756492060738</td><td>{\"id\": \"application_1754529907328_0001\", \"name\": \"CleanRoomExplodedDealAvailsAggTransformer\", \"attempts\": [{\"attemptId\": \"2\", \"startTime\": \"2025-08-07T01:29:43.109GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:41.211GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754530183109, \"lastUpdatedEpoch\": 1756492061211, \"endTimeEpoch\": -1}, {\"attemptId\": \"1\", \"startTime\": \"2025-08-07T01:26:39.098GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:40.738GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754529999098, \"lastUpdatedEpoch\": 1756492060738, \"endTimeEpoch\": -1}]}</td><td>hadoop</td><td>2025-08-07T01:26:39.098GMT</td><td>1754529999098</td></tr><tr><td>3.3.2-amzn-0.1</td><td>application_1756141648487_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>1</td><td>j-2XGZVZTB95DO0</td><td>spark-canary-job-aws @ 2025-08-25T17:03:33.192584+00:00 :: Airflow</td><td>true</td><td>21915</td><td>2025-08-25T17:09:45.688GMT</td><td>1756141785688</td><td>2025-08-25T17:11:01.000GMT</td><td>1756141861000</td><td>{\"id\": \"application_1756141648487_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-25T17:09:23.773GMT\", \"endTime\": \"2025-08-25T17:09:45.688GMT\", \"lastUpdated\": \"2025-08-25T17:11:01.000GMT\", \"duration\": 21915, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756141763773, \"lastUpdatedEpoch\": 1756141861000, \"endTimeEpoch\": 1756141785688}]}</td><td>hadoop</td><td>2025-08-25T17:09:23.773GMT</td><td>1756141763773</td></tr><tr><td>3.5.5-amzn-0</td><td>application_1756436818864_0004</td><td>DailyConversion-class-job.ConversionDataDailyProcessor</td><td>1</td><td>j-1HX6NYOHT3PI3</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>true</td><td>192007</td><td>2025-08-29T03:23:38.148GMT</td><td>1756437818148</td><td>2025-08-29T03:24:49.000GMT</td><td>1756437889000</td><td>{\"id\": \"application_1756436818864_0004\", \"name\": \"DailyConversion-class-job.ConversionDataDailyProcessor\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:20:26.141GMT\", \"endTime\": \"2025-08-29T03:23:38.148GMT\", \"lastUpdated\": \"2025-08-29T03:24:49.000GMT\", \"duration\": 192007, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756437626141, \"lastUpdatedEpoch\": 1756437889000, \"endTimeEpoch\": 1756437818148}]}</td><td>hadoop</td><td>2025-08-29T03:20:26.141GMT</td><td>1756437626141</td></tr><tr><td>3.5.5-amzn-0</td><td>application_1756436818864_0003</td><td>DailyFeedbackSignals-class-job.DailyFeedbackSignals</td><td>1</td><td>j-1HX6NYOHT3PI3</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>true</td><td>142049</td><td>2025-08-29T03:20:03.289GMT</td><td>1756437603289</td><td>2025-08-29T03:24:54.000GMT</td><td>1756437894000</td><td>{\"id\": \"application_1756436818864_0003\", \"name\": \"DailyFeedbackSignals-class-job.DailyFeedbackSignals\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:17:41.240GMT\", \"endTime\": \"2025-08-29T03:20:03.289GMT\", \"lastUpdated\": \"2025-08-29T03:24:54.000GMT\", \"duration\": 142049, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756437461240, \"lastUpdatedEpoch\": 1756437894000, \"endTimeEpoch\": 1756437603289}]}</td><td>hadoop</td><td>2025-08-29T03:17:41.240GMT</td><td>1756437461240</td></tr><tr><td>3.5.5-amzn-0</td><td>application_1756436818864_0002</td><td>DailyAttributedEvents-class-job.DailyAttributedEvents</td><td>1</td><td>j-1HX6NYOHT3PI3</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>true</td><td>57899</td><td>2025-08-29T03:15:33.698GMT</td><td>1756437333698</td><td>2025-08-29T03:24:52.000GMT</td><td>1756437892000</td><td>{\"id\": \"application_1756436818864_0002\", \"name\": \"DailyAttributedEvents-class-job.DailyAttributedEvents\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:14:35.799GMT\", \"endTime\": \"2025-08-29T03:15:33.698GMT\", \"lastUpdated\": \"2025-08-29T03:24:52.000GMT\", \"duration\": 57899, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756437275799, \"lastUpdatedEpoch\": 1756437892000, \"endTimeEpoch\": 1756437333698}]}</td><td>hadoop</td><td>2025-08-29T03:14:35.799GMT</td><td>1756437275799</td></tr><tr><td>3.5.5-amzn-0</td><td>application_1756436818864_0001</td><td>DailyScoreSet-class-job.DailyOfflineScoringSet</td><td>1</td><td>j-1HX6NYOHT3PI3</td><td>KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test</td><td>true</td><td>328898</td><td>2025-08-29T03:14:09.491GMT</td><td>1756437249491</td><td>2025-08-29T03:24:50.000GMT</td><td>1756437890000</td><td>{\"id\": \"application_1756436818864_0001\", \"name\": \"DailyScoreSet-class-job.DailyOfflineScoringSet\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:08:40.593GMT\", \"endTime\": \"2025-08-29T03:14:09.491GMT\", \"lastUpdated\": \"2025-08-29T03:24:50.000GMT\", \"duration\": 328898, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756436920593, \"lastUpdatedEpoch\": 1756437890000, \"endTimeEpoch\": 1756437249491}]}</td><td>hadoop</td><td>2025-08-29T03:08:40.593GMT</td><td>1756436920593</td></tr><tr><td>3.3.2-amzn-0.1</td><td>application_1755294357227_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>1</td><td>j-160Y1JZF990JG</td><td>spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow</td><td>true</td><td>21453</td><td>2025-08-15T21:47:34.910GMT</td><td>1755294454910</td><td>2025-08-15T21:48:30.000GMT</td><td>1755294510000</td><td>{\"id\": \"application_1755294357227_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-15T21:47:13.457GMT\", \"endTime\": \"2025-08-15T21:47:34.910GMT\", \"lastUpdated\": \"2025-08-15T21:48:30.000GMT\", \"duration\": 21453, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1755294433457, \"lastUpdatedEpoch\": 1755294510000, \"endTimeEpoch\": 1755294454910}]}</td><td>hadoop</td><td>2025-08-15T21:47:13.457GMT</td><td>1755294433457</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "3.3.2-amzn-0.1",
         "application_1756490232506_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         "1",
         "j-2LDTC0KC4RWZR",
         "spark-canary-job-aws @ 2025-08-29T17:13:00+00:00 :: Airflow :: Test",
         true,
         22520,
         "2025-08-29T17:58:50.155GMT",
         1756490330155,
         "2025-08-29T17:59:52.000GMT",
         1756490392000,
         "{\"id\": \"application_1756490232506_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T17:58:27.635GMT\", \"endTime\": \"2025-08-29T17:58:50.155GMT\", \"lastUpdated\": \"2025-08-29T17:59:52.000GMT\", \"duration\": 22520, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756490307635, \"lastUpdatedEpoch\": 1756490392000, \"endTimeEpoch\": 1756490330155}]}",
         "hadoop",
         "2025-08-29T17:58:27.635GMT",
         1756490307635
        ],
        [
         "3.3.2-amzn-0.1",
         "application_1756238221578_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         "1",
         "j-CK04NK3092PH",
         "spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow :: Test",
         true,
         21512,
         "2025-08-26T19:58:44.682GMT",
         1756238324682,
         "2025-08-26T19:59:39.000GMT",
         1756238379000,
         "{\"id\": \"application_1756238221578_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-26T19:58:23.170GMT\", \"endTime\": \"2025-08-26T19:58:44.682GMT\", \"lastUpdated\": \"2025-08-26T19:59:39.000GMT\", \"duration\": 21512, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756238303170, \"lastUpdatedEpoch\": 1756238379000, \"endTimeEpoch\": 1756238324682}]}",
         "hadoop",
         "2025-08-26T19:58:23.170GMT",
         1756238303170
        ],
        [
         "3.3.2-amzn-0.1",
         "application_1756398594692_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         "1",
         "j-1SRRJD4VIGE8I",
         "spark-canary-job-aws @ 2025-08-28T16:24:51.236322+00:00 :: Airflow :: Test",
         true,
         21735,
         "2025-08-28T16:31:39.411GMT",
         1756398699411,
         "2025-08-28T16:32:24.000GMT",
         1756398744000,
         "{\"id\": \"application_1756398594692_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-28T16:31:17.676GMT\", \"endTime\": \"2025-08-28T16:31:39.411GMT\", \"lastUpdated\": \"2025-08-28T16:32:24.000GMT\", \"duration\": 21735, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756398677676, \"lastUpdatedEpoch\": 1756398744000, \"endTimeEpoch\": 1756398699411}]}",
         "hadoop",
         "2025-08-28T16:31:17.676GMT",
         1756398677676
        ],
        [
         "3.5.5-amzn-0",
         "application_1756435989506_0001",
         "DailyScoreSet-class-job.DailyOfflineScoringSet",
         "1",
         "j-1S25M8TOS4B9V",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         true,
         319869,
         "2025-08-29T03:00:16.549GMT",
         1756436416549,
         "2025-08-29T03:01:30.000GMT",
         1756436490000,
         "{\"id\": \"application_1756435989506_0001\", \"name\": \"DailyScoreSet-class-job.DailyOfflineScoringSet\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T02:54:56.680GMT\", \"endTime\": \"2025-08-29T03:00:16.549GMT\", \"lastUpdated\": \"2025-08-29T03:01:30.000GMT\", \"duration\": 319869, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756436096680, \"lastUpdatedEpoch\": 1756436490000, \"endTimeEpoch\": 1756436416549}]}",
         "hadoop",
         "2025-08-29T02:54:56.680GMT",
         1756436096680
        ],
        [
         "3.5.5-amzn-0",
         "application_1756435989506_0002",
         "DailyAttributedEvents-class-job.DailyAttributedEvents",
         "1",
         "j-1S25M8TOS4B9V",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         false,
         0,
         "1969-12-31T23:59:59.999GMT",
         -1,
         "2025-08-29T18:27:27.454GMT",
         1756492047454,
         "{\"id\": \"application_1756435989506_0002\", \"name\": \"DailyAttributedEvents-class-job.DailyAttributedEvents\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:00:39.179GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:27.454GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756436439179, \"lastUpdatedEpoch\": 1756492047454, \"endTimeEpoch\": -1}]}",
         "hadoop",
         "2025-08-29T03:00:39.179GMT",
         1756436439179
        ],
        [
         "3.5.5-amzn-0",
         "application_1756435822132_0002",
         "DailyAttributedEvents-class-job.DailyAttributedEvents",
         "1",
         "j-2LQQYXCT2O98L",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         true,
         65920,
         "2025-08-29T02:59:10.706GMT",
         1756436350706,
         "2025-08-29T03:01:48.000GMT",
         1756436508000,
         "{\"id\": \"application_1756435822132_0002\", \"name\": \"DailyAttributedEvents-class-job.DailyAttributedEvents\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T02:58:04.786GMT\", \"endTime\": \"2025-08-29T02:59:10.706GMT\", \"lastUpdated\": \"2025-08-29T03:01:48.000GMT\", \"duration\": 65920, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756436284786, \"lastUpdatedEpoch\": 1756436508000, \"endTimeEpoch\": 1756436350706}]}",
         "hadoop",
         "2025-08-29T02:58:04.786GMT",
         1756436284786
        ],
        [
         "3.5.5-amzn-0",
         "application_1756435822132_0001",
         "DailyScoreSet-class-job.DailyOfflineScoringSet",
         "1",
         "j-2LQQYXCT2O98L",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         true,
         310995,
         "2025-08-29T02:57:36.755GMT",
         1756436256755,
         "2025-08-29T03:01:43.000GMT",
         1756436503000,
         "{\"id\": \"application_1756435822132_0001\", \"name\": \"DailyScoreSet-class-job.DailyOfflineScoringSet\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T02:52:25.760GMT\", \"endTime\": \"2025-08-29T02:57:36.755GMT\", \"lastUpdated\": \"2025-08-29T03:01:43.000GMT\", \"duration\": 310995, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756435945760, \"lastUpdatedEpoch\": 1756436503000, \"endTimeEpoch\": 1756436256755}]}",
         "hadoop",
         "2025-08-29T02:52:25.760GMT",
         1756435945760
        ],
        [
         "3.3.2-amzn-0.1",
         "application_1756142359759_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         "1",
         "j-N11M3Y9IQDEM",
         "spark-canary-job-aws @ 2025-08-25T17:15:55.695896+00:00 :: Airflow",
         true,
         20911,
         "2025-08-25T17:20:54.060GMT",
         1756142454060,
         "2025-08-25T17:22:01.000GMT",
         1756142521000,
         "{\"id\": \"application_1756142359759_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-25T17:20:33.149GMT\", \"endTime\": \"2025-08-25T17:20:54.060GMT\", \"lastUpdated\": \"2025-08-25T17:22:01.000GMT\", \"duration\": 20911, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756142433149, \"lastUpdatedEpoch\": 1756142521000, \"endTimeEpoch\": 1756142454060}]}",
         "hadoop",
         "2025-08-25T17:20:33.149GMT",
         1756142433149
        ],
        [
         "3.3.2-amzn-0.1",
         "application_1755540700327_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         "1",
         "j-2C9L9Y9WYQ6H8",
         "spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow",
         true,
         22014,
         "2025-08-18T18:13:51.696GMT",
         1755540831696,
         "2025-08-18T18:15:12.000GMT",
         1755540912000,
         "{\"id\": \"application_1755540700327_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-18T18:13:29.682GMT\", \"endTime\": \"2025-08-18T18:13:51.696GMT\", \"lastUpdated\": \"2025-08-18T18:15:12.000GMT\", \"duration\": 22014, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1755540809682, \"lastUpdatedEpoch\": 1755540912000, \"endTimeEpoch\": 1755540831696}]}",
         "hadoop",
         "2025-08-18T18:13:29.682GMT",
         1755540809682
        ],
        [
         "3.3.2-amzn-0.1",
         "application_1755296864657_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         "1",
         "j-137KTJ1T8J5TS",
         "spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow",
         true,
         20543,
         "2025-08-15T22:29:13.894GMT",
         1755296953894,
         "2025-08-15T22:30:30.000GMT",
         1755297030000,
         "{\"id\": \"application_1755296864657_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-15T22:28:53.351GMT\", \"endTime\": \"2025-08-15T22:29:13.894GMT\", \"lastUpdated\": \"2025-08-15T22:30:30.000GMT\", \"duration\": 20543, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1755296933351, \"lastUpdatedEpoch\": 1755297030000, \"endTimeEpoch\": 1755296953894}]}",
         "hadoop",
         "2025-08-15T22:28:53.351GMT",
         1755296933351
        ],
        [
         "3.3.2-amzn-0.1",
         "application_1755529524344_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         "1",
         "j-2F9HRWX3WB2KP",
         "spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow",
         true,
         21326,
         "2025-08-18T15:07:01.907GMT",
         1755529621907,
         "2025-08-18T15:07:56.000GMT",
         1755529676000,
         "{\"id\": \"application_1755529524344_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-18T15:06:40.581GMT\", \"endTime\": \"2025-08-18T15:07:01.907GMT\", \"lastUpdated\": \"2025-08-18T15:07:56.000GMT\", \"duration\": 21326, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1755529600581, \"lastUpdatedEpoch\": 1755529676000, \"endTimeEpoch\": 1755529621907}]}",
         "hadoop",
         "2025-08-18T15:06:40.581GMT",
         1755529600581
        ],
        [
         "3.5.3-amzn-1",
         "application_1754583978065_0001",
         "CleanRoomUserIdSellerAgg",
         "1",
         "j-1KPXX0HR3U0BN",
         "Gautam: CleanRoomUserIdSellerAgg",
         false,
         0,
         "1969-12-31T23:59:59.999GMT",
         -1,
         "2025-08-29T18:27:33.140GMT",
         1756492053140,
         "{\"id\": \"application_1754583978065_0001\", \"name\": \"CleanRoomUserIdSellerAgg\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-07T16:28:10.196GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:33.140GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754584090196, \"lastUpdatedEpoch\": 1756492053140, \"endTimeEpoch\": -1}]}",
         "hadoop",
         "2025-08-07T16:28:10.196GMT",
         1754584090196
        ],
        [
         "3.5.3-amzn-1",
         "application_1754533647407_0001",
         "CleanRoomExplodedDealAvailsAggTransformer",
         "1",
         "j-1VJLYTUUO02M0",
         "Gautam: UserId Seller Agg",
         false,
         0,
         "1969-12-31T23:59:59.999GMT",
         -1,
         "2025-08-29T18:27:32.649GMT",
         1756492052649,
         "{\"id\": \"application_1754533647407_0001\", \"name\": \"CleanRoomExplodedDealAvailsAggTransformer\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-07T02:29:10.021GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:32.649GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754533750021, \"lastUpdatedEpoch\": 1756492052649, \"endTimeEpoch\": -1}]}",
         "hadoop",
         "2025-08-07T02:29:10.021GMT",
         1754533750021
        ],
        [
         "3.5.3-amzn-1",
         "application_1754586783613_0001",
         "CleanRoomUserIdSellerAgg",
         "1",
         "j-1ILKHEQ510RAI",
         "Gautam: CleanRoomUserIdSellerAgg",
         true,
         2067182,
         "2025-08-07T17:49:23.215GMT",
         1754588963215,
         "2025-08-07T17:50:42.000GMT",
         1754589042000,
         "{\"id\": \"application_1754586783613_0001\", \"name\": \"CleanRoomUserIdSellerAgg\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-07T17:14:56.033GMT\", \"endTime\": \"2025-08-07T17:49:23.215GMT\", \"lastUpdated\": \"2025-08-07T17:50:42.000GMT\", \"duration\": 2067182, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754586896033, \"lastUpdatedEpoch\": 1754589042000, \"endTimeEpoch\": 1754588963215}]}",
         "hadoop",
         "2025-08-07T17:14:56.033GMT",
         1754586896033
        ],
        [
         "3.5.3-amzn-1",
         "application_1754529907328_0001",
         "CleanRoomExplodedDealAvailsAggTransformer",
         "2",
         "j-3UI2PXNPHBW5I",
         "Gautam: UserId Seller Agg",
         false,
         0,
         "1969-12-31T23:59:59.999GMT",
         -1,
         "2025-08-29T18:27:41.211GMT",
         1756492061211,
         "{\"id\": \"application_1754529907328_0001\", \"name\": \"CleanRoomExplodedDealAvailsAggTransformer\", \"attempts\": [{\"attemptId\": \"2\", \"startTime\": \"2025-08-07T01:29:43.109GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:41.211GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754530183109, \"lastUpdatedEpoch\": 1756492061211, \"endTimeEpoch\": -1}, {\"attemptId\": \"1\", \"startTime\": \"2025-08-07T01:26:39.098GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:40.738GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754529999098, \"lastUpdatedEpoch\": 1756492060738, \"endTimeEpoch\": -1}]}",
         "hadoop",
         "2025-08-07T01:29:43.109GMT",
         1754530183109
        ],
        [
         "3.5.3-amzn-1",
         "application_1754529907328_0001",
         "CleanRoomExplodedDealAvailsAggTransformer",
         "1",
         "j-3UI2PXNPHBW5I",
         "Gautam: UserId Seller Agg",
         false,
         0,
         "1969-12-31T23:59:59.999GMT",
         -1,
         "2025-08-29T18:27:40.738GMT",
         1756492060738,
         "{\"id\": \"application_1754529907328_0001\", \"name\": \"CleanRoomExplodedDealAvailsAggTransformer\", \"attempts\": [{\"attemptId\": \"2\", \"startTime\": \"2025-08-07T01:29:43.109GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:41.211GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754530183109, \"lastUpdatedEpoch\": 1756492061211, \"endTimeEpoch\": -1}, {\"attemptId\": \"1\", \"startTime\": \"2025-08-07T01:26:39.098GMT\", \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"lastUpdated\": \"2025-08-29T18:27:40.738GMT\", \"duration\": 0, \"sparkUser\": \"hadoop\", \"completed\": false, \"appSparkVersion\": \"3.5.3-amzn-1\", \"startTimeEpoch\": 1754529999098, \"lastUpdatedEpoch\": 1756492060738, \"endTimeEpoch\": -1}]}",
         "hadoop",
         "2025-08-07T01:26:39.098GMT",
         1754529999098
        ],
        [
         "3.3.2-amzn-0.1",
         "application_1756141648487_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         "1",
         "j-2XGZVZTB95DO0",
         "spark-canary-job-aws @ 2025-08-25T17:03:33.192584+00:00 :: Airflow",
         true,
         21915,
         "2025-08-25T17:09:45.688GMT",
         1756141785688,
         "2025-08-25T17:11:01.000GMT",
         1756141861000,
         "{\"id\": \"application_1756141648487_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-25T17:09:23.773GMT\", \"endTime\": \"2025-08-25T17:09:45.688GMT\", \"lastUpdated\": \"2025-08-25T17:11:01.000GMT\", \"duration\": 21915, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1756141763773, \"lastUpdatedEpoch\": 1756141861000, \"endTimeEpoch\": 1756141785688}]}",
         "hadoop",
         "2025-08-25T17:09:23.773GMT",
         1756141763773
        ],
        [
         "3.5.5-amzn-0",
         "application_1756436818864_0004",
         "DailyConversion-class-job.ConversionDataDailyProcessor",
         "1",
         "j-1HX6NYOHT3PI3",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         true,
         192007,
         "2025-08-29T03:23:38.148GMT",
         1756437818148,
         "2025-08-29T03:24:49.000GMT",
         1756437889000,
         "{\"id\": \"application_1756436818864_0004\", \"name\": \"DailyConversion-class-job.ConversionDataDailyProcessor\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:20:26.141GMT\", \"endTime\": \"2025-08-29T03:23:38.148GMT\", \"lastUpdated\": \"2025-08-29T03:24:49.000GMT\", \"duration\": 192007, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756437626141, \"lastUpdatedEpoch\": 1756437889000, \"endTimeEpoch\": 1756437818148}]}",
         "hadoop",
         "2025-08-29T03:20:26.141GMT",
         1756437626141
        ],
        [
         "3.5.5-amzn-0",
         "application_1756436818864_0003",
         "DailyFeedbackSignals-class-job.DailyFeedbackSignals",
         "1",
         "j-1HX6NYOHT3PI3",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         true,
         142049,
         "2025-08-29T03:20:03.289GMT",
         1756437603289,
         "2025-08-29T03:24:54.000GMT",
         1756437894000,
         "{\"id\": \"application_1756436818864_0003\", \"name\": \"DailyFeedbackSignals-class-job.DailyFeedbackSignals\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:17:41.240GMT\", \"endTime\": \"2025-08-29T03:20:03.289GMT\", \"lastUpdated\": \"2025-08-29T03:24:54.000GMT\", \"duration\": 142049, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756437461240, \"lastUpdatedEpoch\": 1756437894000, \"endTimeEpoch\": 1756437603289}]}",
         "hadoop",
         "2025-08-29T03:17:41.240GMT",
         1756437461240
        ],
        [
         "3.5.5-amzn-0",
         "application_1756436818864_0002",
         "DailyAttributedEvents-class-job.DailyAttributedEvents",
         "1",
         "j-1HX6NYOHT3PI3",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         true,
         57899,
         "2025-08-29T03:15:33.698GMT",
         1756437333698,
         "2025-08-29T03:24:52.000GMT",
         1756437892000,
         "{\"id\": \"application_1756436818864_0002\", \"name\": \"DailyAttributedEvents-class-job.DailyAttributedEvents\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:14:35.799GMT\", \"endTime\": \"2025-08-29T03:15:33.698GMT\", \"lastUpdated\": \"2025-08-29T03:24:52.000GMT\", \"duration\": 57899, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756437275799, \"lastUpdatedEpoch\": 1756437892000, \"endTimeEpoch\": 1756437333698}]}",
         "hadoop",
         "2025-08-29T03:14:35.799GMT",
         1756437275799
        ],
        [
         "3.5.5-amzn-0",
         "application_1756436818864_0001",
         "DailyScoreSet-class-job.DailyOfflineScoringSet",
         "1",
         "j-1HX6NYOHT3PI3",
         "KongmingEtlCluster @ 2025-08-27T10:00:00+00:00 :: Airflow :: Test",
         true,
         328898,
         "2025-08-29T03:14:09.491GMT",
         1756437249491,
         "2025-08-29T03:24:50.000GMT",
         1756437890000,
         "{\"id\": \"application_1756436818864_0001\", \"name\": \"DailyScoreSet-class-job.DailyOfflineScoringSet\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-29T03:08:40.593GMT\", \"endTime\": \"2025-08-29T03:14:09.491GMT\", \"lastUpdated\": \"2025-08-29T03:24:50.000GMT\", \"duration\": 328898, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.5.5-amzn-0\", \"startTimeEpoch\": 1756436920593, \"lastUpdatedEpoch\": 1756437890000, \"endTimeEpoch\": 1756437249491}]}",
         "hadoop",
         "2025-08-29T03:08:40.593GMT",
         1756436920593
        ],
        [
         "3.3.2-amzn-0.1",
         "application_1755294357227_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         "1",
         "j-160Y1JZF990JG",
         "spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow",
         true,
         21453,
         "2025-08-15T21:47:34.910GMT",
         1755294454910,
         "2025-08-15T21:48:30.000GMT",
         1755294510000,
         "{\"id\": \"application_1755294357227_0001\", \"name\": \"CanaryPipeline-class-jobs.dataproc.canary.CanaryJob\", \"attempts\": [{\"attemptId\": \"1\", \"startTime\": \"2025-08-15T21:47:13.457GMT\", \"endTime\": \"2025-08-15T21:47:34.910GMT\", \"lastUpdated\": \"2025-08-15T21:48:30.000GMT\", \"duration\": 21453, \"sparkUser\": \"hadoop\", \"completed\": true, \"appSparkVersion\": \"3.3.2-amzn-0.1\", \"startTimeEpoch\": 1755294433457, \"lastUpdatedEpoch\": 1755294510000, \"endTimeEpoch\": 1755294454910}]}",
         "hadoop",
         "2025-08-15T21:47:13.457GMT",
         1755294433457
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "appSparkVersion",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "application_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "application_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attemptId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cluster_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cluster_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "completed",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "duration",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "endTime",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "endTimeEpoch",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "lastUpdated",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastUpdatedEpoch",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "raw_json",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sparkUser",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "startTime",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "startTimeEpoch",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(applications_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d97cb24-ca2e-4704-a5ae-2becbb2000c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(executors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c557931-2eb5-49c4-b43a-a3664a7394f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(jobs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3426ed49-3dc0-4e8f-9938-9c592a27377d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(stages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2984bc7-aef0-435a-8ca7-a4c2fa50cda8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(tasks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cdb52b-034d-42f5-a802-c28f3370d2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sql_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5745386645768922,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "aws_region",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "batch_size",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "cluster_name_filter",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "batch_delay_seconds",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "emr_cluster_arn",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "cluster_states",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "created_after_date",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "created_before_date",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "environment",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "max_applications",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "max_clusters",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "max_app_threads",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "max_cluster_threads",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "persistent_ui_timeout_seconds",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "timeout_seconds",
      "width": 195
     },
     {
      "breakBefore": false,
      "name": "s3_output_path",
      "width": 195
     }
    ]
   },
   "notebookName": "emr_spark_profiler",
   "widgets": {
    "aws_region": {
     "currentValue": "us-west-2",
     "nuid": "511e7e68-b1b4-4749-84ef-1ec405556c44",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "us-east-1",
      "label": "AWS Region",
      "name": "aws_region",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "us-east-1",
      "label": "AWS Region",
      "name": "aws_region",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "batch_delay_seconds": {
     "currentValue": "30",
     "nuid": "17b439be-9696-4d26-b104-6edf251b88a4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "120",
      "label": "Delay Between Batches (seconds)",
      "name": "batch_delay_seconds",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "120",
      "label": "Delay Between Batches (seconds)",
      "name": "batch_delay_seconds",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "batch_size": {
     "currentValue": "100",
     "nuid": "8060dd06-2abf-4f9e-abaf-367125e503b7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "50",
      "label": "Batch Size (clusters to process concurrently)",
      "name": "batch_size",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "50",
      "label": "Batch Size (clusters to process concurrently)",
      "name": "batch_size",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "cluster_name_filter": {
     "currentValue": "",
     "nuid": "b6dc4bb7-c099-48a6-9837-90276589e649",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Cluster Name Filter (optional - partial name match)",
      "name": "cluster_name_filter",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Cluster Name Filter (optional - partial name match)",
      "name": "cluster_name_filter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "cluster_states": {
     "currentValue": "TERMINATED",
     "nuid": "1d5498b2-b955-4ae9-a5c3-7eea2bed9753",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "TERMINATED,WAITING",
      "label": "EMR Cluster States to Analyze",
      "name": "cluster_states",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "TERMINATED",
        "WAITING",
        "TERMINATED,WAITING",
        "ALL"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "TERMINATED,WAITING",
      "label": "EMR Cluster States to Analyze",
      "name": "cluster_states",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "TERMINATED",
        "WAITING",
        "TERMINATED,WAITING",
        "ALL"
       ]
      }
     }
    },
    "created_after_date": {
     "currentValue": "",
     "nuid": "3b80c4ba-25ef-4015-9f10-4dc347dbcd33",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "EMR Clusters Created After (YYYY-MM-DD)",
      "name": "created_after_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "EMR Clusters Created After (YYYY-MM-DD)",
      "name": "created_after_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "created_before_date": {
     "currentValue": "",
     "nuid": "d33922e8-a59a-4c01-bd81-fb68b9626194",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "EMR Clusters Created Before (YYYY-MM-DD)",
      "name": "created_before_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "EMR Clusters Created Before (YYYY-MM-DD)",
      "name": "created_before_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "emr_cluster_arn": {
     "currentValue": "",
     "nuid": "622c9df7-a591-4bce-9304-c0ad4da31422",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "EMR Cluster ARN (optional - leave blank to discover clusters)",
      "name": "emr_cluster_arn",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "EMR Cluster ARN (optional - leave blank to discover clusters)",
      "name": "emr_cluster_arn",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "environment": {
     "currentValue": "dev",
     "nuid": "e6280d95-7b01-40f0-9a6c-48eadd1e04fa",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment (dev/prod)",
      "name": "environment",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment (dev/prod)",
      "name": "environment",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "prod"
       ]
      }
     }
    },
    "max_app_threads": {
     "currentValue": "50",
     "nuid": "5286d8e4-0626-4bae-ac7a-de5e903f3e9f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "100",
      "label": "Max Concurrent App Analysis Threads per Cluster",
      "name": "max_app_threads",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "100",
      "label": "Max Concurrent App Analysis Threads per Cluster",
      "name": "max_app_threads",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_applications": {
     "currentValue": "50",
     "nuid": "b755965a-8868-4bca-9257-35d1977e0c69",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "50",
      "label": "Max Applications to Analyze per Cluster",
      "name": "max_applications",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "50",
      "label": "Max Applications to Analyze per Cluster",
      "name": "max_applications",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_cluster_threads": {
     "currentValue": "100",
     "nuid": "f7ab2744-7a55-4986-b040-5d916859c9c0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "50",
      "label": "Max Concurrent Cluster Analysis Threads",
      "name": "max_cluster_threads",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "50",
      "label": "Max Concurrent Cluster Analysis Threads",
      "name": "max_cluster_threads",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_clusters": {
     "currentValue": "100",
     "nuid": "b22a3439-4c46-4f67-88d6-863f5b985180",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "5",
      "label": "Max Clusters to Analyze",
      "name": "max_clusters",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": "Max Clusters to Analyze",
      "name": "max_clusters",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "persistent_ui_timeout_seconds": {
     "currentValue": "30",
     "nuid": "c46034ae-d60c-445d-95c6-620cf31dd311",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "180",
      "label": "Persistent App UI Timeout (seconds)",
      "name": "persistent_ui_timeout_seconds",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "180",
      "label": "Persistent App UI Timeout (seconds)",
      "name": "persistent_ui_timeout_seconds",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_output_path": {
     "currentValue": "s3://ttd-vertica-backups/env=test/vertica-ext/databricks-stage/sparkmetrics",
     "nuid": "16520b78-96cc-48a8-b2cd-dc660e10c83f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "S3 Output Path (prod only)",
      "name": "s3_output_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "S3 Output Path (prod only)",
      "name": "s3_output_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "timeout_seconds": {
     "currentValue": "300",
     "nuid": "fc534296-f87a-4378-ad10-6778a2ddbd2c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "300",
      "label": "Request Timeout (seconds)",
      "name": "timeout_seconds",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "300",
      "label": "Request Timeout (seconds)",
      "name": "timeout_seconds",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}