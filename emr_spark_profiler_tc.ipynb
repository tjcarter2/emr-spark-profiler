{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7beb536e-20fd-4d00-b66e-3315a49409de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Run cell on cluster restart or if receiving error: \n",
    "# # AttributeError: 'EMR' object has no attribute 'create_persistent_app_ui'\n",
    "\n",
    "# %pip install --upgrade boto3 botocore\n",
    "# %restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705a0759-61ba-44ed-9262-02adec0e86b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## EMR Spark Event Log Analyzer\n",
    " \n",
    "This script analyzes EMR clusters to extract performance metrics from Spark History Servers. It discovers EMR clusters, connects to their persistent Spark History Server UIs, fetches application, job, stage, and SQL query data, and then processes this information into Spark DataFrames for performance analysis and optimization insights.\n",
    "\n",
    "## Required IAM Permissions\n",
    "#### EMR\n",
    "  - elasticmapreduce:ListClusters\n",
    "  - elasticmapreduce:DescribeCluster\n",
    "  - elasticmapreduce:ListSteps\n",
    "  - elasticmapreduce:DescribeStep\n",
    "  - elasticmapreduce:CreatePersistentAppUI\n",
    "  - elasticmapreduce:GetPersistentAppUIPresignedURL\n",
    "  - elasticmapreduce:ListInstanceGroups\n",
    "  - elasticmapreduce:ListInstanceFleets\n",
    "#### S3\n",
    "  - s3:PutObject\n",
    "  - s3:GetObject\n",
    "  - s3:ListBucket\n",
    "#### STS\n",
    "  - sts:GetCallerIdentity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7811f701-661f-4a92-bfbe-fb417ea1d699",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Config"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n Environment: dev\n AWS Region: us-east-1\n EMR Cluster ARN: Auto-discover clusters\n Timeout: 300 seconds\n Max Applications per Cluster: 10\n Cluster States to Analyze: ['STARTING', 'BOOTSTRAPPING', 'RUNNING', 'WAITING', 'TERMINATING', 'TERMINATED', 'TERMINATED_WITH_ERRORS']\n Cluster Name Filter: None\n Max Clusters to Analyze: 10\n Created After Date: None\n Created Before Date: None\n S3 Output Path: s3://ttd-vertica-backups/env=test/vertica-ext/databricks-stage/sparkmetrics\n Persistent UI Timeout: 180 seconds\n Max Cluster Threads: 50\n Max App Threads: 50\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Configuration Parameters\n",
    "# ----------------------------------------------------------------------\n",
    "# Parse and validate configuration\n",
    "dbutils.widgets.text(\"aws_region\", \"us-east-1\", \"AWS Region\")\n",
    "dbutils.widgets.text(\"emr_cluster_arn\", \"\", \"EMR Cluster ARN (optional - leave blank to discover clusters)\")\n",
    "dbutils.widgets.text(\"timeout_seconds\", \"300\", \"Request Timeout (seconds)\")\n",
    "dbutils.widgets.text(\"max_applications\", \"50\", \"Max Applications to Analyze per Cluster\")\n",
    "dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"prod\"], \"Environment (dev/prod)\")\n",
    "dbutils.widgets.text(\"s3_output_path\", \"\", \"S3 Output Path (prod only)\")\n",
    "\n",
    "# Updated: cluster_states dropdown with new options\n",
    "dbutils.widgets.dropdown(\"cluster_states\", \"TERMINATED,WAITING\", [\"TERMINATED\", \"WAITING\", \"TERMINATED,WAITING\", \"ALL\"], \"EMR Cluster States to Analyze\")\n",
    "\n",
    "dbutils.widgets.text(\"cluster_name_filter\", \"\", \"Cluster Name Filter (optional - partial name match)\")\n",
    "dbutils.widgets.text(\"max_clusters\", \"5\", \"Max Clusters to Analyze\")\n",
    "\n",
    "# New: Widgets for CreatedAfter and CreatedBefore dates\n",
    "# Format: YYYY-MM-DD. Leave blank for no date filter.\n",
    "dbutils.widgets.text(\"created_after_date\", \"\", \"EMR Clusters Created After (YYYY-MM-DD)\")\n",
    "dbutils.widgets.text(\"created_before_date\", \"\", \"EMR Clusters Created Before (YYYY-MM-DD)\")\n",
    "\n",
    "# New: Parameters for timeout and concurrency\n",
    "dbutils.widgets.text(\"persistent_ui_timeout_seconds\", \"180\", \"Persistent App UI Timeout (seconds)\")\n",
    "dbutils.widgets.text(\"max_cluster_threads\", \"50\", \"Max Concurrent Cluster Analysis Threads\")\n",
    "dbutils.widgets.text(\"max_app_threads\", \"100\", \"Max Concurrent App Analysis Threads per Cluster\")\n",
    "\n",
    "\n",
    "AWS_REGION = dbutils.widgets.get(\"aws_region\").strip() or \"us-east-1\"\n",
    "EMR_CLUSTER_ARN = dbutils.widgets.get(\"emr_cluster_arn\").strip()\n",
    "TIMEOUT_SECONDS = int(dbutils.widgets.get(\"timeout_seconds\") or \"300\")\n",
    "MAX_APPLICATIONS = int(dbutils.widgets.get(\"max_applications\") or \"50\")\n",
    "CLUSTER_STATES = dbutils.widgets.get(\"cluster_states\").strip()\n",
    "CLUSTER_NAME_FILTER = dbutils.widgets.get(\"cluster_name_filter\").strip()\n",
    "MAX_CLUSTERS = int(dbutils.widgets.get(\"max_clusters\") or \"5\")\n",
    "ENVIRONMENT = dbutils.widgets.get(\"environment\").strip()\n",
    "S3_OUTPUT_PATH = dbutils.widgets.get(\"s3_output_path\").strip()\n",
    "PERSISTENT_UI_TIMEOUT_SECONDS = int(dbutils.widgets.get(\"persistent_ui_timeout_seconds\") or \"180\")\n",
    "MAX_CLUSTER_THREADS = int(dbutils.widgets.get(\"max_cluster_threads\") or \"50\")\n",
    "MAX_APP_THREADS = int(dbutils.widgets.get(\"max_app_threads\") or \"100\")\n",
    "\n",
    "\n",
    "# Date parameters\n",
    "if ENVIRONMENT == \"dev\":\n",
    "    CREATED_AFTER_DATE_STR = dbutils.widgets.get(\"created_after_date\").strip()\n",
    "    CREATED_BEFORE_DATE_STR = dbutils.widgets.get(\"created_before_date\").strip()\n",
    "    PARSED_CREATED_AFTER_DATE = None\n",
    "    if CREATED_AFTER_DATE_STR:\n",
    "        try:\n",
    "            PARSED_CREATED_AFTER_DATE = datetime.datetime.strptime(CREATED_AFTER_DATE_STR, \"%Y-%m-%d\")\n",
    "        except ValueError as e:\n",
    "            logger.error(\"❌ Invalid format for created_after_date: %s. Expected YYYY-MM-DD.\", CREATED_AFTER_DATE_STR, exc_info=True)\n",
    "            raise ValueError(f\"Invalid format for created_after_date: {CREATED_AFTER_DATE_STR}. Expected YYYY-MM-DD.\") from e\n",
    "    PARSED_CREATED_BEFORE_DATE = None\n",
    "    if CREATED_BEFORE_DATE_STR:\n",
    "        try:\n",
    "            PARSED_CREATED_BEFORE_DATE = datetime.datetime.strptime(CREATED_BEFORE_DATE_STR, \"%Y-%m-%d\") + timedelta(days=1, seconds=-1)\n",
    "        except ValueError as e:\n",
    "            logger.error(\"❌ Invalid format for created_before_date: %s. Expected YYYY-MM-DD.\", CREATED_BEFORE_DATE_STR, exc_info=True)\n",
    "            raise ValueError(f\"Invalid format for created_before_date: {CREATED_BEFORE_DATE_STR}. Expected YYYY-MM-DD.\") from e\n",
    "    # Validate date range\n",
    "    if PARSED_CREATED_AFTER_DATE and PARSED_CREATED_BEFORE_DATE and PARSED_CREATED_AFTER_DATE >= PARSED_CREATED_BEFORE_DATE:\n",
    "        logger.error(\"❌ created_after_date (%s) cannot be on or after created_before_date (%s).\", CREATED_AFTER_DATE_STR, CREATED_BEFORE_DATE_STR, exc_info=True)\n",
    "        raise ValueError(\"created_after_date cannot be on or after created_before_date.\")\n",
    "else:\n",
    "    # In prod, analyze only the last 24 hours\n",
    "    PARSED_CREATED_BEFORE_DATE = datetime.datetime.now()\n",
    "    PARSED_CREATED_AFTER_DATE = PARSED_CREATED_BEFORE_DATE - timedelta(days=1)\n",
    "    CREATED_AFTER_DATE_STR = PARSED_CREATED_AFTER_DATE.strftime(\"%Y-%m-%d\")\n",
    "    CREATED_BEFORE_DATE_STR = PARSED_CREATED_BEFORE_DATE.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# Updated: Parse cluster states based on the new dropdown options\n",
    "if CLUSTER_STATES == \"ALL\":\n",
    "    # Comprehensive list of all EMR cluster states for 'ALL' option\n",
    "    CLUSTER_STATES_LIST = [\n",
    "        'STARTING', 'BOOTSTRAPPING', 'RUNNING', 'WAITING',\n",
    "        'TERMINATING', 'TERMINATED', 'TERMINATED_WITH_ERRORS'\n",
    "    ]\n",
    "elif CLUSTER_STATES == \"TERMINATED\":\n",
    "    CLUSTER_STATES_LIST = ['TERMINATED']\n",
    "elif CLUSTER_STATES == \"WAITING\":\n",
    "    CLUSTER_STATES_LIST = ['WAITING']\n",
    "elif CLUSTER_STATES == \"TERMINATED,WAITING\":\n",
    "    CLUSTER_STATES_LIST = ['TERMINATED', 'WAITING']\n",
    "else:\n",
    "    # Default to TERMINATED and WAITING for any unexpected value\n",
    "    CLUSTER_STATES_LIST = ['TERMINATED', 'WAITING']\n",
    "    logger.warning(\"Invalid cluster_states value '%s'. Defaulting to ['TERMINATED', 'WAITING'].\", CLUSTER_STATES)\n",
    "\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\" Environment: {ENVIRONMENT}\")\n",
    "print(f\" AWS Region: {AWS_REGION}\")\n",
    "print(f\" EMR Cluster ARN: {EMR_CLUSTER_ARN or 'Auto-discover clusters'}\")\n",
    "print(f\" Timeout: {TIMEOUT_SECONDS} seconds\")\n",
    "print(f\" Max Applications per Cluster: {MAX_APPLICATIONS}\")\n",
    "print(f\" Cluster States to Analyze: {CLUSTER_STATES_LIST}\")\n",
    "print(f\" Cluster Name Filter: {CLUSTER_NAME_FILTER or 'None'}\")\n",
    "print(f\" Max Clusters to Analyze: {MAX_CLUSTERS}\")\n",
    "print(f\" Created After Date: {CREATED_AFTER_DATE_STR or 'None'}\")\n",
    "print(f\" Created Before Date: {CREATED_BEFORE_DATE_STR or 'None'}\")\n",
    "print(f\" S3 Output Path: {S3_OUTPUT_PATH or 'None (dev mode or not specified)'}\")\n",
    "print(f\" Persistent UI Timeout: {PERSISTENT_UI_TIMEOUT_SECONDS} seconds\")\n",
    "print(f\" Max Cluster Threads: {MAX_CLUSTER_THREADS}\")\n",
    "print(f\" Max App Threads: {MAX_APP_THREADS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40a36f5-72d3-41c5-aeb9-1149bdba19f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### AWS Boto3 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1ba9040d-56ae-4fcf-99ed-640ee71fe760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EMR Persistent App UI Client\n",
    "This module provides functionality to create an EMR Persistent App UI, retrieve its details and presigned URL,\n",
    "and establish an HTTP session with proper cookie management for Spark History Server access.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ServerConfig:\n",
    "    \"\"\"Configuration class for EMR Persistent UI client.\"\"\"\n",
    "\n",
    "    def __init__(self, emr_cluster_arn: str, timeout: int = 300):\n",
    "        \"\"\"\n",
    "        Initialize ServerConfig.\n",
    "\n",
    "        :param emr_cluster_arn: The EMR cluster ARN\n",
    "        :param timeout: Request timeout in seconds\n",
    "        :raises ValueError: If emr_cluster_arn is invalid\n",
    "        \"\"\"\n",
    "        if not emr_cluster_arn or not emr_cluster_arn.startswith(\"arn:aws:elasticmapreduce:\"):\n",
    "            raise ValueError(\"Invalid EMR cluster ARN format\")\n",
    "        self.emr_cluster_arn = emr_cluster_arn\n",
    "        self.timeout = timeout\n",
    "\n",
    "\n",
    "class EMRPersistentUIClient:\n",
    "    \"\"\"Client for managing EMR Persistent App UI and HTTP sessions.\"\"\"\n",
    "\n",
    "    def __init__(self, server_config: ServerConfig):\n",
    "        \"\"\"\n",
    "        Initialize the EMR client.\n",
    "\n",
    "        :param server_config: ServerConfig object\n",
    "        \"\"\"\n",
    "        self.emr_cluster_arn = server_config.emr_cluster_arn\n",
    "        self.region = self.emr_cluster_arn.split(\":\")[3] # Extract region from ARN\n",
    "        # Initialize boto3 client with credentials\n",
    "        self.emr_client = boto3.client(\n",
    "            \"emr\",\n",
    "            region_name=self.region\n",
    "        )\n",
    "        self.session = requests.Session()\n",
    "        self.persistent_ui_id: Optional[str] = None\n",
    "        self.presigned_url: Optional[str] = None\n",
    "        self.base_url: Optional[str] = None\n",
    "        self.timeout: int = server_config.timeout\n",
    "        self.presigned_url_ready: bool = False\n",
    "\n",
    "    def create_persistent_app_ui(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Create a persistent app UI for the given cluster.\n",
    "\n",
    "        :returns: Response from create-persistent-app-ui API call\n",
    "        :raises ClientError: If the API call fails\n",
    "        \"\"\"\n",
    "        logger.info(\"Creating persistent app UI for cluster: %s\", self.emr_cluster_arn)\n",
    "        try:\n",
    "            response = self.emr_client.create_persistent_app_ui(\n",
    "                TargetResourceArn=self.emr_cluster_arn\n",
    "            )\n",
    "            self.persistent_ui_id = response.get(\"PersistentAppUIId\")\n",
    "            runtime_role_enabled = response.get(\"RuntimeRoleEnabledCluster\", False)\n",
    "            logger.info(\"✅ Persistent App UI created successfully\")\n",
    "            logger.info(\" Persistent UI ID: %s\", self.persistent_ui_id)\n",
    "            logger.info(\" Runtime Role Enabled: %s\", runtime_role_enabled)\n",
    "            return response\n",
    "        except ClientError as e:\n",
    "            error_code = e.response[\"Error\"][\"Code\"]\n",
    "            error_message = e.response[\"Error\"][\"Message\"]\n",
    "            logger.error(\n",
    "                \"❌ Failed to create persistent app UI: %s - %s\", error_code, error_message, exc_info=True\n",
    "            )\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error creating persistent app UI: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def get_presigned_url(self, ui_type: str = \"SHS\") -> str:\n",
    "        \"\"\"\n",
    "        Get presigned URL for the persistent app UI.\n",
    "\n",
    "        :param ui_type: Type of UI ('SHS' for Spark History Server)\n",
    "        :returns: Presigned URL string\n",
    "        :raises ValueError: If no persistent UI ID is available\n",
    "        :raises ClientError: If the API call fails\n",
    "        \"\"\"\n",
    "        if not self.persistent_ui_id:\n",
    "            raise ValueError(\"No persistent UI ID available. Create one first.\")\n",
    "        logger.info(\n",
    "            \"Getting presigned URL for persistent app UI: %s (type: %s)\", self.persistent_ui_id, ui_type\n",
    "        )\n",
    "        try:\n",
    "            response = self.emr_client.get_persistent_app_ui_presigned_url(\n",
    "                PersistentAppUIId=self.persistent_ui_id,\n",
    "                PersistentAppUIType=ui_type\n",
    "            )\n",
    "            self.presigned_url_ready = response.get(\"PresignedURLReady\")\n",
    "            self.presigned_url = response.get(\"PresignedURL\")\n",
    "            # Extract base URL from presigned URL\n",
    "            parsed_url = urlparse(self.presigned_url)\n",
    "            self.base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/shs\"\n",
    "            logger.info(\"✅ Presigned URL obtained successfully\")\n",
    "            logger.info(\" Base URL: %s\", self.base_url)\n",
    "            return self.presigned_url\n",
    "        except ClientError as e:\n",
    "            error_code = e.response[\"Error\"][\"Code\"]\n",
    "            error_message = e.response[\"Error\"][\"Message\"]\n",
    "            logger.error(\n",
    "                \"❌ Failed to get presigned URL: %s - %s\", error_code, error_message, exc_info=True\n",
    "            )\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error getting presigned URL: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def setup_http_session(self) -> requests.Session:\n",
    "        \"\"\"\n",
    "        Set up HTTP session with proper headers and cookie management.\n",
    "\n",
    "        :returns: Configured requests.Session object\n",
    "        :raises ValueError: If no presigned URL is available\n",
    "        \"\"\"\n",
    "        if not self.presigned_url:\n",
    "            raise ValueError(\"No presigned URL available. Get one first.\")\n",
    "        logger.info(\"Setting up HTTP session with cookie management\")\n",
    "        # Configure session with appropriate headers\n",
    "        self.session.headers.update(\n",
    "            {\n",
    "                \"User-Agent\": \"EMR-Persistent-UI-Client/1.0\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "                \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "                \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Upgrade-Insecure-Requests\": \"1\"\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            # Make initial request to establish session and get cookies\n",
    "            logger.info(\"Making initial request\")\n",
    "            response = self.session.get(\n",
    "                self.presigned_url, timeout=self.timeout, allow_redirects=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            logger.info(\"✅ HTTP session established successfully\")\n",
    "            logger.info(\" Status Code: %s\", response.status_code)\n",
    "            logger.info(\" Cookies: %s cookie(s) stored\", len(self.session.cookies))\n",
    "            # Log cookie details (without sensitive values)\n",
    "            for cookie in self.session.cookies:\n",
    "                logger.debug(\" Cookie: %s (domain: %s)\", cookie.name, cookie.domain)\n",
    "            return self.session\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(\"❌ Failed to establish HTTP session: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error setting up HTTP session: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def initialize(self, max_wait_time: int) -> Tuple[str, requests.Session]:\n",
    "        \"\"\"\n",
    "        Initialize the EMR Persistent UI client by creating a persistent app UI,\n",
    "        polling until it is ready, getting a presigned URL, and setting up an HTTP session.\n",
    "\n",
    "        :param max_wait_time: The maximum time in seconds to wait for the persistent UI to become ready.\n",
    "        :returns: Tuple containing the base URL and configured session\n",
    "        :raises ValueError: If the persistent UI does not become ready within the timeout period\n",
    "        \"\"\"\n",
    "        # Step 1: Create persistent app UI\n",
    "        self.create_persistent_app_ui()\n",
    "\n",
    "        # Step 2: Poll for the presigned URL until it's ready\n",
    "        wait_interval = 10  # Check every 10 seconds\n",
    "        total_waited = 0\n",
    "        url_is_ready = False\n",
    "\n",
    "        logger.info(\"Waiting for Persistent App UI to become ready...\")\n",
    "\n",
    "        while total_waited < max_wait_time:\n",
    "            try:\n",
    "                # Directly call the boto3 client to check the ready status without failing\n",
    "                response = self.emr_client.get_persistent_app_ui_presigned_url(\n",
    "                    PersistentAppUIId=self.persistent_ui_id, PersistentAppUIType=\"SHS\"\n",
    "                )\n",
    "                url_is_ready = response.get(\"PresignedURLReady\", False)\n",
    "            except ClientError as e:\n",
    "                # This can happen if the UI is not yet fully initialized; we just wait and retry.\n",
    "                logger.warning(\n",
    "                    \"Could not check for presigned URL, will retry. Error: %s\", str(e)\n",
    "                )\n",
    "\n",
    "            if url_is_ready:\n",
    "                logger.info(\"✅ Persistent App UI is ready.\")\n",
    "                break\n",
    "\n",
    "            logger.info(\n",
    "                \"Persistent App UI not ready yet. Waiting %s seconds before retrying...\",\n",
    "                wait_interval\n",
    "            )\n",
    "            time.sleep(wait_interval)\n",
    "            total_waited += wait_interval\n",
    "\n",
    "        # After waiting, check if we succeeded\n",
    "        if not url_is_ready:\n",
    "            raise ValueError(\n",
    "                f\"Persistent App UI did not become ready after waiting {total_waited} seconds.\"\n",
    "            )\n",
    "\n",
    "        # Step 3: Get the actual presigned URL and setup its properties (now that we know it's ready)\n",
    "        self.get_presigned_url()\n",
    "\n",
    "        # Step 4: Setup HTTP session\n",
    "        self.setup_http_session()\n",
    "\n",
    "        return self.base_url, self.session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bde7f2f-4a63-4200-a05c-d5fa4ed277eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### EMR Cluster Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bf723b93-c2a3-40f3-bb86-6e33e675264e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional # Added Optional\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "class EMRClusterDiscovery:\n",
    "    \"\"\"Discovery and management of EMR clusters.\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, region: str):\n",
    "        \"\"\"\n",
    "        Initialize the EMR cluster discovery client.\n",
    "\n",
    "\n",
    "        :param region: AWS region\n",
    "        :raises TypeError: If region is not a non-empty string.\n",
    "        \"\"\"\n",
    "        if not isinstance(region, str) or not region:\n",
    "            raise TypeError(\"AWS region must be a non-empty string.\")\n",
    "        self.region = region\n",
    "        self.emr_client = boto3.client(\"emr\", region_name=region)\n",
    "\n",
    "\n",
    "    def discover_clusters(self,\n",
    "                          states: Optional[List[str]] = None,\n",
    "                          name_filter: Optional[str] = None,\n",
    "                          max_clusters: int = 10,\n",
    "                          created_after: Optional[datetime] = None,   # Modified: New parameter\n",
    "                          created_before: Optional[datetime] = None) -> List[Dict]: # Modified: New parameter\n",
    "        \"\"\"\n",
    "        Discover EMR clusters based on criteria.\n",
    "\n",
    "\n",
    "        :param states: List of cluster states to filter by. Defaults to ['TERMINATED', 'WAITING'].\n",
    "                       Refer to boto3 EMR documentation for valid states.\n",
    "        :param name_filter: Partial name to filter clusters. Optional.\n",
    "        :param max_clusters: Maximum number of clusters to return. Defaults to 10.\n",
    "        :param created_after: datetime object. Only return clusters created after this time. Optional.\n",
    "        :param created_before: datetime object. Only return clusters created before this time. Optional.\n",
    "        :returns: List of cluster summaries, each a dictionary containing cluster metadata.\n",
    "        :raises TypeError: If inputs are not of the correct type.\n",
    "        :raises ValueError: If input values are out of acceptable ranges (e.g., max_clusters < 1).\n",
    "        :raises ClientError: If AWS API call fails.\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        if not isinstance(states, (list, type(None))):\n",
    "            raise TypeError(\":param states: must be a list of strings or None.\")\n",
    "        if states:\n",
    "            for state in states:\n",
    "                if not isinstance(state, str):\n",
    "                    raise TypeError(\"Each state in the :param states: list must be a string.\")\n",
    "        if not isinstance(name_filter, (str, type(None))):\n",
    "            raise TypeError(\":param name_filter: must be a string or None.\")\n",
    "        if not isinstance(max_clusters, int):\n",
    "            raise TypeError(\":param max_clusters: must be an integer.\")\n",
    "        if max_clusters < 1:\n",
    "            raise ValueError(\":param max_clusters: must be at least 1.\")\n",
    "        # Corrected: Input validation for datetime objects\n",
    "        if created_after is not None and not isinstance(created_after, datetime):\n",
    "            raise TypeError(\":param created_after: must be a datetime object or None.\")\n",
    "        if created_before is not None and not isinstance(created_before, datetime):\n",
    "            raise TypeError(\":param created_before: must be a datetime object or None.\")\n",
    "        if created_after and created_before and created_after >= created_before:\n",
    "            raise ValueError(\":param created_after: date cannot be on or after :param created_before: date.\")\n",
    "\n",
    "\n",
    "\n",
    "        logger.info(\"\uD83D\uDD0D Discovering EMR clusters in region: %s\", self.region)\n",
    "        logger.info(\" States: %s\", states)\n",
    "        logger.info(\" Name filter: %s\", name_filter or \"None\")\n",
    "        logger.info(\" Max clusters: %s\", max_clusters)\n",
    "        logger.info(\" Created After: %s\", created_after or \"None\")\n",
    "        logger.info(\" Created Before: %s\", created_before or \"None\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Build parameters for list_clusters\n",
    "            list_clusters_params = {}\n",
    "            if states:\n",
    "                list_clusters_params['ClusterStates'] = states\n",
    "            else:\n",
    "                list_clusters_params['ClusterStates'] = ['TERMINATED', 'WAITING'] # Default as per new requirement\n",
    "            if created_after:\n",
    "                list_clusters_params['CreatedAfter'] = created_after\n",
    "            if created_before:\n",
    "                list_clusters_params['CreatedBefore'] = created_before\n",
    "\n",
    "\n",
    "            # Use paginator to handle large cluster lists\n",
    "            paginator = self.emr_client.get_paginator('list_clusters')\n",
    "            page_iterator = paginator.paginate(**list_clusters_params)\n",
    "\n",
    "\n",
    "\n",
    "            discovered_clusters = []\n",
    "            for page in page_iterator:\n",
    "                for cluster in page.get('Clusters', []):\n",
    "                    # Apply name filter if specified\n",
    "                    if name_filter and name_filter.lower() not in cluster.get('Name', '').lower():\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    cluster_info = {\n",
    "                        'cluster_id': cluster.get('Id'),\n",
    "                        'cluster_name': cluster.get('Name'),\n",
    "                        'cluster_arn': cluster.get('ClusterArn'),\n",
    "                        'status': cluster.get('Status', {}).get('State'),\n",
    "                        'creation_time': cluster.get('Status', {}).get('Timeline', {}).get('CreationDateTime'),\n",
    "                        'normalized_instance_hours': cluster.get('NormalizedInstanceHours', 0)\n",
    "                    }\n",
    "                    discovered_clusters.append(cluster_info)\n",
    "\n",
    "\n",
    "                    # Stop if we've reached max clusters\n",
    "                    if len(discovered_clusters) >= max_clusters:\n",
    "                        break\n",
    "                if len(discovered_clusters) >= max_clusters:\n",
    "                    break\n",
    "\n",
    "\n",
    "            logger.info(\"✅ Discovered %s clusters\", len(discovered_clusters))\n",
    "            # Log cluster details\n",
    "            for i, cluster in enumerate(discovered_clusters, 1):\n",
    "                logger.info(\" %d. %s (%s) - %s\", i, cluster['cluster_name'], cluster['cluster_id'], cluster['status'])\n",
    "            return discovered_clusters\n",
    "\n",
    "\n",
    "        except ClientError as e:\n",
    "            logger.error(\"❌ Failed to discover clusters: %s - %s\", e.response[\"Error\"][\"Code\"], e.response[\"Error\"][\"Message\"], exc_info=True)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error during cluster discovery: %s\", str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "\n",
    "    def get_cluster_details(self, cluster_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get detailed information about a specific cluster.\n",
    "\n",
    "\n",
    "        :param cluster_id: EMR cluster ID.\n",
    "        :returns: Detailed cluster information dictionary.\n",
    "        :raises TypeError: If cluster_id is not a string.\n",
    "        :raises ClientError: If AWS API call fails.\n",
    "        \"\"\"\n",
    "        if not isinstance(cluster_id, str) or not cluster_id:\n",
    "            raise TypeError(\"Cluster ID must be a non-empty string.\")\n",
    "        try:\n",
    "            response = self.emr_client.describe_cluster(ClusterId=cluster_id)\n",
    "            cluster = response.get('Cluster', {})\n",
    "\n",
    "\n",
    "            cluster_details = {\n",
    "                'cluster_id': cluster.get('Id'),\n",
    "                'cluster_name': cluster.get('Name'),\n",
    "                'cluster_arn': cluster.get('ClusterArn'),\n",
    "                'status': cluster.get('Status', {}).get('State'),\n",
    "                'emr_release_label': cluster.get('ReleaseLabel'),\n",
    "                'applications': [app.get('Name') for app in cluster.get('Applications', [])],\n",
    "                'instance_count': 0,\n",
    "                'master_instance_type': None,\n",
    "                'core_instance_type': None,\n",
    "                'log_uri': cluster.get('LogUri'),\n",
    "                'ec2_attributes': cluster.get('Ec2InstanceAttributes', {}),\n",
    "                'creation_time': cluster.get('Status', {}).get('Timeline', {}).get('CreationDateTime'),\n",
    "                'ready_time': cluster.get('Status', {}).get('Timeline', {}).get('ReadyDateTime'),\n",
    "                'normalized_instance_hours': cluster.get('NormalizedInstanceHours', 0)\n",
    "            }\n",
    "\n",
    "\n",
    "            # Attempt to get instance group information first\n",
    "            try:\n",
    "                instance_groups = self.emr_client.list_instance_groups(ClusterId=cluster_id)\n",
    "                for group in instance_groups.get('InstanceGroups', []):\n",
    "                    instance_type = group.get('InstanceType')\n",
    "                    running_count = group.get('RunningInstanceCount', 0)\n",
    "                    cluster_details['instance_count'] += running_count\n",
    "                    if group.get('InstanceGroupType') == 'MASTER':\n",
    "                        cluster_details['master_instance_type'] = instance_type\n",
    "                    elif group.get('InstanceGroupType') == 'CORE':\n",
    "                        cluster_details['core_instance_type'] = instance_type\n",
    "                logger.info(\"Successfully retrieved instance group info for %s\", cluster_id)\n",
    "            except ClientError as ce:\n",
    "                # Check if the error is specifically due to instance fleets being used\n",
    "                if (ce.response[\"Error\"][\"Code\"] == \"InvalidRequestException\" and\n",
    "                        \"Instance fleets and instance groups are mutually exclusive\" in ce.response[\"Error\"][\"Message\"]):\n",
    "                    logger.info(\"Cluster %s uses instance fleets. Fetching instance fleet details.\", cluster_id)\n",
    "                    # If it uses instance fleets, try to get instance fleet information\n",
    "                    try:\n",
    "                        instance_fleets = self.emr_client.list_instance_fleets(ClusterId=cluster_id)\n",
    "                        for fleet in instance_fleets.get('InstanceFleets', []):\n",
    "                            # Sum up the target capacities for total instances\n",
    "                            target_on_demand = fleet.get('TargetOnDemandCapacity', 0)\n",
    "                            target_spot = fleet.get('TargetSpotCapacity', 0)\n",
    "                            cluster_details['instance_count'] += target_on_demand + target_spot\n",
    "\n",
    "\n",
    "                            # Extract master/core instance types from fleet details\n",
    "                            # InstanceTypeSpecifications is a LIST, not a dict\n",
    "                            instance_type_specs = fleet.get('InstanceTypeSpecifications', [])\n",
    "                            if fleet.get('InstanceFleetType') == 'MASTER' and instance_type_specs:\n",
    "                                cluster_details['master_instance_type'] = instance_type_specs[0].get('InstanceType')\n",
    "                            elif fleet.get('InstanceFleetType') == 'CORE' and instance_type_specs:\n",
    "                                cluster_details['core_instance_type'] = instance_type_specs[0].get('InstanceType')\n",
    "                        logger.info(\"Successfully retrieved instance fleet info for %s\", cluster_id)\n",
    "                    except ClientError as fleet_ce:\n",
    "                        logger.warning(\"Could not get instance fleet details for %s: %s - %s\",\n",
    "                                       cluster_id, fleet_ce.response[\"Error\"][\"Code\"],\n",
    "                                       fleet_ce.response[\"Error\"][\"Message\"], exc_info=True)\n",
    "                    except Exception as fleet_e:\n",
    "                        logger.warning(\"Unexpected error getting instance fleet details for %s: %s\",\n",
    "                                       cluster_id, str(fleet_e), exc_info=True)\n",
    "                else:\n",
    "                    # Re-raise any other ClientErrors that are not the mutual exclusivity error\n",
    "                    logger.warning(\"Could not get instance group details for %s: %s - %s\",\n",
    "                                   cluster_id, ce.response[\"Error\"][\"Code\"],\n",
    "                                   ce.response[\"Error\"][\"Message\"], exc_info=True)\n",
    "                    raise # Re-raise original ClientError\n",
    "            except Exception as e:\n",
    "                # Catch any other general exceptions during initial instance group/fleet fetching\n",
    "                logger.warning(\"Unexpected error during instance details fetching for %s: %s\",\n",
    "                               cluster_id, str(e), exc_info=True)\n",
    "\n",
    "\n",
    "            return cluster_details\n",
    "\n",
    "\n",
    "        except ClientError as e:\n",
    "            logger.error(\"❌ Failed to describe cluster %s: %s - %s\",\n",
    "                         cluster_id, e.response[\"Error\"][\"Code\"],\n",
    "                         e.response[\"Error\"][\"Message\"], exc_info=True)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Unexpected error getting cluster details for %s: %s\",\n",
    "                         cluster_id, str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "\n",
    "    def validate_cluster_for_analysis(self, cluster_info: Dict) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if a cluster is suitable for Spark History Server analysis.\n",
    "        A cluster is suitable if it has the Spark application installed and is in a state\n",
    "        where history data might be available.\n",
    "\n",
    "\n",
    "        :param cluster_info: Cluster information dictionary.\n",
    "        :returns: True if cluster is suitable for analysis, False otherwise.\n",
    "        :raises TypeError: If cluster_info is not a dictionary or has incorrect types.\n",
    "        \"\"\"\n",
    "        if not isinstance(cluster_info, dict):\n",
    "            raise TypeError(\":param cluster_info: must be a dictionary.\")\n",
    "\n",
    "\n",
    "        cluster_id = cluster_info.get('cluster_id')\n",
    "        status = cluster_info.get('status')\n",
    "        applications = cluster_info.get('applications', [])\n",
    "\n",
    "\n",
    "        if not isinstance(cluster_id, str):\n",
    "            raise TypeError(\"Cluster ID in cluster_info must be a string.\")\n",
    "        if not isinstance(status, str):\n",
    "            raise TypeError(\"Status in cluster_info must be a string.\")\n",
    "        if not isinstance(applications, list):\n",
    "            raise TypeError(\"Applications in cluster_info must be a list.\")\n",
    "        for app in applications:\n",
    "            if not isinstance(app, str):\n",
    "                raise TypeError(\"Each application name in cluster_info must be a string.\")\n",
    "\n",
    "\n",
    "        # Check if cluster has Spark application\n",
    "        has_spark = any('Spark' in app for app in applications)\n",
    "        if not has_spark:\n",
    "            logger.warning(\"⚠️ Cluster %s does not have Spark installed - skipping\", cluster_id)\n",
    "            return False\n",
    "\n",
    "\n",
    "        # Check if cluster is in a valid state for analysis.\n",
    "        # Even 'TERMINATED' clusters can have history data.\n",
    "        valid_states = ['RUNNING', 'WAITING', 'TERMINATED']\n",
    "        if status not in valid_states:\n",
    "            logger.warning(\"⚠️ Cluster %s is in state '%s' - may not have history data or is not a target state for analysis\", cluster_id, status)\n",
    "            return False # Exclude clusters not in RUNNING, WAITING, or TERMINATED\n",
    "\n",
    "\n",
    "        logger.info(\"✅ Cluster %s is valid for analysis (Spark: %s, State: %s)\", cluster_id, has_spark, status)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e8de55-97c8-47b7-aaba-b715e1e3a1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark History Server REST Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "85c9cf87-29db-4926-8873-0d8f70727cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Any, Dict, Optional\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SparkHistoryServerClient:\n",
    "    \"\"\"Client for interacting with Spark History Server REST API.\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str, session: requests.Session) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Spark History Server client.\n",
    "\n",
    "        :param base_url: Base URL for the Spark History Server\n",
    "        :param session: Configured HTTP session with authentication\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.session = session\n",
    "        self.api_base = f\"{base_url}/api/v1\"\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: Optional[Dict] = None) -> Any:\n",
    "        \"\"\"\n",
    "        Make a REST API request to the Spark History Server.\n",
    "\n",
    "        :param endpoint: API endpoint (relative to /api/v1)\n",
    "        :param params: Query parameters\n",
    "        :returns: JSON response\n",
    "        :raises requests.exceptions.RequestException: If request fails\n",
    "        :raises json.JSONDecodeError: If JSON parsing fails\n",
    "        \"\"\"\n",
    "        url = f\"{self.api_base}/{endpoint}\"\n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(\"❌ Failed to make request to %s: %s\", url, str(e), exc_info=True)\n",
    "            raise\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(\"❌ Failed to parse JSON response from %s: %s\", url, str(e), exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def get_applications(self, status: Optional[str] = None, limit: int = 100) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get list of Spark applications.\n",
    "\n",
    "        :param status: Filter by status (running, completed, failed)\n",
    "        :param limit: Maximum number of applications to return\n",
    "        :returns: List of application metadata\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching applications (status: %s, limit: %s)\", status, limit)\n",
    "        params = {}\n",
    "        if status:\n",
    "            params['status'] = status\n",
    "        if limit:\n",
    "            params['limit'] = limit\n",
    "        applications = self._make_request(\"applications\", params)\n",
    "        logger.info(\"✅ Retrieved %s applications\", len(applications))\n",
    "        return applications\n",
    "\n",
    "    def get_application_details(self, app_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get detailed information about a specific application.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :returns: Application details\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching application details for: %s\", app_id)\n",
    "        details = self._make_request(f\"applications/{app_id}\")\n",
    "        logger.info(\"✅ Retrieved details for application: %s\", app_id)\n",
    "        return details\n",
    "\n",
    "    def get_application_jobs(self, app_id: str, attempt_id: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get jobs for a specific application.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param attempt_id: The application attempt ID. If None, uses the base application endpoint.\n",
    "        :returns: List of jobs\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching jobs for application: %s (Attempt ID: %s)\", app_id, attempt_id or \"N/A\")\n",
    "        endpoint = f\"applications/{app_id}\"\n",
    "        if attempt_id:\n",
    "            endpoint += f\"/{attempt_id}\"\n",
    "        jobs = self._make_request(f\"{endpoint}/jobs\")\n",
    "        logger.info(\"✅ Retrieved %s jobs for application: %s\", len(jobs), app_id)\n",
    "        return jobs\n",
    "\n",
    "    def get_application_stages(self, app_id: str, attempt_id: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get stages for a specific application.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param attempt_id: The application attempt ID. If None, uses the base application endpoint.\n",
    "        :returns: List of stages\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching stages for application: %s (Attempt ID: %s)\", app_id, attempt_id or \"N/A\")\n",
    "        endpoint = f\"applications/{app_id}\"\n",
    "        if attempt_id:\n",
    "            endpoint += f\"/{attempt_id}\"\n",
    "        stages = self._make_request(f\"{endpoint}/stages\")\n",
    "        logger.info(\"✅ Retrieved %s stages for application: %s\", len(stages), app_id)\n",
    "        return stages\n",
    "\n",
    "    def get_application_executors(self, app_id: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get executors for a specific application.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :returns: List of executors\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching executors for application: %s\", app_id)\n",
    "        executors = self._make_request(f\"applications/{app_id}/executors\")\n",
    "        logger.info(\"✅ Retrieved %s executors for application: %s\", len(executors), app_id)\n",
    "        return executors\n",
    "\n",
    "    def get_stage_tasks(self, app_id: str, stage_id: int, stage_attempt: int = 0) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get tasks for a specific stage.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param stage_id: Stage ID\n",
    "        :param stage_attempt: Stage attempt number\n",
    "        :returns: List of tasks\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching tasks for stage %s (attempt %s) in application: %s\", stage_id, stage_attempt, app_id)\n",
    "        tasks = self._make_request(f\"applications/{app_id}/stages/{stage_id}/{stage_attempt}/taskList\")\n",
    "        logger.info(\"✅ Retrieved %s tasks for stage %s\", len(tasks), stage_id)\n",
    "        return tasks\n",
    "\n",
    "    def get_application_sql_queries(self, app_id: str, min_duration_minutes: int = 0, attempt_id: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get SQL queries for a specific application and filter by minimum duration.\n",
    "\n",
    "        :param app_id: Application ID\n",
    "        :param min_duration_minutes: Minimum duration in minutes for SQL queries to include\n",
    "        :param attempt_id: The application attempt ID. If None, uses the base application endpoint.\n",
    "        :returns: List of SQL query data (raw JSON)\n",
    "        \"\"\"\n",
    "        logger.info(\"Fetching SQL queries for application: %s (min_duration_minutes: %s, Attempt ID: %s)\", app_id, min_duration_minutes, attempt_id or \"N/A\")\n",
    "        endpoint = f\"applications/{app_id}\"\n",
    "        if attempt_id:\n",
    "            endpoint += f\"/{attempt_id}\"\n",
    "        sql_queries = self._make_request(f\"{endpoint}/sql\")\n",
    "        logger.info(\"✅ Retrieved %s raw SQL queries for application: %s\", len(sql_queries), app_id)\n",
    "\n",
    "        # Filter by duration and include the full JSON\n",
    "        filtered_queries = []\n",
    "        for query in sql_queries:\n",
    "            duration_ms = query.get(\"duration\", 0)\n",
    "            if duration_ms >= (min_duration_minutes * 60 * 1000):\n",
    "                filtered_queries.append(query)\n",
    "        \n",
    "        logger.info(\"✅ Filtered to %s SQL queries with duration >= %s minutes for application: %s\", len(filtered_queries), min_duration_minutes, app_id)\n",
    "        return filtered_queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f69e3b9-a1fb-4d4f-89b6-a6e7f2838359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Metrics Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7d5750e9-6716-4a47-89a8-d4cc356a9d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, ArrayType, MapType, IntegerType\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SparkMetricsAnalyzer:\n",
    "    \"\"\"Analyzer for Spark application metrics and performance data.\"\"\"\n",
    "\n",
    "    def __init__(self, spark: SparkSession) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the metrics analyzer.\n",
    "        :param spark: Spark session\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "\n",
    "    def analyze_application_performance(self, app_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for a single application.\n",
    "        :param app_data: Application data from Spark History Server\n",
    "        :returns: Performance analysis results\n",
    "        \"\"\"\n",
    "        duration_ms = app_data.get('duration') or 0\n",
    "        \n",
    "        analysis = {\n",
    "            'application_id': app_data.get('id'),\n",
    "            'application_name': app_data.get('name') or 'Unknown',\n",
    "            'duration_ms': duration_ms,\n",
    "            'duration_minutes': 0.0, # Placeholder, will be calculated next\n",
    "            'start_time': app_data.get('startTime'),\n",
    "            'end_time': app_data.get('endTime'),\n",
    "            'spark_version': app_data.get('sparkVersion') or 'Unknown',\n",
    "            'cores': app_data.get('cores') or 0,\n",
    "            'memory_per_executor_mb': app_data.get('memoryPerExecutorMB') or 0,\n",
    "            'max_cores': app_data.get('maxCores') or 0,\n",
    "            'core_hours': 0.0,\n",
    "            'efficiency_score': 0.0,\n",
    "            'cluster_id': '', # Will be populated by caller\n",
    "            'cluster_name': '' # Will be populated by caller\n",
    "        }\n",
    "\n",
    "        # Explicitly use Python's built-in round function to avoid name conflicts\n",
    "        analysis['duration_minutes'] = __builtins__.round(duration_ms / (1000 * 60), 2)\n",
    "\n",
    "        # Calculate efficiency metrics - ensure no None values\n",
    "        cores = analysis['cores']\n",
    "        max_cores = analysis['max_cores']\n",
    "        if duration_ms > 0 and cores > 0:\n",
    "            analysis['core_hours'] = (duration_ms / (1000 * 3600)) * cores\n",
    "            analysis['efficiency_score'] = min(100.0, (cores / max(1, max_cores)) * 100.0)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def analyze_job_performance(self, jobs: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for jobs.\n",
    "        :param jobs: List of job data\n",
    "        :returns: Job performance analysis\n",
    "        \"\"\"\n",
    "        job_analysis = []\n",
    "        for job in jobs:\n",
    "            analysis = {\n",
    "                'job_id': job.get('jobId'),\n",
    "                'job_name': job.get('name') or 'Unknown',\n",
    "                'status': job.get('status') or 'UNKNOWN',\n",
    "                'submission_time': job.get('submissionTime'),\n",
    "                'completion_time': job.get('completionTime'),\n",
    "                'num_tasks': job.get('numTasks') or 0,\n",
    "                'num_active_tasks': job.get('numActiveTasks') or 0,\n",
    "                'num_completed_tasks': job.get('numCompletedTasks') or 0,\n",
    "                'num_failed_tasks': job.get('numFailedTasks') or 0,\n",
    "                'num_stages': len(job.get('stageIds', [])),\n",
    "                'stage_ids': str(job.get('stageIds', [])), # Convert to string for schema consistency\n",
    "                'task_success_rate': 0.0,\n",
    "                'cluster_id': '', # Will be populated by caller\n",
    "                'cluster_name': '', # Will be populated by caller\n",
    "                'application_id': '' # Will be populated by caller\n",
    "            }\n",
    "\n",
    "            # Calculate success rate - ensure no None values\n",
    "            num_tasks = analysis['num_tasks']\n",
    "            num_completed = analysis['num_completed_tasks']\n",
    "            if num_tasks > 0:\n",
    "                analysis['task_success_rate'] = (num_completed / num_tasks) * 100.0\n",
    "            \n",
    "            job_analysis.append(analysis)\n",
    "        return job_analysis\n",
    "\n",
    "    def analyze_stage_performance(self, stages: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for stages.\n",
    "        :param stages: List of stage data\n",
    "        :returns: Stage performance analysis\n",
    "        \"\"\"\n",
    "        stage_analysis = []\n",
    "        for stage in stages:\n",
    "            analysis = {\n",
    "                'stage_id': stage.get('stageId'),\n",
    "                'stage_name': stage.get('name') or 'Unknown',\n",
    "                'status': stage.get('status') or 'UNKNOWN',\n",
    "                'num_tasks': stage.get('numTasks') or 0,\n",
    "                'num_active_tasks': stage.get('numActiveTasks') or 0,\n",
    "                'num_complete_tasks': stage.get('numCompleteTasks') or 0,\n",
    "                'num_failed_tasks': stage.get('numFailedTasks') or 0,\n",
    "                'executor_run_time': stage.get('executorRunTime') or 0,\n",
    "                'executor_cpu_time': stage.get('executorCpuTime') or 0,\n",
    "                'submission_time': stage.get('submissionTime'),\n",
    "                'first_task_launched_time': stage.get('firstTaskLaunchedTime'),\n",
    "                'completion_time': stage.get('completionTime'),\n",
    "                'input_bytes': stage.get('inputBytes') or 0,\n",
    "                'output_bytes': stage.get('outputBytes') or 0,\n",
    "                'shuffle_read_bytes': stage.get('shuffleReadBytes') or 0,\n",
    "                'shuffle_write_bytes': stage.get('shuffleWriteBytes') or 0,\n",
    "                'memory_bytes_spilled': stage.get('memoryBytesSpilled') or 0,\n",
    "                'disk_bytes_spilled': stage.get('diskBytesSpilled') or 0,\n",
    "                'task_completion_rate': 0.0,\n",
    "                'avg_executor_run_time_per_task': 0.0,\n",
    "                'total_data_processed_mb': 0.0,\n",
    "                'shuffle_data_mb': 0.0,\n",
    "                'cluster_id': '', # Will be populated by caller\n",
    "                'cluster_name': '', # Will be populated by caller\n",
    "                'application_id': '' # Will be populated by caller\n",
    "            }\n",
    "\n",
    "            # Calculate efficiency metrics - ensure no None values\n",
    "            num_tasks = analysis['num_tasks']\n",
    "            num_complete = analysis['num_complete_tasks']\n",
    "            executor_run_time = analysis['executor_run_time']\n",
    "            input_bytes = analysis['input_bytes']\n",
    "            output_bytes = analysis['output_bytes']\n",
    "            shuffle_read_bytes = analysis['shuffle_read_bytes']\n",
    "            shuffle_write_bytes = analysis['shuffle_write_bytes']\n",
    "\n",
    "            if num_tasks > 0:\n",
    "                analysis['task_completion_rate'] = (num_complete / num_tasks) * 100.0\n",
    "                analysis['avg_executor_run_time_per_task'] = executor_run_time / num_tasks\n",
    "\n",
    "            # Data processing metrics\n",
    "            analysis['total_data_processed_mb'] = (input_bytes + output_bytes) / (1024 * 1024)\n",
    "            analysis['shuffle_data_mb'] = (shuffle_read_bytes + shuffle_write_bytes) / (1024 * 1024)\n",
    "            \n",
    "            stage_analysis.append(analysis)\n",
    "        return stage_analysis\n",
    "\n",
    "    def analyze_sql_queries(self, sql_queries: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze SQL query metrics.\n",
    "        Extracts key information and stores the full JSON as a string.\n",
    "        The `description` and `nodes` fields from the JSON are preserved.\n",
    "        :param sql_queries: List of raw SQL query data from Spark History Server.\n",
    "        :returns: List of processed SQL query analysis dictionaries.\n",
    "        \"\"\"\n",
    "        sql_analysis = []\n",
    "        for query in sql_queries:\n",
    "            # Safely get description and nodes, defaulting to empty or sensible values\n",
    "            description = query.get(\"description\", \"N/A\")\n",
    "            nodes = query.get(\"nodes\", [])\n",
    "\n",
    "            # Extract metrics from nodes for summary.\n",
    "            node_metrics = {}\n",
    "            if nodes and isinstance(nodes, list) and len(nodes) > 0:\n",
    "                first_node = nodes[0]\n",
    "                if \"metrics\" in first_node and isinstance(first_node[\"metrics\"], list):\n",
    "                    for metric in first_node[\"metrics\"]:\n",
    "                        if \"name\" in metric and \"value\" in metric:\n",
    "                            metric_name = metric[\"name\"].replace(\" \", \"_\").replace(\".\", \"_\").lower()\n",
    "                            node_metrics[metric_name] = str(metric[\"value\"]) # Store as string for flexibility\n",
    "\n",
    "            duration_ms = query.get(\"duration\", 0)\n",
    "            duration_minutes = __builtins__.round(duration_ms / (1000 * 60), 2)\n",
    "\n",
    "            analysis = {\n",
    "                'sql_id': query.get(\"id\"),\n",
    "                'description': description,\n",
    "                'status': query.get(\"status\"),\n",
    "                'duration_ms': duration_ms,\n",
    "                'duration_minutes': duration_minutes,\n",
    "                'submission_time': query.get(\"submissionTime\"),\n",
    "                'success_job_ids': str(query.get(\"successJobIds\", [])),\n",
    "                'failed_job_ids': str(query.get(\"failedJobIds\", [])),\n",
    "                'num_nodes': len(nodes) if nodes else 0,\n",
    "                'sql_raw_json': json.dumps(query), # Store full JSON as a string\n",
    "                'cluster_id': '', # Will be populated by caller\n",
    "                'cluster_name': '', # Will be populated by caller\n",
    "                'application_id': '' # Will be populated by caller\n",
    "            }\n",
    "            # Add extracted node metrics dynamically\n",
    "            analysis.update(node_metrics)\n",
    "            sql_analysis.append(analysis)\n",
    "        \n",
    "        return sql_analysis\n",
    "\n",
    "    def create_performance_dataframes(\n",
    "        self,\n",
    "        applications_analysis: List[Dict],\n",
    "        jobs_analysis: List[Dict],\n",
    "        stages_analysis: List[Dict],\n",
    "        sql_analysis: List[Dict]\n",
    "    ) -> Tuple[Optional[Any], Optional[Any], Optional[Any], Optional[Any]]:\n",
    "        \"\"\"\n",
    "        Create Spark DataFrames from analysis results with explicit schemas.\n",
    "        :param applications_analysis: Application analysis results\n",
    "        :param jobs_analysis: Job analysis results\n",
    "        :param stages_analysis: Stage analysis results\n",
    "        :param sql_analysis: SQL query analysis results\n",
    "        :returns: Tuple of (applications_df, jobs_df, stages_df, sql_df)\n",
    "        \"\"\"\n",
    "        # Define explicit schemas\n",
    "        applications_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True),\n",
    "            StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True),\n",
    "            StructField(\"application_name\", StringType(), True),\n",
    "            StructField(\"duration_ms\", LongType(), True),\n",
    "            StructField(\"duration_minutes\", DoubleType(), True),\n",
    "            StructField(\"start_time\", StringType(), True),\n",
    "            StructField(\"end_time\", StringType(), True),\n",
    "            StructField(\"spark_version\", StringType(), True),\n",
    "            StructField(\"cores\", LongType(), True),\n",
    "            StructField(\"memory_per_executor_mb\", LongType(), True),\n",
    "            StructField(\"max_cores\", LongType(), True),\n",
    "            StructField(\"core_hours\", DoubleType(), True),\n",
    "            StructField(\"efficiency_score\", DoubleType(), True)\n",
    "        ])\n",
    "        jobs_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True),\n",
    "            StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True),\n",
    "            StructField(\"job_id\", LongType(), True),\n",
    "            StructField(\"job_name\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"submission_time\", StringType(), True),\n",
    "            StructField(\"completion_time\", StringType(), True),\n",
    "            StructField(\"num_tasks\", LongType(), True),\n",
    "            StructField(\"num_active_tasks\", LongType(), True),\n",
    "            StructField(\"num_completed_tasks\", LongType(), True),\n",
    "            StructField(\"num_failed_tasks\", LongType(), True),\n",
    "            StructField(\"num_stages\", LongType(), True),\n",
    "            StructField(\"stage_ids\", StringType(), True),\n",
    "            StructField(\"task_success_rate\", DoubleType(), True)\n",
    "        ])\n",
    "        stages_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True),\n",
    "            StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True),\n",
    "            StructField(\"stage_id\", LongType(), True),\n",
    "            StructField(\"stage_name\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"num_tasks\", LongType(), True),\n",
    "            StructField(\"num_active_tasks\", LongType(), True),\n",
    "            StructField(\"num_complete_tasks\", LongType(), True),\n",
    "            StructField(\"num_failed_tasks\", LongType(), True),\n",
    "            StructField(\"executor_run_time\", LongType(), True),\n",
    "            StructField(\"executor_cpu_time\", LongType(), True),\n",
    "            StructField(\"submission_time\", StringType(), True),\n",
    "            StructField(\"first_task_launched_time\", StringType(), True),\n",
    "            StructField(\"completion_time\", StringType(), True),\n",
    "            StructField(\"input_bytes\", LongType(), True),\n",
    "            StructField(\"output_bytes\", LongType(), True),\n",
    "            StructField(\"shuffle_read_bytes\", LongType(), True),\n",
    "            StructField(\"shuffle_write_bytes\", LongType(), True),\n",
    "            StructField(\"memory_bytes_spilled\", LongType(), True),\n",
    "            StructField(\"disk_bytes_spilled\", LongType(), True),\n",
    "            StructField(\"task_completion_rate\", DoubleType(), True),\n",
    "            StructField(\"avg_executor_run_time_per_task\", DoubleType(), True),\n",
    "            StructField(\"total_data_processed_mb\", DoubleType(), True),\n",
    "            StructField(\"shuffle_data_mb\", DoubleType(), True)\n",
    "        ])\n",
    "        sql_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True),\n",
    "            StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True),\n",
    "            StructField(\"sql_id\", LongType(), True),\n",
    "            StructField(\"description\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"duration_ms\", LongType(), True),\n",
    "            StructField(\"duration_minutes\", DoubleType(), True),\n",
    "            StructField(\"submission_time\", StringType(), True),\n",
    "            StructField(\"success_job_ids\", StringType(), True),\n",
    "            StructField(\"failed_job_ids\", StringType(), True),\n",
    "            StructField(\"num_nodes\", LongType(), True),\n",
    "            StructField(\"sql_raw_json\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        apps_df = self.spark.createDataFrame(applications_analysis, schema=applications_schema) if applications_analysis else None\n",
    "        jobs_df = self.spark.createDataFrame(jobs_analysis, schema=jobs_schema) if jobs_analysis else None\n",
    "        stages_df = self.spark.createDataFrame(stages_analysis, schema=stages_schema) if stages_analysis else None\n",
    "        sql_df = self.spark.createDataFrame(sql_analysis, schema=sql_schema) if sql_analysis else None\n",
    "\n",
    "        return apps_df, jobs_df, stages_df, sql_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c220e39c-99f5-4106-92ce-853067e94ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cluster Analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "362467e8-99d1-4d8c-86a5-fabec728768f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def analyze_single_cluster(\n",
    "    cluster_info: Dict,\n",
    "    shs_client_class: Any,\n",
    "    analyzer_class: Any,\n",
    "    timeout_seconds: int,\n",
    "    max_applications: int,\n",
    "    spark_session: 'SparkSession',\n",
    "    persistent_ui_timeout: int,\n",
    "    max_app_threads: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyzes a single EMR cluster, fetching application, job, stage, and SQL query data.\n",
    "    This function is designed to be executed concurrently.\n",
    "\n",
    "    :param cluster_info: Dictionary containing cluster details (id, name, arn, status).\n",
    "    :param shs_client_class: The SparkHistoryServerClient class to instantiate.\n",
    "    :param analyzer_class: The SparkMetricsAnalyzer class to instantiate.\n",
    "    :param timeout_seconds: Timeout for HTTP requests to the Spark History Server.\n",
    "    :param max_applications: Maximum number of applications to analyze per cluster.\n",
    "    :param spark_session: The active SparkSession for creating DataFrames.\n",
    "    :param persistent_ui_timeout: The maximum time in seconds to wait for the Persistent App UI.\n",
    "    :param max_app_threads: The maximum number of concurrent threads for analyzing applications.\n",
    "    :returns: A dictionary containing analyzed data and a summary for the cluster.\n",
    "    \"\"\"\n",
    "    cluster_id = cluster_info['cluster_id']\n",
    "    cluster_name = cluster_info['cluster_name']\n",
    "    cluster_arn = cluster_info['cluster_arn']\n",
    "\n",
    "    logger.info(\"\uD83D\uDD75️ Starting analysis for cluster: %s (%s)\", cluster_name, cluster_id)\n",
    "\n",
    "    cluster_analysis_results = {\n",
    "        'cluster_id': cluster_id,\n",
    "        'cluster_name': cluster_name,\n",
    "        'status': cluster_info.get('status', 'UNKNOWN'),\n",
    "        'total_applications': 0,\n",
    "        'total_jobs': 0,\n",
    "        'total_stages': 0,\n",
    "        'total_sql_queries': 0,\n",
    "        'analysis_status': 'FAILED',\n",
    "        'applications': [],\n",
    "        'jobs': [],\n",
    "        'stages': [],\n",
    "        'sql_queries': [],\n",
    "        'applications_endpoint_success': None,\n",
    "        'jobs_endpoint_success': None,\n",
    "        'sql_endpoint_success': None,\n",
    "        'error_message': \"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        server_config = ServerConfig(\n",
    "            emr_cluster_arn=cluster_arn,\n",
    "            timeout=timeout_seconds\n",
    "        )\n",
    "        emr_client = EMRPersistentUIClient(server_config)\n",
    "\n",
    "        logger.info(\"\uD83D\uDD11 Initializing EMR Persistent UI connection for %s...\", cluster_name)\n",
    "        base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n",
    "\n",
    "        logger.info(\"\uD83D\uDCCA Setting up Spark History Server client for %s...\", cluster_name)\n",
    "        shs_client = shs_client_class(base_url, session)\n",
    "        analyzer = analyzer_class(spark_session)\n",
    "\n",
    "        logger.info(\"\uD83D\uDCC4 Fetching Spark applications for %s...\", cluster_name)\n",
    "        try:\n",
    "            applications = shs_client.get_applications(limit=max_applications)\n",
    "            cluster_analysis_results['applications_endpoint_success'] = True\n",
    "        except Exception as app_ex:\n",
    "            cluster_analysis_results['applications_endpoint_success'] = False\n",
    "            cluster_analysis_results['error_message'] += f\"Applications endpoint error: {str(app_ex)}; \"\n",
    "            logger.error(\"❌ Failed to fetch applications for %s: %s\", cluster_name, str(app_ex), exc_info=True)\n",
    "            return cluster_analysis_results\n",
    "\n",
    "        if not applications:\n",
    "            logger.warning(\"⚠️ No applications found in Spark History Server for %s\", cluster_name)\n",
    "            cluster_analysis_results['analysis_status'] = 'NO_APPLICATIONS'\n",
    "            return cluster_analysis_results\n",
    "\n",
    "        logger.info(\"✅ Found %s applications to analyze in %s\", len(applications), cluster_name)\n",
    "\n",
    "        app_futures = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_app_threads) as app_executor:\n",
    "            for app in applications:\n",
    "                app_id = app.get('id')\n",
    "                app_futures.append(app_executor.submit(\n",
    "                    process_single_application,\n",
    "                    app_id,\n",
    "                    shs_client,\n",
    "                    analyzer,\n",
    "                    cluster_analysis_results\n",
    "                ))\n",
    "\n",
    "        for future in concurrent.futures.as_completed(app_futures):\n",
    "            try:\n",
    "                app_data = future.result()\n",
    "                if app_data:\n",
    "                    cluster_analysis_results['applications'].append(app_data['application'])\n",
    "                    cluster_analysis_results['jobs'].extend(app_data['jobs'])\n",
    "                    cluster_analysis_results['stages'].extend(app_data['stages'])\n",
    "                    cluster_analysis_results['sql_queries'].extend(app_data['sql_queries'])\n",
    "                    cluster_analysis_results['total_applications'] += 1\n",
    "                    cluster_analysis_results['total_jobs'] += len(app_data['jobs'])\n",
    "                    cluster_analysis_results['total_stages'] += len(app_data['stages'])\n",
    "                    cluster_analysis_results['total_sql_queries'] += len(app_data['sql_queries'])\n",
    "            except Exception as app_e:\n",
    "                logger.error(\" ❌ Error analyzing an application in cluster %s: %s\", cluster_name, str(app_e), exc_info=True)\n",
    "\n",
    "        cluster_analysis_results['analysis_status'] = 'COMPLETED'\n",
    "        logger.info(\"✅ Completed analysis for cluster: %s\", cluster_name)\n",
    "        logger.info(\n",
    "            \" Applications: %s, Jobs: %s, Stages: %s, SQL Queries: %s\",\n",
    "            cluster_analysis_results['total_applications'],\n",
    "            cluster_analysis_results['total_jobs'],\n",
    "            cluster_analysis_results['total_stages'],\n",
    "            cluster_analysis_results['total_sql_queries']\n",
    "        )\n",
    "        return cluster_analysis_results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"❌ Failed to analyze cluster %s: %s\", cluster_name, str(e), exc_info=True)\n",
    "        cluster_analysis_results['analysis_status'] = 'FAILED'\n",
    "        cluster_analysis_results['error_message'] += f\"Cluster analysis error: {str(e)}\"\n",
    "        return cluster_analysis_results\n",
    "\n",
    "def process_single_application(app_id: str, shs_client: Any, analyzer: Any, cluster_results: Dict) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes a single Spark application's data. Designed for concurrent execution.\n",
    "    \"\"\"\n",
    "    logger.info(\" \uD83D\uDD75️ Analyzing application: %s\", app_id)\n",
    "    app_results = {'application': None, 'jobs': [], 'stages': [], 'sql_queries': []}\n",
    "    try:\n",
    "        app_details = shs_client.get_application_details(app_id)\n",
    "\n",
    "        latest_attempt_id_to_use = None\n",
    "        attempts = app_details.get('attempts')\n",
    "        if attempts and isinstance(attempts, list):\n",
    "            attempt_ids = [int(a['attemptId']) for a in attempts if a.get('attemptId')]\n",
    "            if attempt_ids:\n",
    "                latest_attempt_id = max(attempt_ids)\n",
    "                logger.info(\" Latest attempt ID for application %s is: %d\", app_id, latest_attempt_id)\n",
    "                if latest_attempt_id > 1:\n",
    "                    latest_attempt_id_to_use = str(latest_attempt_id)\n",
    "                    logger.info(\" Will use attempt ID %s for API calls.\", latest_attempt_id_to_use)\n",
    "                else:\n",
    "                    logger.info(\" Latest attempt ID is 1, so it will not be appended to the URL.\")\n",
    "\n",
    "        app_results['application'] = analyzer.analyze_application_performance(app_details)\n",
    "\n",
    "        try:\n",
    "            jobs = shs_client.get_application_jobs(app_id, attempt_id=latest_attempt_id_to_use)\n",
    "            if jobs:\n",
    "                app_results['jobs'] = analyzer.analyze_job_performance(jobs)\n",
    "            if cluster_results['jobs_endpoint_success'] is None:\n",
    "                cluster_results['jobs_endpoint_success'] = True\n",
    "        except Exception as job_ex:\n",
    "            if cluster_results['jobs_endpoint_success'] is None:\n",
    "                cluster_results['jobs_endpoint_success'] = False\n",
    "            cluster_results['error_message'] += f\"Jobs endpoint error: {str(job_ex)}; \"\n",
    "            logger.error(\" ❌ Failed to fetch jobs for application %s: %s\", app_id, str(job_ex), exc_info=True)\n",
    "\n",
    "        try:\n",
    "            stages = shs_client.get_application_stages(app_id, attempt_id=latest_attempt_id_to_use)\n",
    "            if stages:\n",
    "                app_results['stages'] = analyzer.analyze_stage_performance(stages)\n",
    "        except Exception as stage_ex:\n",
    "            logger.error(\" ❌ Failed to fetch stages for application %s: %s\", app_id, str(stage_ex), exc_info=True)\n",
    "\n",
    "        try:\n",
    "            sql_queries = shs_client.get_application_sql_queries(app_id, attempt_id=latest_attempt_id_to_use)\n",
    "            if sql_queries:\n",
    "                app_results['sql_queries'] = analyzer.analyze_sql_queries(sql_queries)\n",
    "            if cluster_results['sql_endpoint_success'] is None:\n",
    "                cluster_results['sql_endpoint_success'] = True\n",
    "        except Exception as sql_ex:\n",
    "            if cluster_results['sql_endpoint_success'] is None:\n",
    "                cluster_results['sql_endpoint_success'] = False\n",
    "            cluster_results['error_message'] += f\"SQL endpoint error: {str(sql_ex)}; \"\n",
    "            logger.error(\" ❌ Failed to fetch SQL queries for application %s: %s\", app_id, str(sql_ex), exc_info=True)\n",
    "\n",
    "        return app_results\n",
    "    except Exception as e:\n",
    "        logger.error(\" ❌ Failed to analyze application %s: %s\", app_id, str(e), exc_info=True)\n",
    "        return None\n",
    "\n",
    "def summarize_cluster_status(cluster_results: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate status details based on cluster analysis results.\n",
    "    \"\"\"\n",
    "    status_details = \"\"\n",
    "\n",
    "    if cluster_results['analysis_status'] == 'FAILED':\n",
    "        status_details = cluster_results.get('error_message', 'Unknown error - see logs')\n",
    "    elif cluster_results['analysis_status'] == 'COMPLETED':\n",
    "        endpoint_statuses = []\n",
    "        if cluster_results.get('applications_endpoint_success') is not None:\n",
    "            endpoint_statuses.append(f\"applications: {'OK' if cluster_results['applications_endpoint_success'] else 'FAILED'}\")\n",
    "        if cluster_results.get('jobs_endpoint_success') is not None:\n",
    "            endpoint_statuses.append(f\"jobs: {'OK' if cluster_results['jobs_endpoint_success'] else 'FAILED'}\")\n",
    "        if cluster_results.get('sql_endpoint_success') is not None:\n",
    "            endpoint_statuses.append(f\"sql: {'OK' if cluster_results['sql_endpoint_success'] else 'FAILED'}\")\n",
    "        status_details = \"; \".join(endpoint_statuses) if endpoint_statuses else \"Endpoints not checked\"\n",
    "    elif cluster_results['analysis_status'] == 'NO_APPLICATIONS':\n",
    "        status_details = \"No applications found in Spark History Server\"\n",
    "\n",
    "    return status_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4609b16-b029-4e8c-a02b-838292503a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c190ed-bd9a-46b1-9abe-77fab6102293",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Execution"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 00:03:30,816 - INFO - \uD83D\uDE80 Starting EMR Spark History Server Analysis\n2025-08-19 00:03:30,817 - INFO - \uD83D\uDD2D Multi-cluster discovery mode - searching for EMR clusters\n2025-08-19 00:03:30,819 - INFO - \uD83D\uDD0D Discovering EMR clusters in region: us-east-1\n2025-08-19 00:03:30,820 - INFO -  States: ['STARTING', 'BOOTSTRAPPING', 'RUNNING', 'WAITING', 'TERMINATING', 'TERMINATED', 'TERMINATED_WITH_ERRORS']\n2025-08-19 00:03:30,820 - INFO -  Name filter: None\n2025-08-19 00:03:30,820 - INFO -  Max clusters: 10\n2025-08-19 00:03:30,820 - INFO -  Created After: None\n2025-08-19 00:03:30,820 - INFO -  Created Before: None\n2025-08-19 00:03:31,050 - INFO - ✅ Discovered 10 clusters\n2025-08-19 00:03:31,051 - INFO -  1. spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow (j-2C9L9Y9WYQ6H8) - TERMINATED\n2025-08-19 00:03:31,051 - INFO -  2. spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow (j-2F9HRWX3WB2KP) - TERMINATED\n2025-08-19 00:03:31,051 - INFO -  3. spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow (j-137KTJ1T8J5TS) - TERMINATED\n2025-08-19 00:03:31,051 - INFO -  4. spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow (j-160Y1JZF990JG) - TERMINATED\n2025-08-19 00:03:31,052 - INFO -  5. Gautam: CleanRoomUserIdSellerAgg (j-1ILKHEQ510RAI) - TERMINATED\n2025-08-19 00:03:31,052 - INFO -  6. Gautam: CleanRoomUserIdSellerAgg (j-1KPXX0HR3U0BN) - TERMINATED\n2025-08-19 00:03:31,052 - INFO -  7. Gautam: CleanRoomUserIdSellerAgg (j-2O70N006Y1DEP) - TERMINATED\n2025-08-19 00:03:31,052 - INFO -  8. Gautam: UserId Seller Agg (j-310AFAY6L57Q) - TERMINATED\n2025-08-19 00:03:31,052 - INFO -  9. Gautam: UserId Seller Agg (j-38XT10OQKQR6U) - TERMINATED\n2025-08-19 00:03:31,052 - INFO -  10. Gautam: UserId Seller Agg (j-1VJLYTUUO02M0) - TERMINATED\n2025-08-19 00:03:31,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:31,156 - INFO - Cluster j-2C9L9Y9WYQ6H8 uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:31,270 - INFO - Successfully retrieved instance fleet info for j-2C9L9Y9WYQ6H8\n2025-08-19 00:03:31,271 - INFO - ✅ Cluster j-2C9L9Y9WYQ6H8 is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:31,362 - INFO - Cluster j-2F9HRWX3WB2KP uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:31,450 - INFO - Successfully retrieved instance fleet info for j-2F9HRWX3WB2KP\n2025-08-19 00:03:31,450 - INFO - ✅ Cluster j-2F9HRWX3WB2KP is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:31,540 - INFO - Cluster j-137KTJ1T8J5TS uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:31,656 - INFO - Successfully retrieved instance fleet info for j-137KTJ1T8J5TS\n2025-08-19 00:03:31,656 - INFO - ✅ Cluster j-137KTJ1T8J5TS is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:31,763 - INFO - Cluster j-160Y1JZF990JG uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:31,866 - INFO - Successfully retrieved instance fleet info for j-160Y1JZF990JG\n2025-08-19 00:03:31,866 - INFO - ✅ Cluster j-160Y1JZF990JG is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:32,014 - INFO - Cluster j-1ILKHEQ510RAI uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:32,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:32,196 - INFO - Successfully retrieved instance fleet info for j-1ILKHEQ510RAI\n2025-08-19 00:03:32,196 - INFO - ✅ Cluster j-1ILKHEQ510RAI is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:32,309 - INFO - Cluster j-1KPXX0HR3U0BN uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:32,474 - INFO - Successfully retrieved instance fleet info for j-1KPXX0HR3U0BN\n2025-08-19 00:03:32,474 - INFO - ✅ Cluster j-1KPXX0HR3U0BN is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:32,596 - INFO - Cluster j-2O70N006Y1DEP uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:32,742 - INFO - Successfully retrieved instance fleet info for j-2O70N006Y1DEP\n2025-08-19 00:03:32,743 - INFO - ✅ Cluster j-2O70N006Y1DEP is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:32,894 - INFO - Cluster j-310AFAY6L57Q uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:33,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:34,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:35,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:36,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:36,703 - WARNING - Could not get instance fleet details for j-310AFAY6L57Q: ThrottlingException - Rate exceeded\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1795/command-8691664888252992-1860489246\", line 173, in get_cluster_details\n    instance_groups = self.emr_client.list_instance_groups(ClusterId=cluster_id)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b5ee6ad2-fffd-4e68-bf14-5e54c3c43dce/lib/python3.12/site-packages/botocore/client.py\", line 602, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b5ee6ad2-fffd-4e68-bf14-5e54c3c43dce/lib/python3.12/site-packages/botocore/context.py\", line 123, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b5ee6ad2-fffd-4e68-bf14-5e54c3c43dce/lib/python3.12/site-packages/botocore/client.py\", line 1078, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the ListInstanceGroups operation: Instance fleets and instance groups are mutually exclusive. The EMR cluster specified in the request uses instance fleets. The ListInstanceGroups operation does not support clusters that use instance fleets. Use the ListInstanceFleets operation instead.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1795/command-8691664888252992-1860489246\", line 190, in get_cluster_details\n    instance_fleets = self.emr_client.list_instance_fleets(ClusterId=cluster_id)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b5ee6ad2-fffd-4e68-bf14-5e54c3c43dce/lib/python3.12/site-packages/botocore/client.py\", line 602, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b5ee6ad2-fffd-4e68-bf14-5e54c3c43dce/lib/python3.12/site-packages/botocore/context.py\", line 123, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b5ee6ad2-fffd-4e68-bf14-5e54c3c43dce/lib/python3.12/site-packages/botocore/client.py\", line 1078, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (ThrottlingException) when calling the ListInstanceFleets operation (reached max retries: 4): Rate exceeded\n2025-08-19 00:03:36,704 - INFO - ✅ Cluster j-310AFAY6L57Q is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:36,793 - INFO - Cluster j-38XT10OQKQR6U uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:37,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:38,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:38,346 - INFO - Successfully retrieved instance fleet info for j-38XT10OQKQR6U\n2025-08-19 00:03:38,346 - INFO - ✅ Cluster j-38XT10OQKQR6U is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:38,433 - INFO - Cluster j-1VJLYTUUO02M0 uses instance fleets. Fetching instance fleet details.\n2025-08-19 00:03:38,846 - INFO - Successfully retrieved instance fleet info for j-1VJLYTUUO02M0\n2025-08-19 00:03:38,847 - INFO - ✅ Cluster j-1VJLYTUUO02M0 is valid for analysis (Spark: True, State: TERMINATED)\n2025-08-19 00:03:38,847 - INFO - \uD83D\uDCCA Will analyze 10 cluster(s)\n2025-08-19 00:03:38,848 - INFO - \uD83D\uDD75️ Starting analysis for cluster: spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow (j-2C9L9Y9WYQ6H8)\n2025-08-19 00:03:38,849 - INFO - \uD83D\uDD75️ Starting analysis for cluster: spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow (j-2F9HRWX3WB2KP)\n2025-08-19 00:03:38,849 - INFO - \uD83D\uDD75️ Starting analysis for cluster: spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow (j-137KTJ1T8J5TS)\n2025-08-19 00:03:38,851 - INFO - \uD83D\uDD75️ Starting analysis for cluster: spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow (j-160Y1JZF990JG)\n2025-08-19 00:03:38,853 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow...\n2025-08-19 00:03:38,853 - INFO - \uD83D\uDD75️ Starting analysis for cluster: Gautam: CleanRoomUserIdSellerAgg (j-1ILKHEQ510RAI)\n2025-08-19 00:03:38,854 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow...\n2025-08-19 00:03:38,855 - INFO - \uD83D\uDD75️ Starting analysis for cluster: Gautam: CleanRoomUserIdSellerAgg (j-1KPXX0HR3U0BN)\n2025-08-19 00:03:38,857 - INFO - \uD83D\uDD75️ Starting analysis for cluster: Gautam: CleanRoomUserIdSellerAgg (j-2O70N006Y1DEP)\n2025-08-19 00:03:38,857 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow...\n2025-08-19 00:03:38,954 - INFO - \uD83D\uDD75️ Starting analysis for cluster: Gautam: UserId Seller Agg (j-310AFAY6L57Q)\n2025-08-19 00:03:38,954 - INFO - \uD83D\uDD75️ Starting analysis for cluster: Gautam: UserId Seller Agg (j-38XT10OQKQR6U)\n2025-08-19 00:03:38,973 - INFO - \uD83D\uDD75️ Starting analysis for cluster: Gautam: UserId Seller Agg (j-1VJLYTUUO02M0)\n2025-08-19 00:03:38,973 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-2C9L9Y9WYQ6H8\n2025-08-19 00:03:38,974 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow...\n2025-08-19 00:03:38,977 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for Gautam: CleanRoomUserIdSellerAgg...\n2025-08-19 00:03:38,977 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-2F9HRWX3WB2KP\n2025-08-19 00:03:38,979 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for Gautam: CleanRoomUserIdSellerAgg...\n2025-08-19 00:03:38,981 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for Gautam: CleanRoomUserIdSellerAgg...\n2025-08-19 00:03:38,981 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-137KTJ1T8J5TS\n2025-08-19 00:03:38,984 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for Gautam: UserId Seller Agg...\n2025-08-19 00:03:38,987 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for Gautam: UserId Seller Agg...\n2025-08-19 00:03:38,989 - INFO - \uD83D\uDD11 Initializing EMR Persistent UI connection for Gautam: UserId Seller Agg...\n2025-08-19 00:03:38,990 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-160Y1JZF990JG\n2025-08-19 00:03:38,991 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-1ILKHEQ510RAI\n2025-08-19 00:03:38,992 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-1KPXX0HR3U0BN\n2025-08-19 00:03:38,993 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-2O70N006Y1DEP\n2025-08-19 00:03:38,994 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-310AFAY6L57Q\n2025-08-19 00:03:38,995 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-38XT10OQKQR6U\n2025-08-19 00:03:38,995 - INFO - Creating persistent app UI for cluster: arn:aws:elasticmapreduce:us-east-1:503911722519:cluster/j-1VJLYTUUO02M0\n2025-08-19 00:03:39,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:39,347 - INFO - ✅ Persistent App UI created successfully\n2025-08-19 00:03:39,348 - INFO -  Persistent UI ID: p-2C9L9Y9WYQ6H8\n2025-08-19 00:03:39,348 - INFO -  Runtime Role Enabled: False\n2025-08-19 00:03:39,348 - INFO - Waiting for Persistent App UI to become ready...\n2025-08-19 00:03:39,394 - INFO - ✅ Persistent App UI created successfully\n2025-08-19 00:03:39,395 - INFO -  Persistent UI ID: p-160Y1JZF990JG\n2025-08-19 00:03:39,395 - INFO -  Runtime Role Enabled: False\n2025-08-19 00:03:39,395 - INFO - Waiting for Persistent App UI to become ready...\n2025-08-19 00:03:39,402 - INFO - ✅ Persistent App UI created successfully\n2025-08-19 00:03:39,402 - INFO -  Persistent UI ID: p-137KTJ1T8J5TS\n2025-08-19 00:03:39,402 - INFO -  Runtime Role Enabled: False\n2025-08-19 00:03:39,402 - INFO - Waiting for Persistent App UI to become ready...\n2025-08-19 00:03:39,451 - INFO - ✅ Persistent App UI created successfully\n2025-08-19 00:03:39,451 - INFO -  Persistent UI ID: p-38XT10OQKQR6U\n2025-08-19 00:03:39,451 - INFO -  Runtime Role Enabled: False\n2025-08-19 00:03:39,451 - INFO - Waiting for Persistent App UI to become ready...\n2025-08-19 00:03:39,539 - INFO - ✅ Persistent App UI created successfully\n2025-08-19 00:03:39,540 - INFO -  Persistent UI ID: p-1ILKHEQ510RAI\n2025-08-19 00:03:39,540 - INFO -  Runtime Role Enabled: False\n2025-08-19 00:03:39,540 - INFO - Waiting for Persistent App UI to become ready...\n2025-08-19 00:03:39,542 - INFO - ✅ Persistent App UI is ready.\n2025-08-19 00:03:39,542 - INFO - Getting presigned URL for persistent app UI: p-2C9L9Y9WYQ6H8 (type: SHS)\n2025-08-19 00:03:39,608 - INFO - ✅ Persistent App UI is ready.\n2025-08-19 00:03:39,608 - INFO - Getting presigned URL for persistent app UI: p-160Y1JZF990JG (type: SHS)\n2025-08-19 00:03:39,615 - INFO - ✅ Presigned URL obtained successfully\n2025-08-19 00:03:39,616 - INFO -  Base URL: https://p-2C9L9Y9WYQ6H8.emrappui-prod.us-east-1.amazonaws.com/shs\n2025-08-19 00:03:39,616 - INFO - Setting up HTTP session with cookie management\n2025-08-19 00:03:39,616 - INFO - Making initial request\n2025-08-19 00:03:39,683 - INFO - ✅ Presigned URL obtained successfully\n2025-08-19 00:03:39,683 - INFO -  Base URL: https://p-160Y1JZF990JG.emrappui-prod.us-east-1.amazonaws.com/shs\n2025-08-19 00:03:39,683 - INFO - Setting up HTTP session with cookie management\n2025-08-19 00:03:39,684 - INFO - Making initial request\n2025-08-19 00:03:39,860 - INFO - ✅ Persistent App UI is ready.\n2025-08-19 00:03:39,860 - INFO - Getting presigned URL for persistent app UI: p-1ILKHEQ510RAI (type: SHS)\n2025-08-19 00:03:39,978 - INFO - ✅ Presigned URL obtained successfully\n2025-08-19 00:03:39,978 - INFO -  Base URL: https://p-1ILKHEQ510RAI.emrappui-prod.us-east-1.amazonaws.com/shs\n2025-08-19 00:03:39,978 - INFO - Setting up HTTP session with cookie management\n2025-08-19 00:03:39,978 - INFO - Making initial request\n2025-08-19 00:03:40,039 - INFO - ✅ Persistent App UI created successfully\n2025-08-19 00:03:40,040 - INFO -  Persistent UI ID: p-310AFAY6L57Q\n2025-08-19 00:03:40,040 - INFO -  Runtime Role Enabled: False\n2025-08-19 00:03:40,040 - INFO - Waiting for Persistent App UI to become ready...\n2025-08-19 00:03:40,056 - INFO - Received command c on object id p0\n2025-08-19 00:03:40,088 - INFO - ✅ Persistent App UI created successfully\n2025-08-19 00:03:40,088 - INFO -  Persistent UI ID: p-2F9HRWX3WB2KP\n2025-08-19 00:03:40,089 - INFO -  Runtime Role Enabled: False\n2025-08-19 00:03:40,089 - INFO - Waiting for Persistent App UI to become ready...\n2025-08-19 00:03:40,112 - INFO - ✅ HTTP session established successfully\n2025-08-19 00:03:40,113 - INFO -  Status Code: 200\n2025-08-19 00:03:40,113 - INFO -  Cookies: 8 cookie(s) stored\n2025-08-19 00:03:40,113 - INFO - \uD83D\uDCCA Setting up Spark History Server client for spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow...\n2025-08-19 00:03:40,113 - INFO - \uD83D\uDCC4 Fetching Spark applications for spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow...\n2025-08-19 00:03:40,113 - INFO - Fetching applications (status: None, limit: 10)\n2025-08-19 00:03:40,122 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:03:40,160 - INFO - ✅ Persistent App UI is ready.\n2025-08-19 00:03:40,161 - INFO - Getting presigned URL for persistent app UI: p-310AFAY6L57Q (type: SHS)\n2025-08-19 00:03:40,243 - INFO - ✅ HTTP session established successfully\n2025-08-19 00:03:40,243 - INFO -  Status Code: 200\n2025-08-19 00:03:40,243 - INFO -  Cookies: 8 cookie(s) stored\n2025-08-19 00:03:40,243 - INFO - \uD83D\uDCCA Setting up Spark History Server client for spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow...\n2025-08-19 00:03:40,244 - INFO - \uD83D\uDCC4 Fetching Spark applications for spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow...\n2025-08-19 00:03:40,244 - INFO - Fetching applications (status: None, limit: 10)\n2025-08-19 00:03:40,246 - INFO - ✅ Retrieved 1 applications\n2025-08-19 00:03:40,246 - INFO - ✅ Found 1 applications to analyze in spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow\n2025-08-19 00:03:40,247 - INFO -  \uD83D\uDD75️ Analyzing application: application_1755294357227_0001\n2025-08-19 00:03:40,247 - INFO - Fetching application details for: application_1755294357227_0001\n2025-08-19 00:03:40,250 - INFO - ✅ Presigned URL obtained successfully\n2025-08-19 00:03:40,250 - INFO -  Base URL: https://p-310AFAY6L57Q.emrappui-prod.us-east-1.amazonaws.com/shs\n2025-08-19 00:03:40,250 - INFO - Setting up HTTP session with cookie management\n2025-08-19 00:03:40,250 - INFO - Making initial request\n2025-08-19 00:03:40,272 - INFO - ✅ Persistent App UI created successfully\n2025-08-19 00:03:40,272 - INFO -  Persistent UI ID: p-2O70N006Y1DEP\n2025-08-19 00:03:40,272 - INFO -  Runtime Role Enabled: False\n2025-08-19 00:03:40,272 - INFO - Waiting for Persistent App UI to become ready...\n2025-08-19 00:03:40,340 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:03:40,373 - INFO - ✅ HTTP session established successfully\n2025-08-19 00:03:40,374 - INFO -  Status Code: 200\n2025-08-19 00:03:40,374 - INFO -  Cookies: 8 cookie(s) stored\n2025-08-19 00:03:40,374 - INFO - \uD83D\uDCCA Setting up Spark History Server client for Gautam: CleanRoomUserIdSellerAgg...\n2025-08-19 00:03:40,374 - INFO - \uD83D\uDCC4 Fetching Spark applications for Gautam: CleanRoomUserIdSellerAgg...\n2025-08-19 00:03:40,374 - INFO - Fetching applications (status: None, limit: 10)\n2025-08-19 00:03:40,402 - INFO - ✅ Retrieved details for application: application_1755294357227_0001\n2025-08-19 00:03:40,402 - INFO -  Latest attempt ID for application application_1755294357227_0001 is: 1\n2025-08-19 00:03:40,402 - INFO -  Latest attempt ID is 1, so it will not be appended to the URL.\n2025-08-19 00:03:40,402 - INFO - Fetching jobs for application: application_1755294357227_0001 (Attempt ID: N/A)\n2025-08-19 00:03:40,408 - INFO - ✅ Retrieved 1 applications\n2025-08-19 00:03:40,408 - INFO - ✅ Found 1 applications to analyze in spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow\n2025-08-19 00:03:40,408 - INFO -  \uD83D\uDD75️ Analyzing application: application_1755540700327_0001\n2025-08-19 00:03:40,409 - INFO - Fetching application details for: application_1755540700327_0001\n2025-08-19 00:03:40,504 - INFO - ✅ Retrieved 1 applications\n2025-08-19 00:03:40,504 - INFO - ✅ Found 1 applications to analyze in Gautam: CleanRoomUserIdSellerAgg\n2025-08-19 00:03:40,504 - INFO -  \uD83D\uDD75️ Analyzing application: application_1754586783613_0001\n2025-08-19 00:03:40,505 - INFO - Fetching application details for: application_1754586783613_0001\n2025-08-19 00:03:40,520 - INFO - ✅ Persistent App UI created successfully\n2025-08-19 00:03:40,520 - INFO -  Persistent UI ID: p-1KPXX0HR3U0BN\n2025-08-19 00:03:40,521 - INFO -  Runtime Role Enabled: False\n2025-08-19 00:03:40,521 - INFO - Waiting for Persistent App UI to become ready...\n2025-08-19 00:03:40,542 - ERROR - ❌ Failed to make request to https://p-160Y1JZF990JG.emrappui-prod.us-east-1.amazonaws.com/shs/api/v1/applications/application_1755294357227_0001/jobs: 404 Client Error: Not Found for url: https://p-160y1jzf990jg.emrappui-prod.us-east-1.amazonaws.com/shs/api/v1/applications/application_1755294357227_0001/jobs\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1795/command-8691664888252994-4051220389\", line 37, in _make_request\n    response.raise_for_status()\n  File \"/databricks/python/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://p-160y1jzf990jg.emrappui-prod.us-east-1.amazonaws.com/shs/api/v1/applications/application_1755294357227_0001/jobs\n2025-08-19 00:03:40,543 - ERROR -  ❌ Failed to fetch jobs for application application_1755294357227_0001: 404 Client Error: Not Found for url: https://p-160y1jzf990jg.emrappui-prod.us-east-1.amazonaws.com/shs/api/v1/applications/application_1755294357227_0001/jobs\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1795/command-8691664888254102-553358141\", line 156, in process_single_application\n    jobs = shs_client.get_application_jobs(app_id, attempt_id=latest_attempt_id_to_use)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1795/command-8691664888252994-4051220389\", line 88, in get_application_jobs\n    jobs = self._make_request(f\"{endpoint}/jobs\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1795/command-8691664888252994-4051220389\", line 37, in _make_request\n    response.raise_for_status()\n  File \"/databricks/python/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://p-160y1jzf990jg.emrappui-prod.us-east-1.amazonaws.com/shs/api/v1/applications/application_1755294357227_0001/jobs\n2025-08-19 00:03:40,543 - INFO - Fetching stages for application: application_1755294357227_0001 (Attempt ID: N/A)\n2025-08-19 00:03:40,554 - INFO - ✅ Retrieved details for application: application_1755540700327_0001\n2025-08-19 00:03:40,554 - INFO -  Latest attempt ID for application application_1755540700327_0001 is: 1\n2025-08-19 00:03:40,554 - INFO -  Latest attempt ID is 1, so it will not be appended to the URL.\n2025-08-19 00:03:40,555 - INFO - Fetching jobs for application: application_1755540700327_0001 (Attempt ID: N/A)\n2025-08-19 00:03:40,605 - INFO - ✅ HTTP session established successfully\n2025-08-19 00:03:40,606 - INFO -  Status Code: 200\n2025-08-19 00:03:40,606 - INFO -  Cookies: 8 cookie(s) stored\n2025-08-19 00:03:40,606 - INFO - \uD83D\uDCCA Setting up Spark History Server client for Gautam: UserId Seller Agg...\n2025-08-19 00:03:40,606 - INFO - \uD83D\uDCC4 Fetching Spark applications for Gautam: UserId Seller Agg...\n2025-08-19 00:03:40,607 - INFO - Fetching applications (status: None, limit: 10)\n2025-08-19 00:03:40,640 - INFO - ✅ Retrieved details for application: application_1754586783613_0001\n2025-08-19 00:03:40,640 - INFO -  Latest attempt ID for application application_1754586783613_0001 is: 1\n2025-08-19 00:03:40,641 - INFO -  Latest attempt ID is 1, so it will not be appended to the URL.\n2025-08-19 00:03:40,641 - INFO - Fetching jobs for application: application_1754586783613_0001 (Attempt ID: N/A)\n2025-08-19 00:03:40,659 - INFO - ✅ Persistent App UI is ready.\n2025-08-19 00:03:40,659 - INFO - Getting presigned URL for persistent app UI: p-1KPXX0HR3U0BN (type: SHS)\n2025-08-19 00:03:40,682 - ERROR - ❌ Failed to make request to https://p-160Y1JZF990JG.emrappui-prod.us-east-1.amazonaws.com/shs/api/v1/applications/application_1755294357227_0001/stages: 404 Client Error: Not Found for url: https://p-160y1jzf990jg.emrappui-prod.us-east-1.amazonaws.com/shs/api/v1/applications/application_1755294357227_0001/stages\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1795/command-8691664888252994-4051220389\", line 37, in _make_request\n    response.raise_for_status()\n  File \"/databricks/python/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://p-160y1jzf990jg.emrappui-prod.us-east-1.amazonaws.com/shs/api/v1/applications/application_1755294357227_0001/stages\n2025-08-19 00:03:40,682 - ERROR -  ❌ Failed to fetch stages for application applica\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nINFO - Received command c on object id p0\n2025-08-19 00:04:19,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:20,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:20,699 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:04:20,827 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:04:21,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:22,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:23,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:24,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:25,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:26,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:27,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:28,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:29,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:30,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:30,840 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:04:30,948 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:04:31,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:32,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:33,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:34,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:35,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:36,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:37,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:38,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:39,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:40,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:40,982 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:04:41,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:41,079 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:04:42,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:43,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:44,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:45,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:46,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:47,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:48,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:49,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:50,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:51,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:51,097 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:04:51,180 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:04:52,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:53,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:54,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:55,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:56,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:57,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:58,056 - INFO - Received command c on object id p0\n2025-08-19 00:04:59,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:00,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:01,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:01,237 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:01,292 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:02,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:03,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:04,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:05,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:06,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:07,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:08,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:09,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:10,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:11,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:11,348 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:11,393 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:12,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:13,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:14,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:15,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:16,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:17,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:18,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:19,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:20,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:21,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:21,500 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:21,513 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:22,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:23,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:24,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:25,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:26,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:27,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:28,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:29,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:30,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:31,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:31,643 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:31,644 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:32,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:33,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:34,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:35,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:36,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:37,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:38,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:39,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:40,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:41,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:41,786 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:41,808 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:42,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:43,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:44,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:45,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:46,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:47,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:48,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:49,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:50,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:51,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:51,879 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:51,934 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:05:52,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:53,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:54,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:55,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:56,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:57,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:58,056 - INFO - Received command c on object id p0\n2025-08-19 00:05:59,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:00,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:01,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:02,027 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:02,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:02,067 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:03,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:04,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:05,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:06,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:07,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:08,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:09,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:10,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:11,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:12,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:12,143 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:12,179 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:13,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:14,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:15,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:16,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:17,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:18,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:19,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:20,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:21,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:22,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:22,282 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:22,311 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:23,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:24,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:25,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:26,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:27,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:28,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:29,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:30,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:31,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:32,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:32,439 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:32,443 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:33,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:34,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:35,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:36,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:37,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:38,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:39,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:40,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:41,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:42,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:42,585 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:42,613 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:43,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:44,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:45,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:46,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:47,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:48,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:49,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:50,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:51,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:52,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:52,719 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:52,746 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:06:53,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:54,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:55,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:56,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:57,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:58,056 - INFO - Received command c on object id p0\n2025-08-19 00:06:59,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:00,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:01,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:02,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:02,849 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:02,856 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:03,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:04,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:05,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:06,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:07,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:08,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:09,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:10,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:11,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:12,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:13,022 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:13,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:13,057 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:14,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:15,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:16,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:17,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:18,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:19,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:20,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:21,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:22,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:23,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:23,141 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:23,156 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:24,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:25,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:26,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:27,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:28,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:29,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:30,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:31,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:32,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:33,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:33,293 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:33,299 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:34,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:35,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:36,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:37,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:38,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:39,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:40,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:41,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:42,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:43,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:43,469 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:43,471 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:44,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:45,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:46,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:47,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:48,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:49,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:50,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:51,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:52,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:53,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:53,636 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:53,652 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:07:54,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:55,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:56,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:57,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:58,056 - INFO - Received command c on object id p0\n2025-08-19 00:07:59,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:00,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:01,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:02,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:03,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:03,736 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:08:03,810 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:08:04,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:05,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:06,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:07,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:08,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:09,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:10,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:11,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:12,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:13,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:13,888 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:08:13,909 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:08:14,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:15,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:16,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:17,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:18,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:19,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:20,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:21,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:22,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:23,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:24,040 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:08:24,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:24,071 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:08:25,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:26,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:27,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:28,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:29,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:30,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:31,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:32,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:33,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:34,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:34,166 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:08:34,199 - INFO - Persistent App UI not ready yet. Waiting 10 seconds before retrying...\n2025-08-19 00:08:35,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:36,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:37,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:38,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:39,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:40,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:41,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:42,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:43,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:44,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:44,167 - ERROR - ❌ Failed to analyze cluster Gautam: CleanRoomUserIdSellerAgg: Persistent App UI did not become ready after waiting 300 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1795/command-8691664888254102-553358141\", line 64, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1795/command-8691664888252990-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 300 seconds.\n2025-08-19 00:08:44,200 - ERROR - ❌ Failed to analyze cluster Gautam: UserId Seller Agg: Persistent App UI did not become ready after waiting 300 seconds.\nTraceback (most recent call last):\n  File \"/root/.ipykernel/1795/command-8691664888254102-553358141\", line 64, in analyze_single_cluster\n    base_url, session = emr_client.initialize(max_wait_time=persistent_ui_timeout)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.ipykernel/1795/command-8691664888252990-2145644618\", line 217, in initialize\n    raise ValueError(\nValueError: Persistent App UI did not become ready after waiting 300 seconds.\n2025-08-19 00:08:44,201 - INFO - \uD83D\uDCBE Creating analysis DataFrames...\n2025-08-19 00:08:44,301 - INFO - \uD83D\uDCDD Multi-Cluster Analysis Complete - Summary Results:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n====================================================================================================\n\uD83D\uDCCA CLUSTER ANALYSIS SUMMARY\n====================================================================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 00:08:45,056 - INFO - Received command c on object id p0\n2025-08-19 00:08:46,056 - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Summary (10 clusters):\n+---------------+------------------------------------------------------------------+----------+---------------+-----------------------------------------------------------------------------------------+------------------+----------+------------+-----------------+\n|cluster_id     |cluster_name                                                      |status    |analysis_status|status_details                                                                           |total_applications|total_jobs|total_stages|total_sql_queries|\n+---------------+------------------------------------------------------------------+----------+---------------+-----------------------------------------------------------------------------------------+------------------+----------+------------+-----------------+\n|j-310AFAY6L57Q |Gautam: UserId Seller Agg                                         |TERMINATED|NO_APPLICATIONS|No applications found in Spark History Server                                            |0                 |0         |0           |0                |\n|j-160Y1JZF990JG|spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow|TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED                                              |1                 |0         |0           |0                |\n|j-2C9L9Y9WYQ6H8|spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow|TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED                                              |1                 |0         |0           |0                |\n|j-1ILKHEQ510RAI|Gautam: CleanRoomUserIdSellerAgg                                  |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED                                              |1                 |0         |0           |0                |\n|j-1KPXX0HR3U0BN|Gautam: CleanRoomUserIdSellerAgg                                  |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED                                              |1                 |0         |0           |0                |\n|j-137KTJ1T8J5TS|spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow|TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED                                              |1                 |0         |0           |0                |\n|j-2F9HRWX3WB2KP|spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow|TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED                                              |1                 |0         |0           |0                |\n|j-1VJLYTUUO02M0|Gautam: UserId Seller Agg                                         |TERMINATED|COMPLETED      |applications: OK; jobs: FAILED; sql: FAILED                                              |1                 |0         |0           |0                |\n|j-2O70N006Y1DEP|Gautam: CleanRoomUserIdSellerAgg                                  |TERMINATED|FAILED         |Cluster analysis error: Persistent App UI did not become ready after waiting 300 seconds.|0                 |0         |0           |0                |\n|j-38XT10OQKQR6U|Gautam: UserId Seller Agg                                         |TERMINATED|FAILED         |Cluster analysis error: Persistent App UI did not become ready after waiting 300 seconds.|0                 |0         |0           |0                |\n+---------------+------------------------------------------------------------------+----------+---------------+-----------------------------------------------------------------------------------------+------------------+----------+------------+-----------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 00:08:46,471 - INFO -  • Total Applications analyzed: 7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== TOP 10 APPLICATIONS BY DURATION (ACROSS ALL CLUSTERS) ===\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 00:08:46,810 - INFO - \uD83C\uDF89 Multi-cluster analysis complete! All data returned as DataFrames.\n2025-08-19 00:08:46,812 - INFO - \uD83E\uDDEA Running in DEV mode - S3 output skipped\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+---------------------------------------------------+----------------+-----+----------------+\n|cluster_name                                                      |application_name                                   |duration_minutes|cores|efficiency_score|\n+------------------------------------------------------------------+---------------------------------------------------+----------------+-----+----------------+\n|spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow|CanaryPipeline-class-jobs.dataproc.canary.CanaryJob|0.0             |0    |0.0             |\n|Gautam: CleanRoomUserIdSellerAgg                                  |CleanRoomUserIdSellerAgg                           |0.0             |0    |0.0             |\n|spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow|CanaryPipeline-class-jobs.dataproc.canary.CanaryJob|0.0             |0    |0.0             |\n|spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow|CanaryPipeline-class-jobs.dataproc.canary.CanaryJob|0.0             |0    |0.0             |\n|Gautam: UserId Seller Agg                                         |CleanRoomExplodedDealAvailsAggTransformer          |0.0             |0    |0.0             |\n|spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow|CanaryPipeline-class-jobs.dataproc.canary.CanaryJob|0.0             |0    |0.0             |\n|Gautam: CleanRoomUserIdSellerAgg                                  |CleanRoomUserIdSellerAgg                           |0.0             |0    |0.0             |\n+------------------------------------------------------------------+---------------------------------------------------+----------------+-----+----------------+\n\n\n====================================================================================================\n✨ EMR MULTI-CLUSTER SPARK HISTORY SERVER ANALYSIS COMPLETED! ✨\n====================================================================================================\n✅ Analyzed 7 clusters\n✅ Analyzed 7 applications\n✅ Analyzed 0 jobs\n✅ Analyzed 0 stages\n✅ Analyzed 0 SQL queries\n\n\uD83D\uDCCA DataFrames available for analysis:\n • cluster_summaries_df\n • applications_df\n • jobs_df\n • stages_df\n • sql_df\n • analysis_summary\n\n\uD83D\uDCA1 Example usage:\n applications_df.filter(col('duration_minutes') > 10).show()\n stages_df.groupBy('cluster_name').agg(sum('shuffle_data_mb')).show()\n sql_df.select('description', 'sql_raw_json').show(truncate=False)\n====================================================================================================\n\uD83E\uDDEA DEV Mode: Analysis results available in DataFrames only (no S3 output)\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "\n",
    "# This code assumes that all necessary classes (e.g., EMRClusterDiscovery), \n",
    "# configuration variables (e.g., EMR_CLUSTER_ARN), and the functions \n",
    "# from the 'Cluster Analyzer' block are defined in preceding cells.\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main_analysis() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to execute the complete Spark History Server analysis.\n",
    "    Supports both single cluster analysis and multi-cluster discovery.\n",
    "    Returns all data as DataFrames without creating temp views.\n",
    "\n",
    "    :returns: Dictionary containing all analysis DataFrames and summaries\n",
    "    :raises Exception: If any step of the analysis fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"\uD83D\uDE80 Starting EMR Spark History Server Analysis\")\n",
    "\n",
    "        clusters_to_analyze = []\n",
    "        if EMR_CLUSTER_ARN:\n",
    "            logger.info(\"\uD83C\uDFAF Single cluster mode - using provided EMR Cluster ARN\")\n",
    "            cluster_id = EMR_CLUSTER_ARN.split('/')[-1]\n",
    "            clusters_to_analyze.append({'cluster_id': cluster_id, 'cluster_name': 'User-Specified', 'cluster_arn': EMR_CLUSTER_ARN, 'status': 'UNKNOWN'})\n",
    "        else:\n",
    "            logger.info(\"\uD83D\uDD2D Multi-cluster discovery mode - searching for EMR clusters\")\n",
    "            discovery = EMRClusterDiscovery(AWS_REGION)\n",
    "            discovered_clusters = discovery.discover_clusters(\n",
    "                states=CLUSTER_STATES_LIST,\n",
    "                name_filter=CLUSTER_NAME_FILTER,\n",
    "                max_clusters=MAX_CLUSTERS,\n",
    "                created_after=PARSED_CREATED_AFTER_DATE,\n",
    "                created_before=PARSED_CREATED_BEFORE_DATE\n",
    "            )\n",
    "            if not discovered_clusters:\n",
    "                logger.warning(\"⚠️ No clusters found matching criteria\")\n",
    "                return {'cluster_summaries_df': None, 'applications_df': None, 'jobs_df': None, 'stages_df': None, 'sql_df': None, 'summary': {'total_clusters_analyzed': 0, 'total_applications': 0, 'total_jobs': 0, 'total_stages': 0, 'total_sql_queries': 0}}\n",
    "\n",
    "            for cluster_summary in discovered_clusters:\n",
    "                try:\n",
    "                    cluster_details = discovery.get_cluster_details(cluster_summary['cluster_id'])\n",
    "                    if discovery.validate_cluster_for_analysis(cluster_details):\n",
    "                        clusters_to_analyze.append({'cluster_id': cluster_details['cluster_id'], 'cluster_name': cluster_details['cluster_name'], 'cluster_arn': cluster_details['cluster_arn'], 'status': cluster_details['status'], 'details': cluster_details})\n",
    "                except Exception as e:\n",
    "                    logger.error(\"❌ Failed to validate cluster %s: %s\", cluster_summary['cluster_id'], str(e), exc_info=True)\n",
    "                    continue\n",
    "\n",
    "        if not clusters_to_analyze:\n",
    "            logger.error(\"❌ No valid clusters found for analysis\")\n",
    "            return {'cluster_summaries_df': None, 'applications_df': None, 'jobs_df': None, 'stages_df': None, 'sql_df': None, 'summary': {'total_clusters_analyzed': 0, 'total_applications': 0, 'total_jobs': 0, 'total_stages': 0, 'total_sql_queries': 0}}\n",
    "\n",
    "        logger.info(\"\uD83D\uDCCA Will analyze %s cluster(s)\", len(clusters_to_analyze))\n",
    "\n",
    "        spark = SparkSession.builder.appName(\"EMR-Multi-Cluster-Spark-History-Analysis\").getOrCreate()\n",
    "        analyzer_instance = SparkMetricsAnalyzer(spark)\n",
    "\n",
    "        all_applications_analysis, all_jobs_analysis, all_stages_analysis, all_sql_analysis, cluster_summaries = [], [], [], [], []\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CLUSTER_THREADS) as cluster_executor:\n",
    "            cluster_futures = {\n",
    "                cluster_executor.submit(\n",
    "                    analyze_single_cluster,\n",
    "                    c_info,\n",
    "                    SparkHistoryServerClient,\n",
    "                    SparkMetricsAnalyzer,\n",
    "                    TIMEOUT_SECONDS,\n",
    "                    MAX_APPLICATIONS,\n",
    "                    spark,\n",
    "                    PERSISTENT_UI_TIMEOUT_SECONDS,\n",
    "                    MAX_APP_THREADS\n",
    "                ): c_info['cluster_id']\n",
    "                for c_info in clusters_to_analyze\n",
    "            }\n",
    "\n",
    "            for future in concurrent.futures.as_completed(cluster_futures):\n",
    "                cluster_id = cluster_futures[future]\n",
    "                try:\n",
    "                    cluster_results = future.result()\n",
    "                    if cluster_results:\n",
    "                        cluster_summaries.append({'cluster_id': cluster_results['cluster_id'], 'cluster_name': cluster_results['cluster_name'], 'status': cluster_results['status'], 'analysis_status': cluster_results['analysis_status'], 'status_details': summarize_cluster_status(cluster_results), 'total_applications': cluster_results['total_applications'], 'total_jobs': cluster_results['total_jobs'], 'total_stages': cluster_results['total_stages'], 'total_sql_queries': cluster_results['total_sql_queries']})\n",
    "                        for item_list, key in [(all_applications_analysis, 'applications'), (all_jobs_analysis, 'jobs'), (all_stages_analysis, 'stages'), (all_sql_analysis, 'sql_queries')]:\n",
    "                            for data in cluster_results[key]:\n",
    "                                data['cluster_id'] = cluster_results['cluster_id']\n",
    "                                data['cluster_name'] = cluster_results['cluster_name']\n",
    "                                item_list.append(data)\n",
    "                except Exception as e:\n",
    "                    logger.error(\"❌ Error processing results for cluster %s: %s\", cluster_id, str(e), exc_info=True)\n",
    "                    if not any(s['cluster_id'] == cluster_id for s in cluster_summaries):\n",
    "                        cluster_name = next((c['cluster_name'] for c in clusters_to_analyze if c['cluster_id'] == cluster_id), 'Unknown')\n",
    "                        cluster_summaries.append({'cluster_id': cluster_id, 'cluster_name': cluster_name, 'status': 'FAILED', 'total_applications': 0, 'total_jobs': 0, 'total_stages': 0, 'total_sql_queries': 0, 'analysis_status': 'FAILED', 'status_details': f\"Error processing cluster results: {str(e)}\"})\n",
    "\n",
    "        logger.info(\"\uD83D\uDCBE Creating analysis DataFrames...\")\n",
    "        apps_df, jobs_df, stages_df, sql_df = analyzer_instance.create_performance_dataframes(all_applications_analysis, all_jobs_analysis, all_stages_analysis, all_sql_analysis)\n",
    "\n",
    "        cluster_summary_df = None\n",
    "        if cluster_summaries:\n",
    "            cluster_summary_schema = StructType([StructField('cluster_id', StringType(), True), StructField('cluster_name', StringType(), True), StructField('status', StringType(), True), StructField('analysis_status', StringType(), True), StructField('status_details', StringType(), True), StructField('total_applications', IntegerType(), True), StructField('total_jobs', IntegerType(), True), StructField('total_stages', IntegerType(), True), StructField('total_sql_queries', IntegerType(), True)])\n",
    "            cluster_summary_df = spark.createDataFrame(cluster_summaries, schema=cluster_summary_schema)\n",
    "\n",
    "        logger.info(\"\uD83D\uDCDD Multi-Cluster Analysis Complete - Summary Results:\")\n",
    "        print(\"\\n\" + \"=\"*100 + \"\\n\uD83D\uDCCA CLUSTER ANALYSIS SUMMARY\\n\" + \"=\"*100)\n",
    "        if cluster_summary_df:\n",
    "            print(f\"Cluster Summary ({cluster_summary_df.count()} clusters):\")\n",
    "            cluster_summary_df.show(truncate=False)\n",
    "\n",
    "        if apps_df and apps_df.count() > 0:\n",
    "            logger.info(\" • Total Applications analyzed: %s\", apps_df.count())\n",
    "            print(\"\\n=== TOP 10 APPLICATIONS BY DURATION (ACROSS ALL CLUSTERS) ===\")\n",
    "            apps_df.select(\"cluster_name\", \"application_name\", \"duration_minutes\", \"cores\", \"efficiency_score\").orderBy(\"duration_minutes\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        if jobs_df and jobs_df.count() > 0:\n",
    "            logger.info(\" • Total Jobs analyzed: %s\", jobs_df.count())\n",
    "            print(\"\\n=== JOB SUCCESS RATES BY CLUSTER ===\")\n",
    "            jobs_df.select(\"cluster_name\", \"application_id\", \"job_name\", \"status\", \"task_success_rate\", \"num_tasks\").orderBy(\"task_success_rate\", ascending=True).show(10, truncate=False)\n",
    "\n",
    "        if stages_df and stages_df.count() > 0:\n",
    "            logger.info(\" • Total Stages analyzed: %s\", stages_df.count())\n",
    "            print(\"\\n=== TOP 10 STAGES BY DATA PROCESSED (ACROSS ALL CLUSTERS) ===\")\n",
    "            stages_df.select(\"cluster_name\", \"application_id\", \"stage_name\", \"total_data_processed_mb\", \"shuffle_data_mb\").orderBy(\"total_data_processed_mb\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        if sql_df and sql_df.count() > 0:\n",
    "            logger.info(\" • Total SQL Queries analyzed: %s\", sql_df.count())\n",
    "            print(\"\\n=== TOP 10 SQL QUERIES BY DURATION (ACROSS ALL CLUSTERS) ===\")\n",
    "            sql_df.select(\"cluster_name\", \"application_id\", \"sql_id\", \"description\", \"duration_minutes\", \"status\").orderBy(\"duration_minutes\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        logger.info(\"\uD83C\uDF89 Multi-cluster analysis complete! All data returned as DataFrames.\")\n",
    "        summary = {'total_clusters_analyzed': len([c for c in cluster_summaries if c['analysis_status'] == 'COMPLETED']), 'total_applications': len(all_applications_analysis), 'total_jobs': len(all_jobs_analysis), 'total_stages': len(all_stages_analysis), 'total_sql_queries': len(all_sql_analysis)}\n",
    "\n",
    "        return {'cluster_summaries_df': cluster_summary_df, 'applications_df': apps_df, 'jobs_df': jobs_df, 'stages_df': stages_df, 'sql_df': sql_df, 'summary': summary}\n",
    "    except Exception as e:\n",
    "        logger.error(\"❌ Multi-cluster analysis failed: %s\", str(e), exc_info=True)\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    results = main_analysis()\n",
    "    cluster_summaries_df = results['cluster_summaries_df']\n",
    "    applications_df = results['applications_df']\n",
    "    jobs_df = results['jobs_df']\n",
    "    stages_df = results['stages_df']\n",
    "    sql_df = results['sql_df']\n",
    "    analysis_summary = results['summary']\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n✨ EMR MULTI-CLUSTER SPARK HISTORY SERVER ANALYSIS COMPLETED! ✨\\n\" + \"=\"*100)\n",
    "    print(f\"✅ Analyzed {analysis_summary['total_clusters_analyzed']} clusters\\n✅ Analyzed {analysis_summary['total_applications']} applications\\n✅ Analyzed {analysis_summary['total_jobs']} jobs\\n✅ Analyzed {analysis_summary['total_stages']} stages\\n✅ Analyzed {analysis_summary['total_sql_queries']} SQL queries\")\n",
    "    print(\"\\n\uD83D\uDCCA DataFrames available for analysis:\\n • cluster_summaries_df\\n • applications_df\\n • jobs_df\\n • stages_df\\n • sql_df\\n • analysis_summary\")\n",
    "    print(\"\\n\uD83D\uDCA1 Example usage:\\n applications_df.filter(col('duration_minutes') > 10).show()\\n stages_df.groupBy('cluster_name').agg(sum('shuffle_data_mb')).show()\\n sql_df.select('description', 'sql_raw_json').show(truncate=False)\\n\" + \"=\"*100)\n",
    "\n",
    "    if ENVIRONMENT == \"prod\" and S3_OUTPUT_PATH:\n",
    "        logger.info(\"\uD83D\uDCE6 Writing analysis results to S3: %s\", S3_OUTPUT_PATH)\n",
    "        try:\n",
    "            if cluster_summaries_df and cluster_summaries_df.count() > 0:\n",
    "                cluster_summaries_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/cluster_summaries/\")\n",
    "            if applications_df and applications_df.count() > 0:\n",
    "                applications_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/applications/\")\n",
    "            if jobs_df and jobs_df.count() > 0:\n",
    "                jobs_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/jobs/\")\n",
    "            if stages_df and stages_df.count() > 0:\n",
    "                stages_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/stages/\")\n",
    "            if sql_df and sql_df.count() > 0:\n",
    "                sql_df.write.mode(\"overwrite\").parquet(f\"{S3_OUTPUT_PATH}/sql/\")\n",
    "            logger.info(\"✅ All analysis results successfully written to S3\")\n",
    "        except Exception as s3_error:\n",
    "            logger.error(\"❌ Failed to write results to S3: %s\", str(s3_error), exc_info=True)\n",
    "            print(f\"⚠️ Warning: S3 write failed, but analysis completed successfully. Error: {str(s3_error)}\")\n",
    "    elif ENVIRONMENT == \"dev\":\n",
    "        logger.info(\"\uD83E\uDDEA Running in DEV mode - S3 output skipped\")\n",
    "        print(\"\uD83E\uDDEA DEV Mode: Analysis results available in DataFrames only (no S3 output)\")\n",
    "    else:\n",
    "        logger.info(\"\uD83D\uDEAB PROD mode but no S3 output path specified - S3 output skipped\")\n",
    "        print(\"⚠️ PROD mode detected but no S3 output path provided - results available in DataFrames only\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ANALYSIS FAILED: {str(e)}\\nPlease check the logs above for detailed error information.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ce8970-1746-462e-8b8e-66cab16b3077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Table Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c68ba71-f276-4118-8ba8-213ed1037509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'total_clusters_analyzed': 7,\n",
       " 'total_applications': 7,\n",
       " 'total_jobs': 0,\n",
       " 'total_stages': 0,\n",
       " 'total_sql_queries': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(analysis_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb33c573-1959-4e44-99fd-2a1355377fbd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755032063072}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 00:08:47,056 - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cluster_id</th><th>cluster_name</th><th>status</th><th>analysis_status</th><th>status_details</th><th>total_applications</th><th>total_jobs</th><th>total_stages</th><th>total_sql_queries</th></tr></thead><tbody><tr><td>j-310AFAY6L57Q</td><td>Gautam: UserId Seller Agg</td><td>TERMINATED</td><td>NO_APPLICATIONS</td><td>No applications found in Spark History Server</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-160Y1JZF990JG</td><td>spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2C9L9Y9WYQ6H8</td><td>spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1ILKHEQ510RAI</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1KPXX0HR3U0BN</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-137KTJ1T8J5TS</td><td>spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2F9HRWX3WB2KP</td><td>spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-1VJLYTUUO02M0</td><td>Gautam: UserId Seller Agg</td><td>TERMINATED</td><td>COMPLETED</td><td>applications: OK; jobs: FAILED; sql: FAILED</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-2O70N006Y1DEP</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 300 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>j-38XT10OQKQR6U</td><td>Gautam: UserId Seller Agg</td><td>TERMINATED</td><td>FAILED</td><td>Cluster analysis error: Persistent App UI did not become ready after waiting 300 seconds.</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "j-310AFAY6L57Q",
         "Gautam: UserId Seller Agg",
         "TERMINATED",
         "NO_APPLICATIONS",
         "No applications found in Spark History Server",
         0,
         0,
         0,
         0
        ],
        [
         "j-160Y1JZF990JG",
         "spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED",
         1,
         0,
         0,
         0
        ],
        [
         "j-2C9L9Y9WYQ6H8",
         "spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED",
         1,
         0,
         0,
         0
        ],
        [
         "j-1ILKHEQ510RAI",
         "Gautam: CleanRoomUserIdSellerAgg",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED",
         1,
         0,
         0,
         0
        ],
        [
         "j-1KPXX0HR3U0BN",
         "Gautam: CleanRoomUserIdSellerAgg",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED",
         1,
         0,
         0,
         0
        ],
        [
         "j-137KTJ1T8J5TS",
         "spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED",
         1,
         0,
         0,
         0
        ],
        [
         "j-2F9HRWX3WB2KP",
         "spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED",
         1,
         0,
         0,
         0
        ],
        [
         "j-1VJLYTUUO02M0",
         "Gautam: UserId Seller Agg",
         "TERMINATED",
         "COMPLETED",
         "applications: OK; jobs: FAILED; sql: FAILED",
         1,
         0,
         0,
         0
        ],
        [
         "j-2O70N006Y1DEP",
         "Gautam: CleanRoomUserIdSellerAgg",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 300 seconds.",
         0,
         0,
         0,
         0
        ],
        [
         "j-38XT10OQKQR6U",
         "Gautam: UserId Seller Agg",
         "TERMINATED",
         "FAILED",
         "Cluster analysis error: Persistent App UI did not become ready after waiting 300 seconds.",
         0,
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cluster_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cluster_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "analysis_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status_details",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_applications",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_jobs",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_stages",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_sql_queries",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(cluster_summaries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c596d8c-7ad3-40ed-a0e6-5c155e2856bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cluster_id</th><th>cluster_name</th><th>application_id</th><th>application_name</th><th>duration_ms</th><th>duration_minutes</th><th>start_time</th><th>end_time</th><th>spark_version</th><th>cores</th><th>memory_per_executor_mb</th><th>max_cores</th><th>core_hours</th><th>efficiency_score</th></tr></thead><tbody><tr><td>j-160Y1JZF990JG</td><td>spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow</td><td>application_1755294357227_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>0</td><td>0.0</td><td>null</td><td>null</td><td>Unknown</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td></tr><tr><td>j-2C9L9Y9WYQ6H8</td><td>spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow</td><td>application_1755540700327_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>0</td><td>0.0</td><td>null</td><td>null</td><td>Unknown</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td></tr><tr><td>j-1ILKHEQ510RAI</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>application_1754586783613_0001</td><td>CleanRoomUserIdSellerAgg</td><td>0</td><td>0.0</td><td>null</td><td>null</td><td>Unknown</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td></tr><tr><td>j-1KPXX0HR3U0BN</td><td>Gautam: CleanRoomUserIdSellerAgg</td><td>application_1754583978065_0001</td><td>CleanRoomUserIdSellerAgg</td><td>0</td><td>0.0</td><td>null</td><td>null</td><td>Unknown</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td></tr><tr><td>j-137KTJ1T8J5TS</td><td>spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow</td><td>application_1755296864657_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>0</td><td>0.0</td><td>null</td><td>null</td><td>Unknown</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td></tr><tr><td>j-2F9HRWX3WB2KP</td><td>spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow</td><td>application_1755529524344_0001</td><td>CanaryPipeline-class-jobs.dataproc.canary.CanaryJob</td><td>0</td><td>0.0</td><td>null</td><td>null</td><td>Unknown</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td></tr><tr><td>j-1VJLYTUUO02M0</td><td>Gautam: UserId Seller Agg</td><td>application_1754533647407_0001</td><td>CleanRoomExplodedDealAvailsAggTransformer</td><td>0</td><td>0.0</td><td>null</td><td>null</td><td>Unknown</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "j-160Y1JZF990JG",
         "spark-canary-job-aws @ 2025-08-15T21:42:36.417304+00:00 :: Airflow",
         "application_1755294357227_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         0,
         0.0,
         null,
         null,
         "Unknown",
         0,
         0,
         0,
         0.0,
         0.0
        ],
        [
         "j-2C9L9Y9WYQ6H8",
         "spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow",
         "application_1755540700327_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         0,
         0.0,
         null,
         null,
         "Unknown",
         0,
         0,
         0,
         0.0,
         0.0
        ],
        [
         "j-1ILKHEQ510RAI",
         "Gautam: CleanRoomUserIdSellerAgg",
         "application_1754586783613_0001",
         "CleanRoomUserIdSellerAgg",
         0,
         0.0,
         null,
         null,
         "Unknown",
         0,
         0,
         0,
         0.0,
         0.0
        ],
        [
         "j-1KPXX0HR3U0BN",
         "Gautam: CleanRoomUserIdSellerAgg",
         "application_1754583978065_0001",
         "CleanRoomUserIdSellerAgg",
         0,
         0.0,
         null,
         null,
         "Unknown",
         0,
         0,
         0,
         0.0,
         0.0
        ],
        [
         "j-137KTJ1T8J5TS",
         "spark-canary-job-aws @ 2025-08-15T22:24:16.775314+00:00 :: Airflow",
         "application_1755296864657_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         0,
         0.0,
         null,
         null,
         "Unknown",
         0,
         0,
         0,
         0.0,
         0.0
        ],
        [
         "j-2F9HRWX3WB2KP",
         "spark-canary-job-aws @ 2025-08-18T15:02:04.727576+00:00 :: Airflow",
         "application_1755529524344_0001",
         "CanaryPipeline-class-jobs.dataproc.canary.CanaryJob",
         0,
         0.0,
         null,
         null,
         "Unknown",
         0,
         0,
         0,
         0.0,
         0.0
        ],
        [
         "j-1VJLYTUUO02M0",
         "Gautam: UserId Seller Agg",
         "application_1754533647407_0001",
         "CleanRoomExplodedDealAvailsAggTransformer",
         0,
         0.0,
         null,
         null,
         "Unknown",
         0,
         0,
         0,
         0.0,
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cluster_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cluster_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "application_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "application_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "duration_ms",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "duration_minutes",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "start_time",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "end_time",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "spark_version",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cores",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "memory_per_executor_mb",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "max_cores",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "core_hours",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "efficiency_score",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(applications_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c557931-2eb5-49c4-b43a-a3664a7394f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(jobs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3426ed49-3dc0-4e8f-9938-9c592a27377d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 00:08:48,056 - INFO - Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "display(stages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cdb52b-034d-42f5-a802-c28f3370d2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sql_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5745386645768922,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "emr_spark_profiler_tc",
   "widgets": {
    "aws_region": {
     "currentValue": "us-east-1",
     "nuid": "511e7e68-b1b4-4749-84ef-1ec405556c44",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "us-east-1",
      "label": "AWS Region",
      "name": "aws_region",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "us-east-1",
      "label": "AWS Region",
      "name": "aws_region",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "cluster_name_filter": {
     "currentValue": "",
     "nuid": "b6dc4bb7-c099-48a6-9837-90276589e649",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Cluster Name Filter (optional - partial name match)",
      "name": "cluster_name_filter",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Cluster Name Filter (optional - partial name match)",
      "name": "cluster_name_filter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "cluster_states": {
     "currentValue": "ALL",
     "nuid": "1d5498b2-b955-4ae9-a5c3-7eea2bed9753",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "TERMINATED,WAITING",
      "label": "EMR Cluster States to Analyze",
      "name": "cluster_states",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "TERMINATED",
        "WAITING",
        "TERMINATED,WAITING",
        "ALL"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "TERMINATED,WAITING",
      "label": "EMR Cluster States to Analyze",
      "name": "cluster_states",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "TERMINATED",
        "WAITING",
        "TERMINATED,WAITING",
        "ALL"
       ]
      }
     }
    },
    "created_after_date": {
     "currentValue": "",
     "nuid": "3b80c4ba-25ef-4015-9f10-4dc347dbcd33",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "EMR Clusters Created After (YYYY-MM-DD)",
      "name": "created_after_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "EMR Clusters Created After (YYYY-MM-DD)",
      "name": "created_after_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "created_before_date": {
     "currentValue": "",
     "nuid": "d33922e8-a59a-4c01-bd81-fb68b9626194",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "EMR Clusters Created Before (YYYY-MM-DD)",
      "name": "created_before_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "EMR Clusters Created Before (YYYY-MM-DD)",
      "name": "created_before_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "emr_cluster_arn": {
     "currentValue": "",
     "nuid": "622c9df7-a591-4bce-9304-c0ad4da31422",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "EMR Cluster ARN (optional - leave blank to discover clusters)",
      "name": "emr_cluster_arn",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "EMR Cluster ARN (optional - leave blank to discover clusters)",
      "name": "emr_cluster_arn",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "environment": {
     "currentValue": "dev",
     "nuid": "e6280d95-7b01-40f0-9a6c-48eadd1e04fa",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment (dev/prod)",
      "name": "environment",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment (dev/prod)",
      "name": "environment",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "prod"
       ]
      }
     }
    },
    "max_app_threads": {
     "currentValue": "50",
     "nuid": "5286d8e4-0626-4bae-ac7a-de5e903f3e9f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "100",
      "label": "Max Concurrent App Analysis Threads per Cluster",
      "name": "max_app_threads",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "100",
      "label": "Max Concurrent App Analysis Threads per Cluster",
      "name": "max_app_threads",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_applications": {
     "currentValue": "10",
     "nuid": "b755965a-8868-4bca-9257-35d1977e0c69",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "50",
      "label": "Max Applications to Analyze per Cluster",
      "name": "max_applications",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "50",
      "label": "Max Applications to Analyze per Cluster",
      "name": "max_applications",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_cluster_threads": {
     "currentValue": "50",
     "nuid": "f7ab2744-7a55-4986-b040-5d916859c9c0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "50",
      "label": "Max Concurrent Cluster Analysis Threads",
      "name": "max_cluster_threads",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "50",
      "label": "Max Concurrent Cluster Analysis Threads",
      "name": "max_cluster_threads",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_clusters": {
     "currentValue": "10",
     "nuid": "b22a3439-4c46-4f67-88d6-863f5b985180",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "5",
      "label": "Max Clusters to Analyze",
      "name": "max_clusters",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": "Max Clusters to Analyze",
      "name": "max_clusters",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "persistent_ui_timeout_seconds": {
     "currentValue": "180",
     "nuid": "c46034ae-d60c-445d-95c6-620cf31dd311",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "180",
      "label": "Persistent App UI Timeout (seconds)",
      "name": "persistent_ui_timeout_seconds",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "180",
      "label": "Persistent App UI Timeout (seconds)",
      "name": "persistent_ui_timeout_seconds",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_output_path": {
     "currentValue": "s3://ttd-vertica-backups/env=test/vertica-ext/databricks-stage/sparkmetrics",
     "nuid": "16520b78-96cc-48a8-b2cd-dc660e10c83f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "S3 Output Path (prod only)",
      "name": "s3_output_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "S3 Output Path (prod only)",
      "name": "s3_output_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "timeout_seconds": {
     "currentValue": "300",
     "nuid": "fc534296-f87a-4378-ad10-6778a2ddbd2c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "300",
      "label": "Request Timeout (seconds)",
      "name": "timeout_seconds",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "300",
      "label": "Request Timeout (seconds)",
      "name": "timeout_seconds",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}